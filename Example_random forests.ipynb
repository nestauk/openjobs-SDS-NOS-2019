{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import clone\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import nltk\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import cophenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for fitting random forest model\n",
    "def dropcol_importances(rf, X_train, y_train):\n",
    "    rf_ = clone(rf)\n",
    "    rf_.random_state = 999\n",
    "    rf_.fit(X_train, y_train)\n",
    "    baseline = rf_.oob_score_\n",
    "    imp = []\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        X = X_train.drop(col, axis=1)\n",
    "        rf_ = clone(rf)\n",
    "        rf_.random_state = 999\n",
    "        rf_.fit(X, y_train)\n",
    "        o = rf_.oob_score_\n",
    "        imp.append(baseline - o)\n",
    "    imp = np.array(imp)\n",
    "    I = pd.DataFrame(\n",
    "            data={'Feature':X_train.columns,\n",
    "                  'Importance':imp})\n",
    "    I = I.set_index('Feature')\n",
    "    I = I.sort_values('Importance', ascending=True)\n",
    "    return I\n",
    "\n",
    "\n",
    "def get_predictive_terms(df, bin_data, features, route, n):\n",
    "    is_route = df.Route == route\n",
    "    positive = bin_data[is_route]\n",
    "    _negative = bin_data[~is_route]\n",
    "    size = positive.shape[0]\n",
    "    y=[1]*size + [0]*size\n",
    "    print(route, \"-->\", size)\n",
    "    \n",
    "    importances = []\n",
    "    for i in range(0, n):\n",
    "        random_idx = np.random.choice(_negative.shape[0], size=size, replace=False)\n",
    "        negative = _negative[random_idx]\n",
    "        X=np.vstack([positive, negative])\n",
    "        rf = RandomForestClassifier(max_depth=3, n_estimators=100, oob_score=True)\n",
    "        rf.fit(X, y)\n",
    "\n",
    "        # Build dataframe only with used columns\n",
    "        X_train = pd.DataFrame(X, columns=features)\n",
    "        drop_cols = X_train.columns[rf.feature_importances_ == 0.0]\n",
    "        X_train.drop(drop_cols, axis=1, inplace=True)\n",
    "        \n",
    "        I = dropcol_importances(rf, X_train, y)\n",
    "        importances.append(I)\n",
    "        \n",
    "    df_imp = pd.concat(importances, axis=1, sort=False)\n",
    "    df_imp[pd.isnull(df_imp)] = 0.0\n",
    "    \n",
    "    keywords_mean = df_imp.apply(np.mean, axis=1).sort_values(ascending=False)\n",
    "    return dict(keywords_mean[keywords_mean > 0]), dict(keywords_mean[keywords_mean == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for identifying terms with highest tf-idf\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/jdjumalieva/ESCoE/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/jdjumalieva/ESCoE/lookups/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "(os.path.join(lookup_dir, 'word2vec_output.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api = pd.read_csv(os.path.join(output_dir, 'df_api.csv'),\n",
    "                     encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.iloc[1]['clean_desc'][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting important terms that describe routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['g','e g', 'uk', 'org', '-', '–', 'le', 'kpis', 'anti', 'client ’',\n",
    "                'l', '’ need', 'b', 'k', 'd', '”', '“', 'customer ’', \"'s\",\n",
    "                '‘',  'v', 'h', 'ass', 'http', 'http www', 'www', 'c', 'ac',\n",
    "                'skill –', 'h s', 'nh', 'customers ’', 'process e', \n",
    "                's requirement', 's degree', \"'\", \"organisation 's\", 'level',\n",
    "                'degree', 'de', '·', 'companies ’', 'e', '•', '’']\n",
    "stopwords.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfortoken= df_api['clean_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(elem) for elem in textfortoken]\n",
    "tags = [nltk.pos_tag(elem) for elem in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keeping nouns\n",
    "select = [[word for word,pos in elem if (pos == 'NN' or pos == 'NNP')]\n",
    " for elem in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [elem for elem in select if 'delivers' in elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['efficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api['pruned'] = [' '.join(elem) for elem in select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api['pruned'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfortoken= df_api['pruned']\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize,\n",
    "                        stop_words=new_stopwords,\n",
    "                        ngram_range=(1,2), \n",
    "                        max_df = 0.4, \n",
    "                        min_df = 2)\n",
    "tfidfm = tfidf.fit_transform(textfortoken)\n",
    "feature_names = tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_dict = {}\n",
    "for name, group in df_api.groupby('Route'):    \n",
    "    top_terms = get_top_words(group['pruned'], feature_names, tfidf, n = 100)\n",
    "    print(name, top_terms)\n",
    "    top_terms_dict[name] = top_terms\n",
    "    print('**************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_df = pd.DataFrame.from_dict(top_terms_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_df.to_csv(os.path.join(output_dir, 'top_terms_routes.csv'), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df_api.groupby('Route'):    \n",
    "    if name == 'Digital':\n",
    "        top_terms_weights = get_top_words_weights(group['pruned'], feature_names, tfidf, n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_weights.sort_values(by = 'tfidf', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "for ix, row in top_terms_weights.iterrows():\n",
    "    feature = row['feature']\n",
    "    weight = row['tfidf']\n",
    "    top_words[feature] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words['vulnerability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and calculating weighted sum of terms predictive of a given route for all standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_vec = CountVectorizer(binary=True, stop_words = stopwords)\n",
    "bin_data = bin_vec.fit_transform(textfortoken).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = bin_vec.get_feature_names()\n",
    "    \n",
    "importances = {}\n",
    "scores = {}\n",
    "out_data = {}\n",
    "unimportances = {}\n",
    "for route in set(df_api.Route):\n",
    "    print(route)\n",
    "#    if route == 'Digital':\n",
    "    keywords, keywords_unimportant = get_predictive_terms(df_api, bin_data, features, route, n=50)\n",
    "\n",
    "    importances[route] = keywords\n",
    "    unimportances[route] = keywords_unimportant\n",
    "    count_vec = CountVectorizer(vocabulary=keywords.keys(), stop_words=new_stopwords)\n",
    "    count_data = count_vec.fit_transform(textfortoken).toarray()\n",
    "\n",
    "    weighted_count = []\n",
    "    for _, row in pd.DataFrame(count_data, columns=count_vec.get_feature_names()).iterrows():\n",
    "        wc = sum(np.log(count + 1)*importances[route][term] for term, count in row.iteritems() if count > 0)\n",
    "        weighted_count.append(wc)\n",
    "    weighted_count = np.array(weighted_count)\n",
    "\n",
    "    #count_data[count_data > 0] = 1\n",
    "\n",
    "    corpus_length = np.array([len(doc) for doc in textfortoken])\n",
    "    digitalness = weighted_count/corpus_length\n",
    "\n",
    "    _df = df_api.copy()\n",
    "    _df[route] = digitalness #- np.median(digitalness)\n",
    "\n",
    "    approved = _df.Status == \"Approved for delivery\"\n",
    "\n",
    "\n",
    "    score = {group: grouped[route].median()\n",
    "              for group, grouped in _df[approved].groupby(\"Route\")}\n",
    "    order = [route for route, score in Counter(score).most_common()]\n",
    "    scores[route] = score\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax = sns.boxplot(x=route, y=\"Route\", \n",
    "                     data=_df[_df.Status == \"Approved for delivery\"], \n",
    "                     orient=\"h\", order=order,\n",
    "                     palette=\"Paired\", ax=ax)\n",
    "\n",
    "    ax.set_xlabel(f\"'{route.replace(' ','-').replace(',','').lower()}ness' of apprenticeship standard description\")\n",
    "    ax.set_ylabel(\"Apprenticeship standard route\")\n",
    "    #ax.set_xlim(-0.003, 0.01)\n",
    "    ax.set_title(route)\n",
    "    plt.savefig(f\"{route}-{int(stop_pc*100)}.png\", bbox_inches = \"tight\")        \n",
    "\n",
    "    out_data[route] = _df\n",
    "    out_data[route].sort_values(route, ascending=False)[[\"Title\", \"Route\", route]].to_csv(\n",
    "        f\"{route.replace(' ','-').replace(',','').lower()}ness-{int(stop_pc*100)}.csv\", index=False)\n",
    "\n",
    "print()\n",
    "for route, imps in importances.items():\n",
    "    print(route, \"-->\", [k for k, v in Counter(imps).most_common(20)])\n",
    "    print()\n",
    "\n",
    "print([out_data, scores, importances, unimportances, stop_words])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax = sns.boxplot(x='Digital', y=\"Route\", \n",
    "                 data=_df[_df.Status == \"Approved for delivery\"], \n",
    "                 orient=\"h\", order=order,\n",
    "                 palette=\"Paired\", ax=ax)\n",
    "\n",
    "#ax.set_xlabel(f\"'{route.replace(' ','-').replace(',','').lower()}ness' of apprenticeship standard description\")\n",
    "#ax.set_ylabel(\"Apprenticeship standard route\")\n",
    "#ax.set_xlim(-0.003, 0.01)\n",
    "ax.set_title(route)\n",
    "#plt.savefig(f\"figs/language_specialisation/{route}-{int(stop_pc*100)}.png\", bbox_inches = \"tight\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
