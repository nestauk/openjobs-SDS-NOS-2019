{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.gridspec as gridspec\n",
    "#from matplotlib.patches import Rectangle\n",
    "#import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import clone\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "import networkx as nx\n",
    "import xlrd\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n",
    "\n",
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v'\n",
    "    }\n",
    "\n",
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_with_pos(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i, pos_to_wornet_dict[p]) \n",
    "                       if i not in keep_asis else i for i,p in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_pruned(x, pofs = 'nv'):\n",
    "    if pofs == 'nv':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['V','N']]\n",
    "    elif pofs == 'n':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['N']]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return lemmatise_with_pos(tags)\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tidy_desc_with_pos(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    # add part of speech tagging\n",
    "    nopunct = [(t,nltk.pos_tag([t])[0][1]) for t in nopunct.split()]\n",
    "    nopunct = [t for t in nopunct if t[1] in pos_to_wornet_dict.keys()]\n",
    "    lemm = lemmatise_with_pos(nopunct)\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values\n",
    "\n",
    "\n",
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "#df_nos['pruned'] = df_nos['pruned'].map(remove_pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create your TFidfVectorizer model. This doesn't depend on whether it's used on suites or NOS. However,\n",
    "# it does require that the docs collection is already given as a collection of tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, we will use a different tokenizer\n",
    "def define_tfidf(params, stopwords):\n",
    "    if params['ngrams'] == 'bi':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,2), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    elif params['ngrams'] == 'tri':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,3), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    else:\n",
    "        # unigrams is the default\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "# now, collect the text to transform\n",
    "def combine_nos_text(df_nos, col = 'pruned'):\n",
    "    all_joint_tokens = []\n",
    "    # group by suites and concatenate all docs in it\n",
    "    row_names = []\n",
    "    for name, group in df_nos.groupby('One_suite'):\n",
    "        row_names.append(name)\n",
    "        joint_tokens = []\n",
    "        for idoc in group[col].index:\n",
    "            joint_tokens += group[col].loc[idoc]\n",
    "        all_joint_tokens.append(joint_tokens)\n",
    "    # return a dataframe\n",
    "    return pd.DataFrame({'tokens': all_joint_tokens}, index = row_names)\n",
    "\n",
    "def get_tfidf_matrix(params, df_nos, tfidf, col = 'pruned'):\n",
    "    # Note: this can simply be used to get the tfidf transform, by setting bywhich=docs and any mode\n",
    "    t0 = time.time()\n",
    "    # first, get the dataframe of tokens\n",
    "    if params['bywhich'] == 'docs':\n",
    "        textfortoken = df_nos[col]\n",
    "        \n",
    "    elif params['bywhich'] == 'suites':\n",
    "        if params['mode'] == 'meantfidf':\n",
    "            textfortoken = df_nos[col]\n",
    "                \n",
    "        elif params['mode'] == 'combinedtfidf':\n",
    "            # note that this is the only case where the tfidf min and max are computed considering the number of \n",
    "            # suites as the number of elements in the collection.\n",
    "            # TODO: allow for the alternative case, where the transform is computed on individual NOS and then \n",
    "            # applied to the joint tokens\n",
    "            textfortoken = combine_nos_text(df_nos, col)['tokens']\n",
    "    \n",
    "    # apply tfidf transform to the tokenised text\n",
    "    tfidfm = tfidf.fit_transform(textfortoken)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    \n",
    "    # if the average is needed, compute it and overwrite the matrix. Note that the step above is still needed to\n",
    "    # initialise the tfidf transform with the proper features and stopwords\n",
    "    if (params['bywhich'] == 'suites') and (params['mode'] =='meantfidf'):\n",
    "        row_names = df_nos['One_suite'].value_counts().index.values\n",
    "        tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "        for name, group in df_nos.groupby('One_suite'):\n",
    "            tmp = get_mean_tfidf(group[col], tfidf)\n",
    "            tfidfm[igroup] = tmp\n",
    "\n",
    "    print_elapsed(t0, 'computing the tfidf matrix')\n",
    "    return tfidfm, feature_names, tfidf, textfortoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_supersuite(x):\n",
    "    for supersuite in all_match_names.keys():\n",
    "        if x in all_match_names[supersuite]:\n",
    "            return supersuite.lower()\n",
    "    # if no match has been found\n",
    "    return 'other'\n",
    "\n",
    "def adjustsoccode(x):\n",
    "    y = re.findall(r\"[\\d']+\", str(x))\n",
    "    if len(y):\n",
    "        return y[0][1:-1]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract2digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:2])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract3digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:3])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract1digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract4digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_gensim(list_of_terms, some_model, weights = None):\n",
    "    # replace space with underscore\n",
    "    new_terms = [convert_to_undersc(elem) for elem in list_of_terms]\n",
    "    # check if each element in the list is in the model\n",
    "    is_in = [elem for elem in new_terms if elem in some_model]\n",
    "    # also check the weights\n",
    "    if weights:\n",
    "        weights_in = [weights[ix] for ix,elem in enumerate(new_terms) \n",
    "                        if elem in some_model]\n",
    "    # only return the element in the model\n",
    "    return is_in, weights_in\n",
    "\n",
    "def get_mean_vec(skill_list, model, weights= None):\n",
    "    if not weights:\n",
    "        weights = np.ones(len(skill_list))\n",
    "    skill_list_conv = [convert_to_undersc(elem) for elem in skill_list]\n",
    "    wvector_list = [model[elem]*weights[ix] for ix,elem in enumerate(skill_list_conv) \n",
    "                if elem in model]\n",
    "    vector_list = [model[elem] for ix,elem in enumerate(skill_list_conv) \n",
    "                if elem in model]\n",
    "    vec_array = np.asarray(vector_list)\n",
    "    wvec_array = np.asarray(wvector_list)\n",
    "    avg_vec = np.mean(wvec_array, axis=0)\n",
    "    return avg_vec, vec_array\n",
    "\n",
    "def extract_top_words(tfidfv, feature_names, N=20):\n",
    "    top_ngrams = np.argsort(tfidfv[0,:])\n",
    "    top_ngrams = top_ngrams.tolist()[0][-N:]\n",
    "    top_ngrams = top_ngrams[::-1]\n",
    "    # only retain the ones with non zero features\n",
    "    top_ngrams = [elem for elem in top_ngrams if tfidfv[0,elem]>0]\n",
    "    top_weights = [tfidfv[0,elem] for elem in top_ngrams]\n",
    "    top_features = [feature_names[elem] for elem in top_ngrams]\n",
    "    return top_ngrams, top_weights, top_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subdf(SELECT_MODE, clusters2use, suites_clusters, df_nos_select, dependent_lists = None):\n",
    "    if isinstance(SELECT_MODE, str):\n",
    "        tmp_dict = {'engineering': 'Engineering', 'management': 'Management',\n",
    "                    'financialservices': 'Financial services', \n",
    "                    'construction': 'Construction'}\n",
    "        # select NOS from super suite\n",
    "        cluster_name = SELECT_MODE\n",
    "        cluster_name_save = cluster_name\n",
    "        cluster_name_figs = tmp_dict[SELECT_MODE]\n",
    "        indices = df_nos_select['supersuite']== SELECT_MODE\n",
    "        subset_nos = df_nos_select[indices]\n",
    "    elif isinstance(SELECT_MODE, int):\n",
    "        cluster_name = clusters2use[SELECT_MODE][1]\n",
    "        cluster_name_save = cluster_name.replace(' ','_')\n",
    "        cluster_name_figs = cluster_name.capitalize()\n",
    "        suites2use = list(suites_clusters[suites_clusters['hierarchical'].map(\n",
    "                lambda x: x in clusters2use[SELECT_MODE][0])]['Suite_names'].values)\n",
    "        indices = df_nos_select['One_suite'].map(\n",
    "                lambda x: x in suites2use)\n",
    "        subset_nos = df_nos_select[indices]\n",
    "    print('Number of NOS selected: ', len(subset_nos))\n",
    "    #print(subset_nos.columns)\n",
    "    # select from each item in the list too\n",
    "    if dependent_list:\n",
    "        dependent_list2 = []\n",
    "        for item in dependent_list:\n",
    "            dependent_list2.append(dependent_list[indices])\n",
    "    else:\n",
    "        dependent_list2 = None\n",
    "    \n",
    "    #%\n",
    "    # only select those engineering nos with SOC codes\n",
    "    nosoc = subset_nos['SOC4'].isnull()\n",
    "    print('percentage of nos without SOC codes: ', nosoc.sum()/len(nosoc))\n",
    "    if (nosoc.sum())/len(nosoc)<0.9:\n",
    "        final_nos = subset_nos[~nosoc] #np.isnan(engineering_nos['SOC4'])]\n",
    "        indices = ~nosoc\n",
    "    else:\n",
    "        final_nos = subset_nos\n",
    "        indices = nosoc\n",
    "    # select from each item in the list too\n",
    "    if dependent_list2:\n",
    "        dependent_list = []\n",
    "        for item in dependent_list2:\n",
    "            dependent_list.append(dependent_list[indices])\n",
    "    else:\n",
    "        dependent_list = None\n",
    "    final_groups = final_nos.groupby(by = 'One_suite')\n",
    "    larger_suites = []\n",
    "    all_lengths = final_groups.agg(len)['NOS Title'].values\n",
    "    all_lengths[::-1].sort()\n",
    "    print('Number of NOS in suites belonging to this cluster: ',all_lengths)\n",
    "    #th_supers = ['engineering': 40, 'financialservices': ]\n",
    "    for name, group in final_groups:\n",
    "        if isinstance(SELECT_MODE, int):\n",
    "            larger_suites.append(name)\n",
    "        elif len(group)> all_lengths[15]:#th_supers[SELECT_MODE]:\n",
    "            #print(name, len(group))\n",
    "            larger_suites.append(name)\n",
    "\n",
    "    return final_nos, final_groups, larger_suites, cluster_name,  \\\n",
    "                    cluster_name_save, cluster_name_figs, dependent_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some parameters and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svq_directory = '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/svq_qualifications/'\n",
    "\n",
    "#%%\n",
    "'''  Start of the main script   '''\n",
    "\n",
    "#%% set up main parameters\n",
    "#from set_params_thematic_groups import qualifier, qualifier0, pofs, WHICH_GLOVE, \n",
    "#from set_params_thematic_groups import glove_dir, paramsn\n",
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "\n",
    "\n",
    "WHICH_GLOVE = 'glove.6B.100d' #'glove.6B.100d' #'glove.840B.300d', \n",
    "#glove.twitter.27B.100d\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/'\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])\n",
    "pofs = 'n'\n",
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n",
    "output_dir += 'svq_qualifications_{}_{}'.format(pofs, qualifier)\n",
    "print(output_dir)\n",
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the svq data and do some processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_and_strip(x):\n",
    "    if isinstance(x,str):\n",
    "        x = x.lower().strip().replace('\\xa0',' ').replace('fualt', 'fault')\n",
    "        x = x.replace('\\n',' ')\n",
    "        x = x.replace('sub-assemblies', 'sub assemblies')\n",
    "        x = x.replace('commision', 'commission')\n",
    "        x = x.replace('assesmbly', 'assembly')\n",
    "        x = x.replace('sheetmetal', 'sheet metal')\n",
    "        x = x.replace('compoents', 'components')\n",
    "        x = x.replace('prodution', 'production')\n",
    "        #x = x.replace('– ','')\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "all_svq_files = os.listdir(svq_directory)\n",
    "print('Number of files in directory: ',len(all_svq_files))\n",
    "#print(all_svq_files)\n",
    "\n",
    "svq_dict= {}\n",
    "unit_dict = {}\n",
    "unit_dict_mand = {}\n",
    "unit_codes = {}\n",
    "for svq_file in all_svq_files:\n",
    "    # don't try to open non excel files\n",
    "    if not svq_file.endswith('xlsx'):\n",
    "        continue\n",
    "    # don't try to open file beginning with ~\n",
    "    if svq_file[0] == '~':\n",
    "        continue\n",
    "    try:\n",
    "        xls = xlrd.open_workbook(os.path.join(svq_directory,svq_file), on_demand=True)\n",
    "    except:\n",
    "        print(svq_file)\n",
    "        print('Can not read the file')\n",
    "        continue\n",
    "    Nrows = 14\n",
    "    try:\n",
    "        if xls.sheet_names()[0] == 'Drop Downs': #len(xls.sheet_names)\n",
    "            sheet_name = 1\n",
    "        else:\n",
    "            sheet_name = 0\n",
    "        df_svq = pd.read_excel(os.path.join(svq_directory,svq_file), sheet_name = sheet_name, \n",
    "                               header = None, nrows= Nrows)\n",
    "        # make everything lwoer case and remove extra white spaces\n",
    "        df_svq = df_svq.applymap(lower_and_strip)\n",
    "    except:\n",
    "        print('Failed with ', svq_file)\n",
    "    if len(df_svq)<Nrows:\n",
    "        print(svq_file)\n",
    "        continue\n",
    "    svq_dict[svq_file] = {}\n",
    "    # collect the entries in the first column if they are a string - togethere with the good indices\n",
    "    L = [df_svq.iloc[t][0] for t in range(Nrows) if isinstance(df_svq.iloc[t][0],str)]\n",
    "    tL = [t for t in range(Nrows) if isinstance(df_svq.iloc[t][0],str)]\n",
    "    # find the row where \"structure information\" is\n",
    "    try:\n",
    "        separator_row = tL[['structure information' in t.lower() for t in L].index(True)]\n",
    "        #if not any(['structure information' in t.lower() for t in L]):\n",
    "        #    print(svq_file)\n",
    "    except:\n",
    "        print('*** Separator string not found in file \\'{}\\''.format(svq_file))\n",
    "        separator_row = False\n",
    "    if separator_row:\n",
    "        df_svq_header = df_svq.iloc[:separator_row+1]\n",
    "        df_svq = pd.read_excel(os.path.join(svq_directory,svq_file), sheet_name= sheet_name, header= separator_row+1)\n",
    "        # make everything lwoer case and remove extra white spaces\n",
    "        df_svq = df_svq.applymap(lower_and_strip)\n",
    "    else:\n",
    "        print('*** Something went wrong with file \\'{}\\''.format(svq_file))\n",
    "    # extract info\n",
    "    # 1. look for the title and the scqf in the header\n",
    "    for search_item in ['title','scqf overall level']:\n",
    "        flag = df_svq_header[0].map(lambda x: search_item in x.lower() if isinstance(x,str) else False)\n",
    "        row = df_svq_header[flag]\n",
    "        if len(row):\n",
    "            # take the non null columns\n",
    "            tmp = row.columns[(~row.isnull()).values[0]]\n",
    "            # the first one is column 0 which is the description, the second non-null column is the value\n",
    "            svq_dict[svq_file][search_item] = row[tmp[1]].values[0]\n",
    "    # 2. look for the svq level\n",
    "    flag = df_svq_header.applymap(lambda x: 'svq level' in x.lower() if isinstance(x,str) else False)\n",
    "    if flag.sum().sum()>0:\n",
    "        icol = flag.columns[flag.sum(axis = 0)==1].values[0]\n",
    "        irow = flag.index[flag.sum(axis = 1)==1].values[0]\n",
    "        for col in range(icol+1, len(df_svq_header.columns)):\n",
    "            tmp = df_svq_header.loc[irow][col]\n",
    "            if not np.isnan(tmp):#.isnull():\n",
    "                svq_dict[svq_file]['svq level'] = tmp\n",
    "                break\n",
    "    else:\n",
    "        svq_dict[svq_file]['svq level'] = np.nan\n",
    "    # 3. Get the Unit titles and their scqf levels\n",
    "    #try:\n",
    "    all_cols = [t for t in df_svq.columns if isinstance(t,str)]\n",
    "    col_unit = [t for t in all_cols if 'title' in t.lower()][0]\n",
    "    series_unit = df_svq[col_unit]\n",
    "    col_scqf = [t for t in all_cols if 'level' in t.lower().replace('\\n','')][0]\n",
    "    series_scqf = df_svq[col_scqf]\n",
    "    svq_dict[svq_file]['units'] = {}\n",
    "    col_dev = [t for t in all_cols if 'unit code' in t.lower().replace('\\n','')]\n",
    "    if len(col_dev):\n",
    "        col_dev = col_dev[0]\n",
    "        has_a_code = True\n",
    "    else:\n",
    "        has_a_code = False\n",
    "        #print(svq_file)\n",
    "    col_mand = [t for t in all_cols if 'mandatory' in t.lower().replace('\\n','')][0]\n",
    "    series_mand = df_svq[col_mand]\n",
    "    for ix,unit in enumerate(series_unit):\n",
    "        if isinstance(unit,str):\n",
    "            # check if I've found the unit already (exact or near identical match)\n",
    "            is_a_match = unit in unit_dict\n",
    "            if (not is_a_match)&('heating and ventilating systems' not in unit)&('engineering drawings' not in unit):\n",
    "                # check if it's a near match\n",
    "                all_matches = process.extract(unit, list(unit_dict.keys()))\n",
    "                good_matches = [out for out in all_matches if out[1]>=98]\n",
    "                if len(good_matches)>0:\n",
    "                    print(unit, good_matches)\n",
    "                    unit = good_matches[0][0]\n",
    "                    is_a_match = True\n",
    "            if not is_a_match:\n",
    "                unit_dict[unit] = {}\n",
    "            #if unit not in unit_dict_mand:\n",
    "                unit_dict_mand[unit] = {}\n",
    "            unit_dict[unit][svq_dict[svq_file]['title']] = series_scqf.iloc[ix]\n",
    "            unit_dict_mand[unit][svq_dict[svq_file]['title']] = series_mand.iloc[ix]\n",
    "            # save it in a dict structure by framework\n",
    "            svq_dict[svq_file]['units'][unit] =  series_scqf.iloc[ix]\n",
    "            #svq_dict[svq_file]['units'].append({'title': unit, 'scqf level': series_scqf.iloc[ix]}) \n",
    "            # if the framework also gives me the code that's good\n",
    "            if has_a_code:\n",
    "                if unit not in unit_codes:\n",
    "                    unit_codes[unit] = []\n",
    "                unit_codes[unit].append(df_svq[col_dev].iloc[ix])\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svq_data = pd.DataFrame.from_dict(svq_dict, orient = 'index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print units that appear with different levels in different frameworks\n",
    "unit_data = pd.DataFrame.from_dict(unit_dict, orient = 'index')\n",
    "unit_data_mand = pd.DataFrame.from_dict(unit_dict_mand, orient = 'index')\n",
    "print(unit_data.index[unit_data.std(axis = 1)>0])\n",
    "for unit in unit_data.index[unit_data.std(axis = 1)>0]:\n",
    "    print(unit_dict[unit])\n",
    "    print()\n",
    "#print((~unit_data.isnull()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the codes associated with some of the units \n",
    "Cleaning the codes means that I'll be able to match them to URNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_unit_codes = copy.deepcopy(unit_codes)\n",
    "\n",
    "def transform_codes(code):\n",
    "    code = code.lower()\n",
    "    # eliminate occurrences of version and unit number:\n",
    "    code = re.sub('v\\d+|v\\s+\\d+', '', code)\n",
    "    code = re.sub('unit\\d+|unit\\s+\\d+', '', code)\n",
    "    code = code.replace('()','')\n",
    "    # if it starts with vr, it probably was missing cos\n",
    "    if 'vr' == code[:2]:\n",
    "        code = 'cos' + code\n",
    "    # eliminate extra spaces and punctuation\n",
    "    code = code.strip().replace('-','').replace('()','')\n",
    "    #code = re.sub('\\s+()|()|()','',code)\n",
    "    # special case\n",
    "    if code == 'semman12302':\n",
    "        code = 'semman2302'\n",
    "    if code == 'semman12301()':\n",
    "        code = 'semman12301'\n",
    "    return code.split()\n",
    "\n",
    "for unit in unit_codes:\n",
    "    new_codes = [transform_codes(t) for t in unit_codes[unit]]\n",
    "    new_codes = flatten_lol(new_codes)\n",
    "    new_unit_codes[unit] = Counter(new_codes)\n",
    "    #if len(set(new_codes))>1:\n",
    "    #    print(unit)\n",
    "    #    print(Counter(new_codes).most_common())\n",
    "#unit_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# what units appear more often?\n",
    "# note: this is not necessarily very informative because some frameworks are split into multiple paths \n",
    "# and some are not\n",
    "((unit_data_mand == 'mandatory') | (unit_data_mand =='optional')).sum(axis = 1).sort_values(ascending= False)[:20]'''\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_levels = pd.DataFrame(unit_data.apply(np.nanmean, axis = 1).map(np.round), \n",
    "                           columns = ['scqf_levels'])\n",
    "group_unit_levels = unit_levels.groupby(by='scqf_levels')\n",
    "for name, _ in group_unit_levels:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tidy_desc_with_pos_pof(desc):\n",
    "    pof='v'\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    # add part of speech tagging\n",
    "    nopunct = [(t,nltk.pos_tag([t])[0][1]) for t in nopunct.split()]\n",
    "    nopunct = [t for t in nopunct if t[1] in pos_to_wornet_dict.keys()]\n",
    "    if pof == 'v':\n",
    "        nopunct = [t for t in nopunct if t[1][0] == 'V']\n",
    "    lemm = lemmatise_with_pos(nopunct)\n",
    "    return ' '.join(lemm)\n",
    "    \n",
    "# check if there's any easy to spot difference between levels\n",
    "for name, group in group_unit_levels:\n",
    "    # count bi grams in each group\n",
    "    count_vec = CountVectorizer(stop_words = [], ngram_range= (1,1),\n",
    "                           preprocessor = tidy_desc_with_pos_pof)\n",
    "    raw_docs= group.index.values\n",
    "    count_vec = count_vec.fit(raw_docs)\n",
    "    level_counts = count_vec.transform(raw_docs).toarray()\n",
    "    print('Level ',name)\n",
    "    counts_by_term = level_counts.sum(axis=0)\n",
    "    #print(len(counts_by_term),len(count_vec.get_feature_names()))\n",
    "    #print(count_vec.vocabulary_)\n",
    "    if name < 7.0:\n",
    "        for doc in raw_docs:\n",
    "            if 'bus' in doc:\n",
    "                print(name,doc)\n",
    "    terms_counter = {}\n",
    "    for term in count_vec.vocabulary_:\n",
    "        ix = count_vec.vocabulary_[term]\n",
    "        terms_counter[term] = counts_by_term[ix]\n",
    "    terms_counter = Counter(terms_counter).most_common()\n",
    "    print(terms_counter[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NOS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data \n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset and join it with the rest\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + \n",
    "                    'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "\n",
    "# remove p and k\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)\n",
    "print('Done with loading the dataset')\n",
    "\n",
    "\n",
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„',\n",
    "                     \"'m\", \"'re\", '£','—','‚°','●'])\n",
    "stopwords0 += tuple(set(list(df_nos['Developed By'])))\n",
    "stopwords0 += tuple(['cosvr','unit','standard','sfl','paramount','tp','il','al','ad','hoc',\n",
    "                    'lanleo','ireland','something'])\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# create another column where the texts are lemmatised properly\n",
    "t0 = time.time()\n",
    "df_nos['pruned_lemmas'] = df_nos['tagged_tokens'].map(lambda x: lemmatise_pruned(x,pofs))\n",
    "print(time.time()-t0)\n",
    "\n",
    "\n",
    "# ### Only keep NOS from a super-suite\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "super_suites_files=  ''.join(['/Users/stefgarasto/Google Drive/Documents/data/',\n",
    "                              'NOS_meta_data/NOS_Suite_Priority.xlsx'])\n",
    "super_suites_names = ['Engineering','Management','FinancialServices','Construction']\n",
    "all_super_suites = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_super_suites[which_super_suite] = pd.read_excel(super_suites_files, \n",
    "                    sheet_name = which_super_suite)\n",
    "    all_super_suites[which_super_suite]['NOS Suite name'] = all_super_suites[\n",
    "        which_super_suite]['NOS Suite name'].map(\n",
    "        lambda x: x.replace('(','').replace('(','').replace('&','and').strip().lower())\n",
    "\n",
    "\n",
    "# Match given suites names in super-suites with the names we have in the data\n",
    "standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "all_matches = {}\n",
    "all_match_names = {}\n",
    "#match_name = []\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_matches[which_super_suite] = []\n",
    "    for suite in all_super_suites[which_super_suite]['NOS Suite name'].values:\n",
    "        # do manually some selected suites\n",
    "        if 'insurance claims' in suite:\n",
    "            tmp = standard_labels.index('general insurance')\n",
    "            all_matches[which_super_suite].append(tmp)\n",
    "            continue\n",
    "        # for the \"management and leadership marketing 2013\" both marketing \n",
    "        # and marketing 2013 would fit,\n",
    "        # but I'm only taking the latter\n",
    "        # find a fuzzy match between \n",
    "        out = process.extract(suite, standard_labels, limit=3)\n",
    "        if len(out) and out[0][1]>89:\n",
    "            # note: most of them are above 96% similarity (only one is 90%)\n",
    "            tmp = standard_labels.index(out[0][0])\n",
    "            #print(suite, out[0])\n",
    "            if tmp not in all_matches[which_super_suite]:\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "            else:\n",
    "                if suite == 'installing domestic fascia, soffit, and bargeboards':\n",
    "                    # this suite is kind of a duplicate - I aggregated it in my suites list\n",
    "                    continue\n",
    "                tmp = standard_labels.index(out[2][0])\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "                print(out[0][0],',',out[1][0],',',out[2][0],',',suite)\n",
    "        else:\n",
    "            print(suite, ' not found')\n",
    "            print(out)\n",
    "            print('\\n')\n",
    "    print(len(all_matches[which_super_suite]),len(all_super_suites[which_super_suite]))\n",
    "    all_match_names[which_super_suite] = [standard_labels[t] \n",
    "                    for t in all_matches[which_super_suite]]\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "# assign supersuite and SOC codes\n",
    "df_nos['supersuite'] = df_nos['One_suite'].apply(assign_supersuite)\n",
    "# extract 2 digit soc\n",
    "df_nos['SOC4str'] = df_nos['Clean SOC Code'].map(adjustsoccode)\n",
    "df_nos['SOC1'] = df_nos['SOC4str'].map(extract1digits)\n",
    "df_nos['SOC2'] = df_nos['SOC4str'].map(extract2digits)\n",
    "df_nos['SOC3'] = df_nos['SOC4str'].map(extract3digits)\n",
    "df_nos['SOC4'] = df_nos['SOC4str'].map(extract4digits)\n",
    "print(df_nos['supersuite'].value_counts())\n",
    "print('All done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join NOS with svq units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for those units that have a code, get the NOS with the same code in its file name\n",
    "\n",
    "col_to_match= 'URN'\n",
    "nos_matched = {}\n",
    "for counter,unit in enumerate(unit_data_mand.index):\n",
    "    if unit in new_unit_codes:\n",
    "        all_codes = list(new_unit_codes[unit].keys())\n",
    "        \n",
    "        for code in all_codes:\n",
    "            #print(tmp)\n",
    "            if col_to_match == 'NOS Title':\n",
    "                matches = df_nos[df_nos.index.map(lambda x: code in x.replace('_','&').replace('-',''))]\n",
    "            else:\n",
    "                matches = df_nos[df_nos['URN'].map(lambda x: code in x.replace('-','').replace('_','&'))]\n",
    "\n",
    "            # unique match\n",
    "            if len(matches) == 1:\n",
    "                # check that also the titles are similar\n",
    "                flag = process.extract(unit, matches['NOS Title'])[0][1]\n",
    "                match = matches.index[0]\n",
    "                #print(match)\n",
    "                if flag>=90:\n",
    "                    if match not in nos_matched:\n",
    "                        nos_matched[match] = []\n",
    "                    # what's missing is that I need to add all the information related to this unit\n",
    "                    nos_matched[match].append(unit)\n",
    "                else:\n",
    "                    print('Similar code, but different title: ', code, flag)\n",
    "                    print(unit) \n",
    "                    print(matches['NOS Title'])\n",
    "                    print()\n",
    "            # more than one match\n",
    "            elif len(matches)>1:\n",
    "                flag = process.extract(unit, matches['NOS Title'])\n",
    "                # pick the best match, if there is a best match\n",
    "                values = [t[1] for t in flag]\n",
    "                if len(set(values))>1:\n",
    "                    idx = np.argmax(values)\n",
    "                else: \n",
    "                    # all matches are the same. Does it change if I remove \"legacy\" from the title?\n",
    "                    flag = process.extract(unit, matches['NOS Title'].map(lambda x: x.replace('legacy','')))\n",
    "                    # pick the best match, if there is a best match\n",
    "                    values = [t[1] for t in flag]\n",
    "                    if len(set(values))>1:\n",
    "                        idx = np.argmax(values)\n",
    "                    else:\n",
    "                        #pick the one that's not legacy\n",
    "                        a = [i for i,t in enumerate(flag) if not t[2].endswith('l.pdf')]\n",
    "                        a = [i for i,t in enumerate(flag) if not t[2].endswith('l')]\n",
    "                        idx = a[0]\n",
    "                match= matches.index[idx]\n",
    "                flag = values[idx]\n",
    "                if flag>=90:\n",
    "                    if match not in nos_matched:\n",
    "                        nos_matched[match] = []\n",
    "                    # what's missing is that I need to add all the information related to this unit\n",
    "                    nos_matched[match].append(unit)\n",
    "                else:\n",
    "                    print('multiple matches, none very good', code)\n",
    "                    print(unit)\n",
    "                    print()\n",
    "            else:\n",
    "                1 # no match\n",
    "                print('No match: ', code, unit)\n",
    "                print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.from_dict(nos_matched, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "manual_matching_list = {\n",
    "    'deal with hazards in process engineering maintenance': 'cogpem3.pdf', \n",
    "    'perform electrical work on mechanical building services systems': 'sumbse04.pdf',\n",
    "    'prepare loads for moving during process engineering maintenance': 'cogpem06.pdf',\n",
    "    'working efficiently and effectively in engineering': 'sempeo202.pdf',\n",
    "    'working efficiently and effectively in the power sector': 'sempeo202.pdf',\n",
    "    'work efficiently and effectively in engineering food and drink operations': 'sempeo202.pdf',\n",
    "    'working safely in the power sector': 'sempeo201.pdf',\n",
    "    'install underground cables': 'eusepus042l.pdf',\n",
    "    'carry out excavation activities on underground cables': 'eusepus043l.pdf',\n",
    "    'location and identification of underground utility services': 'eusepus044l.pdf',\n",
    "    ''.join(['identify and suggest improvements to working practices and procedures whilst maintaining ',\n",
    "         'electrical plant and equipment']): 'none',\n",
    "    'diagnose mechanical/electrical faults in ancillary systems and compoents in buses/coaches': 'pplbacem32.pdf',\n",
    "    'recondition electrical components in buses/coaches': 'none',\n",
    "'operate powered units, tools or pedestrian plant, machinery or equipment' : 'cosvr400.pdf',\n",
    " 'operate powered units, tools or pedestrian plant, machinery or equipment (generators)': 'cosvr400.pdf',\n",
    "'operate powered units, tools or pedestrian plant, machinery or equipment (pumps)': 'cosvr400.pdf',\n",
    "    'operate powered units, tools or pedestrian plant, machinery or equipment (mixers)' : 'cosvr400.pdf',\n",
    "    'operate powered units, tools or pedestrian plant, machinery or equipment (compressors)' : 'cosvr400.pdf',\n",
    "'operate powered units, tools or pedestrian plant, machinery or equipment (pedestrian operated plant or machines)' : \n",
    "                                                                                            'cosvr400.pdf',\n",
    "    'operate powered units, tools or pedestrian plant, machinery or equipment (self-powered tools)':'cosvr400.pdf',\n",
    "    'operate plant or machinery to receive and transport loads (forward tipping dumper wheeled)' : 'cosvr391.pdf',\n",
    "    'operate plant or machinery to receive and transport loads (forward tipping dumper tracked)' : 'cosvr391.pdf',\n",
    "    'customer relations for working in the power sector': 'semets358.pdf',\n",
    "    'maintain compressed air systems': 'impem0151s.pdf',\n",
    "  'rectify body damage to bus/coach body components': 'pplbacem09.pdf',\n",
    "    'resolving engineering or manufacturing support problems': 'semem403.pdf',\n",
    "    'inspecting fabricated components and structures': 'semts212.pdf'\n",
    "}\n",
    "manual_matching_reverse= {}\n",
    "for k in manual_matching_list:\n",
    "    manual_matching_reverse[manual_matching_list[k]] = k\n",
    "\n",
    "nos_matched2 = copy.deepcopy(nos_matched)\n",
    "# match the unit titles to NOS in the dataframe, if they have not already been matched by code\n",
    "t0 = time.time()\n",
    "#unit_indices = []\n",
    "#unit_indices_dict = {}\n",
    "unit_missed = {}\n",
    "#unit_data_mand['index'] = 'none'\n",
    "for counter,unit in enumerate(unit_data_mand.index):\n",
    "    if counter%200 == 199:\n",
    "        print('got to unit nb {}'.format(counter))\n",
    "    if unit in new_unit_codes:\n",
    "        #unit_data_mand['index'].loc[unit] = 'matched by code'\n",
    "        continue\n",
    "    ix = df_nos[df_nos['NOS Title'] == unit.lower()].index\n",
    "    if unit in manual_matching_list:\n",
    "        #unit_indices.append(manual_matching_list[unit])\n",
    "        #unit_indices_dict[unit] = manual_matching_list[unit]\n",
    "        #unit_data_mand['index'].loc[unit] = manual_matching_list[unit]\n",
    "        if manual_matching_list[unit] not in nos_matched2:\n",
    "            nos_matched2[manual_matching_list[unit]] = []\n",
    "        nos_matched2[manual_matching_list[unit]].append(unit)\n",
    "    elif len(ix):\n",
    "        #unit_indices.append(ix[0])\n",
    "        #unit_indices_dict[unit] = ix[0]\n",
    "        if ix[0] not in nos_matched2:\n",
    "            nos_matched2[ix[0]] = []\n",
    "        nos_matched2[ix[0]].append(unit)\n",
    "    else:\n",
    "        out = process.extract(unit, df_nos['NOS Title'].values, limit = 1)\n",
    "        if out[0][1]>=95:\n",
    "            ix = df_nos[df_nos['NOS Title'] == out[0][0]].index\n",
    "            #unit_indices.append(ix[0])\n",
    "            #unit_indices_dict[unit] = ix[0]\n",
    "            #unit_data_mand['index'].loc[unit] = ix[0]\n",
    "            if ix[0] not in nos_matched2:\n",
    "                nos_matched2[ix[0]] = []\n",
    "            nos_matched2[ix[0]].append(unit)\n",
    "        else:\n",
    "            unit_missed[unit] = out\n",
    "            #unit_indices.append('none')\n",
    "            #unit_indices_dict[unit] = 'none'\n",
    "        #break\n",
    "print_elapsed(t0, 'matching units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unmatched units\n",
    "len(unit_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# check that everything is consistent (including the order)\n",
    "# the content of the dictionary should be the same as the list\n",
    "print(list(unit_indices_dict.values()) == unit_indices)\n",
    "# the content of the new column should also be the same as the list (not anymore because I skip units with a code)\n",
    "print(list(unit_data_mand['index'].values) == unit_indices)\n",
    "# the keys of the dictionary should be in the same order as the indices in the dataframe\n",
    "# (not anymore because I skip units with a code)\n",
    "print(list(unit_indices_dict.keys()) == list(unit_data_mand.index))'''\n",
    "units_to_nos_last = {}\n",
    "for nos in nos_matched2:\n",
    "    if nos== 'none':\n",
    "        continue\n",
    "    for i in nos_matched2[nos]:\n",
    "        units_to_nos_last[i] = df_nos.loc[nos]['NOS Title']\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now crosswalk to pathways via the units\n",
    "nos_matched_to_svq = {}\n",
    "for match in nos_matched2:\n",
    "    # extract the subset of unit_data matched to this NOS\n",
    "    rows_of_interest = unit_data.loc[nos_matched2[match]]\n",
    "    # combine them so that I take all pathways that appear at least once\n",
    "    combined_rows = rows_of_interest.agg(np.nanmean)\n",
    "    nos_matched_to_svq[match] = combined_rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''# look for units that are matched to the same NOS\n",
    "for ix in unit_indices:\n",
    "    #a = [t for t in unit_indices if t ==ix]\n",
    "    b = [i for i,t in enumerate(unit_indices) if t ==ix]\n",
    "    if (len(b)>1) and (ix != 'none'):\n",
    "        print(ix, list(unit_data_mand.index[b]))\n",
    "        print()'''\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the units in the SVQ frameworks with the NOS database\n",
    "svq_cols = svq_data['title'].values \n",
    "# use this for the order of the columns since it's the same as the count_matrix that'll be built later\n",
    "'''\n",
    "# this one was when I went by units\n",
    "unit_tmp = copy.deepcopy(unit_data)\n",
    "unit_tmp['index'] = unit_indices\n",
    "unit_tmp = unit_tmp.set_index('index')\n",
    "unit_tmp = unit_tmp[~(unit_tmp.index == 'none')]\n",
    "df_nos2 = df_nos.join(unit_tmp, how = 'left')\n",
    "# extract the engineering ones\n",
    "df_nos_eng2 = df_nos2[df_nos2['supersuite'] == 'engineering']\n",
    "'''\n",
    "# this is for when I went by NOS\n",
    "df_nos2 = df_nos.join(pd.DataFrame.from_dict(nos_matched_to_svq, orient = 'index'), how = 'left')\n",
    "df_nos_eng2 = df_nos2[df_nos2['supersuite'] == 'engineering']\n",
    "#df_nos_eng2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = df_nos_eng2[df_nos_eng2[svq_cols].mean(axis =1).notna()][df_nos_eng2['NOS Title'].map(lambda x: 'coaches' in x)\n",
    "                                                            ]['NOS Title']\n",
    "for a in A:\n",
    "    print(a)\n",
    "print()\n",
    "for ix,a in enumerate(A):\n",
    "    #print(A.index[ix])\n",
    "    #print(a)\n",
    "    #print(nos_matched2[A.index[ix]])\n",
    "    tt= []\n",
    "    for t in nos_matched2[A.index[ix]]:\n",
    "        #print(list(unit_dict[t].values()))\n",
    "        tt += list(unit_dict[t].values())\n",
    "    print(np.mean(tt))\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute similarity between pathways using word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vec = CountVectorizer(stop_words = 'english', ngram_range= (1,2),\n",
    "                           preprocessor = tidy_desc_with_pos)\n",
    "unit_corpus = []\n",
    "for row in svq_data.index:\n",
    "    unit_corpus.append( ' '.join(list(svq_data['units'].loc[row].keys())))\n",
    "count_matrix = count_vec.fit_transform(unit_corpus) #pd.DataFrame.from_dict(unit_corpus,orient= 'index'))\n",
    "print('Some vocabulary terms')\n",
    "count_features = count_vec.get_feature_names()\n",
    "print(count_features[1000:1050])\n",
    "#count_matrix = count_vec.transform(unit_corpus) #pd.DataFrame.from_dict(unit_corpus,orient= 'index'))\n",
    "print('nb of documents x nb of features')\n",
    "print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,row in enumerate(svq_data.index):\n",
    "    if np.random.randn()>1.3:\n",
    "        print(row)\n",
    "        top_ngrams, top_weights, top_features = extract_top_words(count_matrix.todense()[ix], count_features)\n",
    "        print(top_features)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distance matrix\n",
    "count_dist = distance.squareform(distance.pdist(count_matrix.toarray(), metric = 'cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distance matrix\n",
    "plt.figure(figsize = (8,8))\n",
    "sns.heatmap(1 - count_dist)\n",
    "plt.title('Similarity')\n",
    "plt.gca().set_yticks(np.arange(.5,65.5))\n",
    "plt.gca().set_yticklabels(svq_data.index.map(lambda x: x[6:-5]), {'fontsize': 7, 'rotation' : 'horizontal'})\n",
    "plt.gca().set_xticks(np.arange(.5,65.5))\n",
    "tmp = plt.gca().set_xticklabels(svq_data.index.map(lambda x: x[6:-5]), {'fontsize': 7, 'rotation' : 'vertical'})\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each engineering NOS assign the average count_matrix of all the pathways the NOS is associated with\n",
    "#df_nos_eng_counts = df_nos_eng2[['NOS Title','One_suite']]\n",
    "# NOS not associated to any units have np.nan\n",
    "nos_eng_counts = np.empty((len(df_nos_eng2),count_matrix.shape[1]))\n",
    "nos_eng_counts[:] = np.nan\n",
    "for ix,nos in enumerate(df_nos_eng2.index):\n",
    "    svq_matrix = df_nos_eng2[svq_cols].loc[nos].values\n",
    "    if np.isnan(svq_matrix).sum() == len(svq_cols):\n",
    "        continue\n",
    "    indices = ~np.isnan(svq_matrix)\n",
    "    # get all the relevant rows of the count matrix\n",
    "    count_sub_matrix = count_matrix[indices,:].toarray()\n",
    "    # take the average across pathways\n",
    "    tmp = np.nanmean(count_sub_matrix, axis = 0) #, keepdims = True)\n",
    "    # assign to this NOS\n",
    "    nos_eng_counts[ix] = tmp\n",
    "print('Done. Count matrix shape:', nos_eng_counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute cosine distances between NOS based on the svq pathways they are associated with\n",
    "nos_eng_count_dist = distance.pdist(nos_eng_counts, metric = 'cosine')\n",
    "print(nos_eng_count_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cosine similarities (higher = more similar). NOS not in SVQ frameworks should have nans\n",
    "sns.heatmap(distance.squareform(1 - nos_eng_count_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# select NOS in super-suites of interest\n",
    "df_nos_select = df_nos[~(df_nos['supersuite']=='other')]\n",
    "print('Nb of NOS in all supersuites' ,len(df_nos_select))\n",
    "df_nos_eng = df_nos_select[df_nos_select['supersuite']=='engineering']\n",
    "\n",
    "'''#%%\n",
    "'''\n",
    "# ## Get raw data and tokenize\n",
    "\n",
    "# ## Choosing parameters for features extraction\n",
    "# \n",
    "# ngrams : uni/bi/tri\n",
    "# \n",
    "# tfidf thresholds: min and max percentage\n",
    "# \n",
    "# which parts of speech were selected before\n",
    "# \n",
    "# whether we are working at the level of suites or of invidual NOS, \n",
    "# and how we aggregate NOS to form the suit level\n",
    "# \n",
    "'''\n",
    "\n",
    "#\n",
    "\n",
    "'''\n",
    "# First, create your TFidfVectorizer model. This doesn't depend on whether \n",
    "it's used on suites or NOS. However,\n",
    "it does require that the docs collection is already given as a collection of\n",
    "tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, \n",
    "we will use a different tokenizer\n",
    "'''\n",
    "\n",
    "paramsn = {}\n",
    "paramsn['ngrams'] = 'uni'\n",
    "paramsn['pofs'] = pofs #'nv'\n",
    "paramsn['tfidf_min'] = 3\n",
    "paramsn['tfidf_max'] = 0.5\n",
    "\n",
    "paramsn['bywhich'] = 'docs' #'docs' #'suites'\n",
    "paramsn['mode'] = 'tfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "# define the transform: this one can easily be the same for both \n",
    "# keywords and the clustering\n",
    "tfidf_n = define_tfidf(paramsn, stopwords0)\n",
    "\n",
    "# get the transform from the whole NOS corpus\n",
    "FULL_CORPUS = True\n",
    "if FULL_CORPUS:\n",
    "    _, feature_names_n, tfidf_n, _ = get_tfidf_matrix(\n",
    "            paramsn, df_nos2, tfidf_n, col = 'pruned_lemmas')\n",
    "else:\n",
    "    _, feature_names_n, tfidf_n, _ = get_tfidf_matrix(\n",
    "            paramsn, df_nos_eng2, tfidf_n, col = 'pruned_lemmas')\n",
    "\n",
    "\n",
    "print('Number of features: {}'.format(len(feature_names_n)))\n",
    "N = 2000\n",
    "print('Some features:')\n",
    "print(feature_names_n[N:N+100:3])\n",
    "print('*'*70)\n",
    "\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "# first transform via tfidf all the NOS in one supersuite because you need the top keywords\n",
    "textfortoken = df_nos_eng2['pruned_lemmas']\n",
    "tfidfm = tfidf_n.transform(textfortoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two important collections of vectors are: df_nos_eng_counts (counts based on pathways presence) and\n",
    "# tfidfm (tfidfm transform) of each engineering NOS\n",
    "tfidf_cols = ['tfidf {}'.format(t) for t in range(tfidfm.toarray().shape[1])]\n",
    "df_nos_eng_tfidfm = pd.DataFrame(tfidfm.toarray(), index = df_nos_eng2.index,\n",
    "                         columns = tfidf_cols)\n",
    "counts_cols= ['counts {}'.format(t) for t in range(nos_eng_counts.shape[1])]\n",
    "df_nos_eng_counts = pd.DataFrame(nos_eng_counts, index = df_nos_eng2.index, \n",
    "                                 columns = counts_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now load NOS that I have to remove/merge before producing pathways\n",
    "# load transferable NOS\n",
    "transferable_file = ''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/',\n",
    "                            'transferable_nos_n_postjoining_final_no_dropped/estimated_transferable_nos.csv'])\n",
    "transferable_nos = pd.read_csv(transferable_file)\n",
    "transferable_nos = transferable_nos.set_index('Unnamed: 0')\n",
    "transferable_nos['transferable'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load duplicated NOS\n",
    "lshduplicate_file = ''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/',\n",
    "         'LSH_results_grouped_with_score_postjoining_final_no_dropped_th0.8.csv'])\n",
    "lshduplicate_nos = pd.read_csv(lshduplicate_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# TODO: find a way to speed this up, there has to be one\n",
    "t0 = time.time()\n",
    "df_nos_eng_lsh = df_nos_eng2[['NOS Title']]\n",
    "df_nos_eng_lsh['lsh_group'] = 'empty'\n",
    "df_nos_eng_lsh['lsh_simil'] = 0\n",
    "for ix, row in enumerate(lshduplicate_nos.index):\n",
    "    if ix%200 == 199:\n",
    "        print('got to group number {}'.format(ix))\n",
    "    if lshduplicate_nos.loc[row][0] == 'group 1':\n",
    "        continue\n",
    "    for c in lshduplicate_nos.loc[row][2:]:\n",
    "        if isinstance(c, str):\n",
    "            c = c.replace('(','').replace(')','').replace('\\'','')\n",
    "            t = [i.strip() for i in c.split(',')]\n",
    "            if t[-1] in df_nos_eng_lsh.index:\n",
    "                df_nos_eng_lsh.loc[t[-1]]['lsh_group'] = lshduplicate_nos.loc[row][0]\n",
    "                df_nos_eng_lsh.loc[t[-1]]['lsh_simil'] = lshduplicate_nos.loc[row][1]\n",
    "print_elapsed(t0, 'assigning group ID')'''\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nos_in_groups(x):\n",
    "    if isinstance(x,str):\n",
    "        x = [t.strip() for t in x.replace(')','').replace('(','').replace('\\'','').split(',')]\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "tmp0 = lshduplicate_nos.applymap(split_nos_in_groups)\n",
    "df_nos_eng_lsh = tmp0[['Unnamed: 0','Avg group similarity','1']]\n",
    "t0 = time.time()\n",
    "for i in range(2, len(lshduplicate_nos.columns)-2):\n",
    "    tmp = tmp0[['Unnamed: 0','Avg group similarity','{}'.format(i)]].rename(columns = {'{}'.format(i):'1'})\n",
    "    tmp = tmp[tmp['1'].notna()]\n",
    "    df_nos_eng_lsh = pd.concat([df_nos_eng_lsh, tmp])\n",
    "print_elapsed(t0, 'concatenating first 10 rows')\n",
    "# NOTE: these are all the groups, not just engineering NOS\n",
    "df_nos_eng_lsh.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all NOS with their LSH group and the transferable NOS\n",
    "df_nos2 = df_nos.join(df_nos_eng_lsh.rename(columns = {'Unnamed: 0': 'lsh_group', \n",
    "                                                      'Avg group similarity': 'lsh_simil',\n",
    "                                                      '1':'index'}).set_index('index'), how = 'left')\n",
    "df_nos2 = df_nos2.join(transferable_nos[['transferable', 'avg similarity2','engineeringness',\n",
    "                                                 'centralities2','we_spread']], how = 'left')\n",
    "df_nos2['lsh_group'].mask(df_nos2['lsh_group'].isnull(), 'na', inplace= True)\n",
    "# remove LSH groups with a low overall similarity\n",
    "th_lsh = 0.7\n",
    "df_nos2['lsh_group'].mask(df_nos2['lsh_simil']<th_lsh, 'na', inplace= True)\n",
    "df_nos2['transferable'].mask(df_nos2['transferable'].isnull(), False, inplace= True)\n",
    "## join with tfidfm and counts\n",
    "#df_nos2 = df_nos2.join(df_nos_eng_tfidfm).join(df_nos_eng_counts)\n",
    "df_nos2.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with group ID and transferable ID\n",
    "df_nos_eng3 = df_nos_eng2.join(df_nos_eng_lsh.rename(columns = {'Unnamed: 0': 'lsh_group', \n",
    "                                                      'Avg group similarity': 'lsh_simil',\n",
    "                                                      '1':'index'}).set_index('index'), how = 'left')\n",
    "df_nos_eng3 = df_nos_eng3.join(transferable_nos[['transferable', 'avg similarity2','engineeringness',\n",
    "                                                 'centralities2','we_spread']], how = 'left')\n",
    "df_nos_eng3['lsh_group'].mask(df_nos_eng3['lsh_group'].isnull(), 'na', inplace= True)\n",
    "# remove LSH groups with a low overall similarity\n",
    "th_lsh = 0.7\n",
    "df_nos_eng3['lsh_group'].mask(df_nos_eng3['lsh_simil']<th_lsh, 'na', inplace= True)\n",
    "df_nos_eng3['transferable'].mask(df_nos_eng3['transferable'].isnull(), False, inplace= True)\n",
    "#df_nos_eng3['transferable'][df_nos_eng3['transferable'].isnull()]= False\n",
    "#df_nos_eng3.sample(n=5)\n",
    "\n",
    "# join with tfidfm and counts\n",
    "df_nos_eng3 = df_nos_eng3.join(df_nos_eng_tfidfm).join(df_nos_eng_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos_eng3[df_nos_eng3['NOS Title'].map(lambda x: 'e-mail' in x)][['NOS Title','SOC4','transferable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first remove legacy and nos without socs\n",
    "not_legacy_flag = (df_nos_eng3['NOS Title'].map(lambda x: 'legacy' not in x)\n",
    "                   ) & (df_nos_eng3.index.map(lambda x: ~(x.endswith('l.pdf'))))\n",
    "with_soc_flag = df_nos_eng3['SOC4'].notna()\n",
    "df_nos_eng4 = df_nos_eng3[not_legacy_flag & with_soc_flag]\n",
    "# now remove transferable NOS too\n",
    "df_nos_eng4 = df_nos_eng4[df_nos_eng4['transferable'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then aggregate by lsh groups and combine by averaging/selecting (so that I don't have to create complicated\n",
    "# rules based on legacies, missing socs and transferability)\n",
    "def aggregate_floats_lsh(x):\n",
    "    x = list(set(list(x)))\n",
    "    #print(x.iloc[0])\n",
    "    if len(x)==1:\n",
    "        x = x[0]\n",
    "    return x\n",
    "\n",
    "def aggregate_suites_lsh(x):\n",
    "    #x = sum(list(x))\n",
    "    print('!',x)\n",
    "    return \"{%s}\" % ', '.join(x) #x = [x]\n",
    "    #return \n",
    "\n",
    "def aggregate_titles_lsh(x):\n",
    "    # join the strings\n",
    "    x = ';'.join(x)\n",
    "    # remove duplicates and join again\n",
    "    x = ';'.join(list(set(x.split(';'))))\n",
    "    return x\n",
    "\n",
    "def aggregate_by_first(x):\n",
    "    return x.iloc[0]\n",
    "\n",
    "'''\n",
    "# questions: \n",
    "1. some LSH groups don't really have a similarity higher than 0.8. Do I only want to merge those that do?\n",
    "Probably yes, but relax the threshold a bit because it's an average similarity for groups with > 2 NOS (0.75?)\n",
    "2. What to do with groups with more than one SOC when assigning requirements?\n",
    "3. How to assign skills cluster to groups? Just using the concatenated lemmas?\n",
    "\n",
    "# TOREMEMBERs: \n",
    "2D. check whether any SOC is not the same (g.SOC4.map(lambda x: len(x)).value_counts()): 3 rows have 2 SOCs.\n",
    "3D. print which rows have two socs\n",
    "'''\n",
    "\n",
    "#print('-')\n",
    "\n",
    "\n",
    "# group details about engineering nos by custom/specific functions\n",
    "cols_of_interest = ['NOS Title','supersuite','One_suite','SOC4','SOC3','SOC2','SOC1','pruned_lemmas', \n",
    "                    'URN', 'lsh_group', 'lsh_simil', 'transferable'] + list(svq_cols) + tfidf_cols + counts_cols\n",
    "\n",
    "# separate singletons NOS from grouped NOS\n",
    "df_nos_eng_singles = df_nos_eng4[df_nos_eng4['lsh_group']=='na'][cols_of_interest]\n",
    "\n",
    "df_nos_eng_grouped = df_nos_eng4[df_nos_eng4['lsh_group']!='na'][cols_of_interest].reset_index().groupby('lsh_group')\n",
    "agg_of_interest= {'URN': aggregate_floats_lsh, 'supersuite': aggregate_by_first, \n",
    "                 'SOC4': aggregate_floats_lsh, 'index': aggregate_floats_lsh,\n",
    "                 'SOC3': aggregate_floats_lsh, 'SOC2': aggregate_floats_lsh,\n",
    "                 'SOC1': aggregate_floats_lsh, 'pruned_lemmas': sum,\n",
    "                  'transferable': aggregate_by_first, #'lsh_group': aggregate_by_first,\n",
    "                'lsh_simil': np.mean}\n",
    "transform_of_interest = {'NOS Title': ','.join, 'One_suite': ','.join}\n",
    "\n",
    "t0 = time.time()\n",
    "gm1 = df_nos_eng_grouped[list(svq_cols)].agg(np.nanmean)\n",
    "g0 = df_nos_eng_grouped[tfidf_cols].agg(np.nanmean)\n",
    "g1 = df_nos_eng_grouped[counts_cols].agg(np.nanmean)\n",
    "g2 = df_nos_eng_grouped[list(agg_of_interest.keys())].agg(agg_of_interest)#.reset_index()\n",
    "g3 = df_nos_eng_grouped['One_suite'].apply(aggregate_titles_lsh)#.reset_index()\n",
    "g4 = df_nos_eng_grouped['NOS Title'].apply(aggregate_titles_lsh)#.reset_index()\n",
    "df_nos_eng_grouped = g2.join(g4, on = 'lsh_group').join(g3, on = 'lsh_group').join(\n",
    "        g1, on='lsh_group').join(g0, on = 'lsh_group').join(gm1, on = 'lsh_group')\n",
    "print_elapsed(t0,'aggregating')\n",
    "\n",
    "# extract the columns of interest (minus lsh group) and concatenate single NOS and groups\n",
    "cols_of_interest = ['NOS Title','supersuite','One_suite','SOC4','SOC3','SOC2','SOC1','pruned_lemmas', \n",
    "                    'URN', 'lsh_simil', 'transferable'] + list(svq_cols) + tfidf_cols + counts_cols\n",
    "df_nos_eng5 = pd.concat([df_nos_eng_singles[cols_of_interest], df_nos_eng_grouped[cols_of_interest]])\n",
    "\n",
    "print('nb NOS x nb columns: ', df_nos_eng5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows with more than one SOC\n",
    "df_nos_eng5[df_nos_eng5['SOC4'].map(lambda x: type(x)==list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity for NOS keywords and SVQ counts\n",
    "eng_keyword_distance = distance.pdist(df_nos_eng5[tfidf_cols].values, metric = 'cosine')\n",
    "eng_svq_distance = distance.pdist(df_nos_eng5[counts_cols].values, metric = 'cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(eng_svq_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "_ = sns.heatmap(distance.squareform(eng_keyword_distance)[:100,:100], yticklabels = df_nos_eng5['NOS Title'][:100])\n",
    "#.shape, distance.squareform(eng_svq_distance).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate cosine distance between tf-idf vectors of the documents\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import fcluster \n",
    "\n",
    "def do_hierarch_clustering2(tfidfm, get_distances = True, method='average', metric = 'cosine', DOPLOTS = True):\n",
    "    t0 = time.time()\n",
    "    N2 = 11914\n",
    "    N = 400 #400*400 = 160000 distance calls per second. For N=21500 -- > 462250000 calls --> 2900*160000 calls \n",
    "    # --> I'm guessing 2900 seconds = 48 minutes (I think it's likely to be more actually)\n",
    "    # 4000*4000 takes approximately 110 seconds. It's double for the cophenet. So, for N=22500, the three functions \n",
    "    # together will take approx 4 hours (I'll do it tonight)\n",
    "\n",
    "    if get_distances:\n",
    "        try:\n",
    "            distances = distance.pdist(tfidfm.todense(), metric = metric) #+np.random.randn(N,N2), metric = 'cosine')\n",
    "            sparse_flag = True\n",
    "        except:\n",
    "            distances = distance.pdist(tfidfm, metric = 'cosine')\n",
    "            sparse_flag = False\n",
    "    else:\n",
    "        distances = tfidfm\n",
    "    print_elapsed(t0, 'calculating cosine distances of tfidf vectors')\n",
    "\n",
    "    #We then build linkage matrix using the distances and specifying the method. For euclidean distances typically\n",
    "    # 'Ward' produces best results. For cosine we can only use 'average' and 'single'.\n",
    "    linkage_matrix = scipy.cluster.hierarchy.linkage(distances,\n",
    "                                                     method = method,\n",
    "                                                     metric = metric)\n",
    "    print_elapsed(t0, 'hierarchical clustering of cosine distances')\n",
    "    #We can test how well the groupings reflect actual distances. If c > 0.75 this is considered to be sufficiently\n",
    "    #good representation\n",
    "    #if get_ditan\n",
    "    #if get:\n",
    "    c, coph_dists = cophenet(linkage_matrix, distances)\n",
    "    #else:\n",
    "    #    c, coph_dists = cophenet(linkage_matrix, \n",
    "    #                         distance.pdist(tfidfm, metric = 'cosine'))\n",
    "\n",
    "    print_elapsed(t0, 'computing the cophenetic correlation')\n",
    "\n",
    "    if DOPLOTS:\n",
    "        fig, ax =plt.subplots(figsize = (5,5))\n",
    "        plt.imshow(scipy.spatial.distance.squareform(distances))\n",
    "        plt.title('cosine distances between suites')\n",
    "        plt.colorbar()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (5,5))\n",
    "        tmp = plt.imshow(scipy.spatial.distance.squareform(coph_dists))\n",
    "        plt.colorbar()\n",
    "    print('The cophenetic coefficient is {:.4f}'.format(c))\n",
    "    return distances, linkage_matrix, c, coph_dists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# perform clustering\n",
    "SAVEHC = False\n",
    "STRATEGY = 'tfidf' #'we' or 'tfidf'\n",
    "SELECT_MODE = 'engineering'\n",
    "cluster_name = SELECT_MODE\n",
    "cluster_name_figs = 'Engineering (using SVQ)'\n",
    "cluster_name_save = 'engineering_svq'\n",
    "#print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "suites_in_clus = {}\n",
    "groups_clus = df_nos_eng5.groupby('One_suite')\n",
    "for name, group in groups_clus:\n",
    "    suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "'''if STRATEGY == 'tfidf':\n",
    "    # this is to get the restricted corpus (to transform, not for fitting)\n",
    "    textfortoken = df_nos_n['pruned_lemmas']\n",
    "    tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "elif STRATEGY == 'we':\n",
    "    tfidfm_n = df_nos_n[all_we_cols].values\n",
    "else:\n",
    "    raise ValueError\n",
    "'''\n",
    "# get labels\n",
    "standard_labels_n2 = list(df_nos_eng5['NOS Title'].values)\n",
    "# use NOS titles for single NOS and group ID for grouped ones?\n",
    "cond = np.array(df_nos_eng5.index.map(lambda x: 'group' in x).values).astype(bool)\n",
    "tmp = df_nos_eng5['NOS Title'].mask(cond, df_nos_eng5.index.values)\n",
    "standard_labels_n = list(tmp.values)\n",
    "#standard_labels_n = list(df_nos_eng5['NOS Title'].mask(\n",
    "#    df_nos_eng5.index.map(lambda x: 'group' in x), df_nos_eng5.index).values)\n",
    "\n",
    "for ix,t in enumerate(standard_labels_n):\n",
    "    if len(t)>500:\n",
    "        # manual correction because of pdf extraction\n",
    "        standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "\n",
    "\n",
    "# use the two similarity matrices into a clustering input\n",
    "WARD = 'substitute'\n",
    "if WARD == 'multiply':\n",
    "    clustering_input = copy.deepcopy(eng_svq_distance)\n",
    "    clustering_input[np.isnan(clustering_input)] = 1 # replace nans with the max distance\n",
    "    clustering_input= eng_keyword_distance * clustering_input #multiply by the usual NOS distances\n",
    "else:\n",
    "    clustering_input = copy.deepcopy(eng_keyword_distance)\n",
    "    clustering_input[~np.isnan(eng_svq_distance)] = eng_svq_distance[~np.isnan(eng_svq_distance)]\n",
    "# perform hierarchical clustering\n",
    "_, linkage_matrix_n, c_n, _ = do_hierarch_clustering2(clustering_input, get_distances = False, \n",
    "                                                               method = 'average',\n",
    "                                                               metric = 'cosine',\n",
    "                                                               DOPLOTS= False)\n",
    "\n",
    "\n",
    "# Plotting the distance between successive clusters: is there a knee?\n",
    "z = linkage_matrix_n[::-1,2]\n",
    "knee = np.diff(z, 2)\n",
    "\n",
    "#fig = plt.figure(figsize = (6,6))\n",
    "fig, ax1 = plt.subplots(figsize = (12,6))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(range(1, len(z)+1), z, 'g-')\n",
    "ax2.plot(range(2, len(linkage_matrix_n)), knee, 'b-')\n",
    "plt.xlim([0,500])\n",
    "ax1.set_ylabel('cluster distance', color='g')\n",
    "ax2.set_ylabel('derivative', color='b')\n",
    "\n",
    "plt.title(cluster_name_figs)\n",
    "goodness = []\n",
    "for i in range(3,100): #len(z)-2):\n",
    "    lr = LinearRegression(normalize = True)\n",
    "    lr = lr.fit(np.arange(1,i+1).reshape(-1, 1), z[:i].reshape(-1, 1))\n",
    "    a1 = lr.score(np.arange(1,i+1).reshape(-1, 1), z[:i].reshape(-1, 1))\n",
    "    lr = LinearRegression(normalize = True)\n",
    "    lr = lr.fit(np.arange(i, len(z)).reshape(-1, 1), z[i:].reshape(-1, 1))\n",
    "    a2 = lr.score(np.arange(i, len(z)).reshape(-1, 1), z[i:].reshape(-1, 1))\n",
    "    goodness.append(np.around(a1 + a2,4))\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "#print(goodness)\n",
    "plt.plot(np.arange(3,100), goodness)#len(z)-2),goodness)\n",
    "plt.title(cluster_name_figs)\n",
    "ixg = np.array(goodness).argmax()+3\n",
    "print('best t-point: ',ixg)\n",
    "\n",
    "num_ideal = np.ceil(len(df_nos_eng5)/10)\n",
    "print('The ideal number of clusters would be: ',num_ideal)\n",
    "num_clust1 = knee.argmax() + 2\n",
    "knee[knee.argmax()] = 0\n",
    "num_clust2 = knee.argmax() + 2\n",
    "\n",
    "if SELECT_MODE == 'engineering':\n",
    "    num_clust = 50 #170 #ixg #max([num_clust1,num_clust2]) #clusters2use[SELECT_MODE][2]\n",
    "else:\n",
    "    if num_clust1 == 2:\n",
    "        num_clust = num_clust2 #2000\n",
    "    elif num_clust2 == 2:\n",
    "        num_clust = num_clust1 #2000\n",
    "    else:\n",
    "        num_clust = min([num_clust1,num_clust2])\n",
    "\n",
    "print('The two peaks are, in order: ',num_clust1, num_clust2)\n",
    "print('The selected num clust is ',num_clust)\n",
    "#num_clust = max([num_clust1,num_clust2])\n",
    "\n",
    "for t in np.arange(0,1,0.05):\n",
    "    labels_n = fcluster(linkage_matrix_n, t, criterion='distance')\n",
    "    n_clust = len(set(labels_n))\n",
    "    if n_clust <= num_clust:\n",
    "        cutting_th_n = t\n",
    "        break\n",
    "# set the threshold manually\n",
    "cutting_th_n = 0.79\n",
    "print('cutting threshold: {}'.format(cutting_th_n))       \n",
    "\n",
    "#Plot the dendrogram (cutting at threshold)\n",
    "#cutting_th_n = 0.6\n",
    "h = .05*len(df_nos_eng5)\n",
    "fig, ax = plt.subplots(figsize=(28, h)) # set size\n",
    "ax = dendrogram(linkage_matrix_n, \n",
    "                labels = [t.capitalize() for t in standard_labels_n], \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = cutting_th_n,\n",
    "               truncate_mode = 'level', p =20)#,\n",
    "               #above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 24)\n",
    "plt.title('Hierarchical clustering for {}'.format(cluster_name_figs), fontsize = 30)\n",
    "#              'Hierarchical Clustering Dendrogram of Selected NOS', fontsize = 20)\n",
    "plt.xlabel('Distance', fontsize = 30)\n",
    "plt.ylabel('NOS title',fontsize = 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVEHC:\n",
    "    plt.savefig(os.path.join(output_dir, \n",
    "                             'all_nos_cut_dendrogram_in_{}_{}_{}_{}_{}.png'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)), \n",
    "                bbox_inches = \"tight\")   \n",
    "\n",
    "# now get and save the clusters\n",
    "labels_n = fcluster(linkage_matrix_n, cutting_th_n, criterion='distance')\n",
    "print('The actual number of clusters is {}'.format(np.unique(labels_n).size))\n",
    "short_df_n = df_nos_eng5.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "\n",
    "short_df_n['hierarchical'] = labels_n\n",
    "short_df_n = short_df_n.set_index('index')\n",
    "if SAVEHC:\n",
    "    short_df_n.to_csv(os.path.join(output_dir, \n",
    "                             'all_nos_cut_labels_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)))\n",
    "\n",
    "# print the result of the cut dendrogram\n",
    "hierarchical_dict= {}\n",
    "L = {}\n",
    "D = {}\n",
    "for ic in range(1,num_clust+1):\n",
    "    tmp_local = short_df_n['NOS Title'][\n",
    "        short_df_n['hierarchical']==ic].values\n",
    "    if len(tmp_local)<3:\n",
    "        continue\n",
    "    hierarchical_dict['{}'.format(ic)] = list(np.unique(tmp_local))\n",
    "    A = distance.squareform(clustering_input)[(short_df_n['hierarchical']==ic).values,:][:,\n",
    "                        (short_df_n['hierarchical']==ic).values]\n",
    "    if A.sum()>0:\n",
    "        A = np.triu(A)\n",
    "        A = A[A[:]>0]\n",
    "    else:\n",
    "        A = np.ones(1)\n",
    "    D['{}'.format(ic)] = np.around(np.mean(A),3)\n",
    "    L['{}'.format(ic)] = (short_df_n['hierarchical']==ic).sum()\n",
    "print('number of clusters with at least three nos is {}'.format(len(hierarchical_dict)))\n",
    "L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "L = L.join(D)\n",
    "if SAVEHC:\n",
    "    L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "        by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                            '/all_nos_cut_clusters_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "                            cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_nos_eng5[df_nos_eng5[list(svq_cols)].notna().sum(axis = 1)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# perform clustering\n",
    "SAVEHC = False\n",
    "STRATEGY = 'tfidf' #'we' or 'tfidf'\n",
    "SELECT_MODE = 'engineering'\n",
    "cluster_name = SELECT_MODE\n",
    "cluster_name_figs = 'Engineering (using SVQ)'\n",
    "cluster_name_save = 'engineering_svq'\n",
    "#print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "df_nos_eng6 = df_nos_eng5[df_nos_eng5[list(svq_cols)].notna().sum(axis = 1)>0]\n",
    "\n",
    "suites_in_clus = {}\n",
    "groups_clus = df_nos_eng6.groupby('One_suite')\n",
    "for name, group in groups_clus:\n",
    "    suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "'''if STRATEGY == 'tfidf':\n",
    "    # this is to get the restricted corpus (to transform, not for fitting)\n",
    "    textfortoken = df_nos_n['pruned_lemmas']\n",
    "    tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "elif STRATEGY == 'we':\n",
    "    tfidfm_n = df_nos_n[all_we_cols].values\n",
    "else:\n",
    "    raise ValueError\n",
    "'''\n",
    "# get labels\n",
    "standard_labels_n2 = list(df_nos_eng6['NOS Title'].values)\n",
    "# use NOS titles for single NOS and group ID for grouped ones?\n",
    "cond = np.array(df_nos_eng6.index.map(lambda x: 'group' in x).values).astype(bool)\n",
    "tmp = df_nos_eng6['NOS Title'].mask(cond, df_nos_eng6.index.values)\n",
    "standard_labels_n = list(tmp.values)\n",
    "#standard_labels_n = list(df_nos_eng5['NOS Title'].mask(\n",
    "#    df_nos_eng5.index.map(lambda x: 'group' in x), df_nos_eng5.index).values)\n",
    "\n",
    "for ix,t in enumerate(standard_labels_n):\n",
    "    if len(t)>500:\n",
    "        # manual correction because of pdf extraction\n",
    "        standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "\n",
    "\n",
    "# use the two similarity matrices into a clustering input\n",
    "WARD = 'svq_only'\n",
    "if WARD == 'svq_only':\n",
    "    clustering_input = distance.pdist(df_nos_eng6[counts_cols].values, metric = 'cosine')\n",
    "else:\n",
    "    raise ValueError\n",
    "# perform hierarchical clustering\n",
    "_, linkage_matrix_n, c_n, _ = do_hierarch_clustering2(clustering_input, get_distances = False, \n",
    "                                                               method = 'average',\n",
    "                                                               metric = 'cosine',\n",
    "                                                               DOPLOTS= False)\n",
    "\n",
    "\n",
    "# Plotting the distance between successive clusters: is there a knee?\n",
    "z = linkage_matrix_n[::-1,2]\n",
    "knee = np.diff(z, 2)\n",
    "\n",
    "#fig = plt.figure(figsize = (6,6))\n",
    "fig, ax1 = plt.subplots(figsize = (12,6))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(range(1, len(z)+1), z, 'g-')\n",
    "ax2.plot(range(2, len(linkage_matrix_n)), knee, 'b-')\n",
    "plt.xlim([0,500])\n",
    "ax1.set_ylabel('cluster distance', color='g')\n",
    "ax2.set_ylabel('derivative', color='b')\n",
    "\n",
    "plt.title(cluster_name_figs)\n",
    "goodness = []\n",
    "for i in range(3,100): #len(z)-2):\n",
    "    lr = LinearRegression(normalize = True)\n",
    "    lr = lr.fit(np.arange(1,i+1).reshape(-1, 1), z[:i].reshape(-1, 1))\n",
    "    a1 = lr.score(np.arange(1,i+1).reshape(-1, 1), z[:i].reshape(-1, 1))\n",
    "    lr = LinearRegression(normalize = True)\n",
    "    lr = lr.fit(np.arange(i, len(z)).reshape(-1, 1), z[i:].reshape(-1, 1))\n",
    "    a2 = lr.score(np.arange(i, len(z)).reshape(-1, 1), z[i:].reshape(-1, 1))\n",
    "    goodness.append(np.around(a1 + a2,4))\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "#print(goodness)\n",
    "plt.plot(np.arange(3,100), goodness)#len(z)-2),goodness)\n",
    "plt.title(cluster_name_figs)\n",
    "ixg = np.array(goodness).argmax()+3\n",
    "print('best t-point: ',ixg)\n",
    "\n",
    "num_ideal = np.ceil(len(df_nos_eng6)/10)\n",
    "print('The ideal number of clusters would be: ',num_ideal)\n",
    "num_clust1 = knee.argmax() + 2\n",
    "knee[knee.argmax()] = 0\n",
    "num_clust2 = knee.argmax() + 2\n",
    "\n",
    "if SELECT_MODE == 'engineering':\n",
    "    num_clust = 22 #170 #ixg #max([num_clust1,num_clust2]) #clusters2use[SELECT_MODE][2]\n",
    "else:\n",
    "    if num_clust1 == 2:\n",
    "        num_clust = num_clust2 #2000\n",
    "    elif num_clust2 == 2:\n",
    "        num_clust = num_clust1 #2000\n",
    "    else:\n",
    "        num_clust = min([num_clust1,num_clust2])\n",
    "\n",
    "print('The two peaks are, in order: ',num_clust1, num_clust2)\n",
    "print('The selected num clust is ',num_clust)\n",
    "#num_clust = max([num_clust1,num_clust2])\n",
    "\n",
    "for t in np.arange(0.0,1.0,0.05):\n",
    "    labels_n = fcluster(linkage_matrix_n, t, criterion='distance')\n",
    "    n_clust = len(set(labels_n))\n",
    "    if n_clust <= num_clust:\n",
    "        cutting_th_n = t\n",
    "        break\n",
    "print('cutting threshold: {}'.format(cutting_th_n))       \n",
    "\n",
    "#Plot the dendrogram (cutting at threshold)\n",
    "#cutting_th_n = 0.6\n",
    "h = .07*len(df_nos_eng6)\n",
    "fig, ax = plt.subplots(figsize=(22, h)) # set size\n",
    "ax = dendrogram(linkage_matrix_n, \n",
    "                labels = [t.capitalize() for t in standard_labels_n], \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = cutting_th_n,\n",
    "               truncate_mode = 'level', p =20)#,\n",
    "               #above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 24)\n",
    "plt.title('Hierarchical clustering for {}'.format(cluster_name_figs), fontsize = 30)\n",
    "#              'Hierarchical Clustering Dendrogram of Selected NOS', fontsize = 20)\n",
    "plt.xlabel('Distance', fontsize = 30)\n",
    "plt.ylabel('NOS title',fontsize = 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVEHC:\n",
    "    plt.savefig(os.path.join(output_dir, \n",
    "                             'svq_nos_cut_dendrogram_in_{}_{}_{}_{}_{}.png'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)), \n",
    "                bbox_inches = \"tight\")   \n",
    "\n",
    "# now get and save the clusters\n",
    "labels_n = fcluster(linkage_matrix_n, cutting_th_n, criterion='distance')\n",
    "print('The actual number of clusters is {}'.format(np.unique(labels_n).size))\n",
    "short_df_n = df_nos_eng6.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "\n",
    "short_df_n['hierarchical'] = labels_n\n",
    "short_df_n = short_df_n.set_index('index')\n",
    "if SAVEHC:\n",
    "    short_df_n.to_csv(os.path.join(output_dir, \n",
    "                             'svq_nos_cut_labels_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)))\n",
    "\n",
    "# print the result of the cut dendrogram\n",
    "hierarchical_dict= {}\n",
    "L = {}\n",
    "D = {}\n",
    "for ic in range(1,num_clust+1):\n",
    "    tmp_local = short_df_n['NOS Title'][\n",
    "        short_df_n['hierarchical']==ic].values\n",
    "    if len(tmp_local)<0:\n",
    "        continue\n",
    "    hierarchical_dict['{}'.format(ic)] = list(tmp_local)\n",
    "    A = distance.squareform(clustering_input)[(short_df_n['hierarchical']==ic).values,:][:,\n",
    "                        (short_df_n['hierarchical']==ic).values]\n",
    "    if A.sum()>0:\n",
    "        A = np.triu(A)\n",
    "        A = A[A[:]>0]\n",
    "    else:\n",
    "        A = np.ones(1)\n",
    "    D['{}'.format(ic)] = np.around(np.mean(A),3)\n",
    "    L['{}'.format(ic)] = (short_df_n['hierarchical']==ic).sum()\n",
    "L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "L = L.join(D)\n",
    "if SAVEHC:\n",
    "    L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "        by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                            '/svq_nos_cut_clusters_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "                            cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svq_semi_manual_names = ['power sector', 'bus and coach',\n",
    "                        'construction and civil engineering', 'conductor engineering', 'land based',\n",
    "                        'construction site', 'signals', 'electrification', 'process engineering maintenance',\n",
    "                        'fabrication and welding', 'performing engineering operations','mechanical manifacturing',\n",
    "                        'marine engineering','engineering maintenance','engineering tech support',\n",
    "                         'engineering manufacture',\n",
    "                         'automotive engineering', 'aeronautical engineering','food and drink','electrical power',\n",
    "                        'engineering surveying','building services']\n",
    "\n",
    "svq_semi_manual_groups = [\n",
    "    ['20181212 R227 04 EPE Substation Plant.xlsx',\n",
    " '20181212 R229 04 EPE Overhead Lines.xlsx',\n",
    " '20181212 R228 04 EPE Underground Cables.xlsx'] ,\n",
    "['SVQ 2   Bus and Coach Engineering and Maintenance- Electrical at SCQF Level 5.xlsx',\n",
    " 'SVQ 2 Bus and Coach Engineering and Maintenance- Body Cladding at SCQF Level 5.xlsx',\n",
    " 'SVQ 2 \\xa0 Bus and Coach Engineering and Maintenance- Mechanical at SCQF Level 5.xlsx',\n",
    " 'SVQ 3   Bus and Coach Engineering and Maintenance- Electrical at SCQF Level 6.xlsx',\n",
    " 'SVQ 3   Bus and Coach Engineering and Maintenance- Mechanical at SCQF Level 6.xlsx',\n",
    " 'SVQ 3 Bus and Coach Engineering and Maintenance- Body Cladding at SCQF Level 6.xlsx',\n",
    " 'SVQ 3 \\xa0 Bus and Coach Engineering and Maintenance- Electrical-Mechanical at SCQF Level 6.xlsx'] ,\n",
    "['20181212 GP36 45 QS SVQ C&CEO.xlsx',\n",
    " 'GM84 22 Construction and Civil Engineering Operations SCQF L5.xlsx',\n",
    " '20170703 GM83 21 QS SVQ Construction CES L4.xlsx'] ,\n",
    "['GM34 22 QS Reaccred Conductor Engineering.xlsx'] ,\n",
    "['GK3G23_Landbased_eng_ops_agriculture_April.xlsx',\n",
    " 'GK3H23_Landbased_eng_ops_arboriculture_forestry_April.xlsx',\n",
    " 'GK3J23_Landbased_eng_ops_ground_care_April.xlsx',\n",
    " 'GK42 22  Land-based Engineering (ArboricultureForestry) 1.xlsx',\n",
    " 'GK43 22  Land-based Engineering (Ground Care) 1.xlsx',\n",
    " 'GK44 22 Land-based Engineering (Agriculture) 1.xlsx',\n",
    " 'GK45 23 Land-based Engineering (ArboricultureForestry) 1.xlsx',\n",
    " 'GK46 23  Land-based Engineering (Ground Care) 1.xlsx',\n",
    " 'GK47 23  Land-based Engineering (Agriculture) 1.xlsx',\n",
    " 'GP6H 22 SVQ in LB Eng (Ground Care) at SCQF Level 5.xlsx',\n",
    " 'GP6J 23 SVQ in LB Eng (Ground Care) at SCQF Level 6.xlsx',\n",
    " 'GP6K 22 SVQ in LB Eng (Agriculture) at SCQF Level 5.xlsx',\n",
    " 'GP6L 23 SVQ in LB Eng (Agriculture) at SCQF Level 6.xlsx',\n",
    " 'GP6M 22 SVQ in LB Eng (Arb and For) at SCQF level 5.xlsx',\n",
    " 'GP6N 23 SVQ in LB Eng (Arb.Forestry) at SCQF Level 6.xlsx'] ,\n",
    "['GJ19 24 SVQ Cons Site Mgmt Bld and Civ Eng.xlsx',\n",
    " 'GJ1C 23 SVQ Cons Site Superv Build & Civ Eng.xlsx'] ,\n",
    "['201.02.26 SVQ2 Signal and Telecoms.xlsx',\n",
    " '201.02.26 SVQ3 Signal and Telecoms.xlsx'] ,\n",
    "['2015.02.26 SVQ 2  Electrification.xlsx',\n",
    " '2015.02.26 SVQ 3  Electrification.xlsx'] ,\n",
    "['GD0J 23 PEM Instrument & Control SCQF L7.xlsx',\n",
    " 'GP75 47 SVQ in Process Eng Maint Instrument and Control at SCQF level 7.xlsx',\n",
    " 'GD0D 22 SVQ in Process Engineering Maintenance (Electrical) at SCQF level 5.xlsx',\n",
    " 'GD0E 22 PEM Instrument & Control SCQF L5.xlsx',\n",
    " 'GD0F 22 SVQ in Process Engineering Maintenance (Mechanical) at SCQF level 5.xlsx',\n",
    " 'GD0G 23 SVQ_PEM_Extension_SCQFL7 (SQA).xlsx',\n",
    " 'GD0H 23 SVQ in Process Engineering Maintenance (Mechanical) at SCQF level 7.xlsx',\n",
    " 'GP74 47 SVQ in Process Engineering Maintenance Electrical at SCQF level 7.xlsx',\n",
    " 'GP76 47 SVQ in Process Engineering Maintenance Mechanical at SCQF level 7.xlsx'] ,\n",
    "['2016.03.30 GL2N 22 SVQ 2 Fabrication and Welding Engineering QS (EAL).xlsx',\n",
    " '2016.04.20 GL3N 23 Reaccred SVQ Fabrication & Welding (EAL).xlsx',\n",
    " '20160608 QS Reaccred SVQ F&WE (SQA) GL6F 23.xlsx',\n",
    " '20180516 QS Accred SVQ Fab&Weld (QFI) GP07 46.xlsx'],\n",
    "['GL6E 22 SVQ Performing Engineering Oprations at SCQF L5.xlsx',\n",
    " 'GM0V 21 SVQ in Performing Engineering Operations at SCQF L4.xlsx',\n",
    " 'SVQ PEO2 Amend SCQF L5 AC2 EAL.xlsx'] ,\n",
    "['GM1V 46 Mechanical Manufacturing Engineering Level 6.xlsx'] ,\n",
    "['2016.04.20 GL3P 23 Reaccred SVQ Marine Engineering (EAL).xlsx'] ,\n",
    "['2016.03.30 GL2M 23 SVQ 3 Engineering Maintenance Reaccred (EAL).xlsx'] ,\n",
    "['2016.04.20 GL3M 23 Reaccred SVQ Engineering Tech Support (EAL).xlsx'] ,\n",
    "['GJ9D 24 Engineering Manufacture.xlsx'] ,\n",
    "['GJ9C 23 Automotive Engineering.xlsx'] ,\n",
    "['20180613 QS SVQ Aeronautical Engineering L6 (EAL).xlsx'] ,\n",
    "['2016.03.09  SVQ 3 Food and Drink Ops Eng Maint (GL46 23) (SQA)v2.xlsx'] ,\n",
    "['R094 04 Dip Elec Power Eng - Wind Turb Ops and Maint SCQF 6.xlsx',\n",
    " 'Certificate in Electrical Power Engineering - Distribution and Transmission (Technical Knowledge) at SCQF Level 5.xlsx',\n",
    " 'R20104 Diploma in Electrical Power Engineering.xlsx',\n",
    " 'R095 04 Dip Elec Power Eng - Wind Turb Maint (Tech Kno) SCQF 6.xlsx'] ,\n",
    "['2015.04.01  SVQ 2 Eng Surv Ops.xlsx'] ,\n",
    "['2015.07.08 GK9M 23 SVQ3 SMCBSE SQA.xlsx']]\n",
    "print(len(svq_semi_manual_names))\n",
    "print(len(svq_semi_manual_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svq_semi_groups_dict = {}\n",
    "svq_semi_groups_dict2 = {}\n",
    "for ix,g in enumerate(svq_semi_manual_groups):\n",
    "    # save units as they are\n",
    "    svq_semi_groups_dict[svq_semi_manual_names[ix]] = list(np.unique(flatten_lol(svq_data.loc[g]['units'].values)))\n",
    "    # crosswalk to NOS based on joining above\n",
    "    tmp = []\n",
    "    for i in list(np.unique(flatten_lol(svq_data.loc[g]['units'].values))):\n",
    "        if i in units_to_nos_last:\n",
    "            tmp.append(units_to_nos_last[i])\n",
    "    svq_semi_groups_dict2[svq_semi_manual_names[ix]] = list(np.unique(tmp))\n",
    "pd.DataFrame.from_dict(svq_semi_groups_dict, orient = 'index').T.to_csv(\n",
    "    output_dir + '/svq_semi_manual_groups_units.csv')\n",
    "pd.DataFrame.from_dict(svq_semi_groups_dict2, orient = 'index').T.to_csv(\n",
    "    output_dir + '/svq_semi_manual_groups_crosswalked_nos.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply HDBSCAN to similarity matrix overriden with the similarity from the pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform clustering\n",
    "\n",
    "STRATEGY = 'tfidf' #'we' or 'tfidf'\n",
    "SELECT_MODE = 'engineering'\n",
    "cluster_name = SELECT_MODE\n",
    "cluster_name_figs = 'Engineering (using SVQ)'\n",
    "cluster_name_save = 'engineering_svq'\n",
    "#print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "suites_in_clus = {}\n",
    "#groups_clus = df_nos_eng5.groupby('One_suite')\n",
    "#for name, group in groups_clus:\n",
    "#    suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "'''if STRATEGY == 'tfidf':\n",
    "    # this is to get the restricted corpus (to transform, not for fitting)\n",
    "    textfortoken = df_nos_n['pruned_lemmas']\n",
    "    tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "elif STRATEGY == 'we':\n",
    "    tfidfm_n = df_nos_n[all_we_cols].values\n",
    "else:\n",
    "    raise ValueError\n",
    "'''\n",
    "# get labels\n",
    "standard_labels_hdb2 = list(df_nos_eng5['NOS Title'].values)\n",
    "# use NOS titles for single NOS and group ID for grouped ones?\n",
    "cond = np.array(df_nos_eng5.index.map(lambda x: 'group' in x).values).astype(bool)\n",
    "tmp = df_nos_eng5['NOS Title'].mask(cond, df_nos_eng5.index.values)\n",
    "standard_labels_hdb = list(tmp.values)\n",
    "#standard_labels_n = list(df_nos_eng5['NOS Title'].mask(\n",
    "#    df_nos_eng5.index.map(lambda x: 'group' in x), df_nos_eng5.index).values)\n",
    "\n",
    "for ix,t in enumerate(standard_labels_hdb):\n",
    "    if len(t)>500:\n",
    "        # manual correction because of pdf extraction\n",
    "        standard_labels_hdb[ix] = standard_labels_hdb[ix][:50]\n",
    "\n",
    "\n",
    "# use the two similarity matrices into a clustering input\n",
    "WARD = 'substitute'\n",
    "if WARD == 'multiply':\n",
    "    clustering_input = copy.deepcopy(eng_svq_distance)\n",
    "    clustering_input[np.isnan(clustering_input)] = 1 # replace nans with the max distance\n",
    "    clustering_input= eng_keyword_distance * clustering_input #multiply by the usual NOS distances\n",
    "else:\n",
    "    clustering_input = copy.deepcopy(eng_keyword_distance)\n",
    "    clustering_input[~np.isnan(eng_svq_distance)] = eng_svq_distance[~np.isnan(eng_svq_distance)]\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(metric = 'precomputed')#, prediction_data = True)\n",
    "clusterer = clusterer.fit(distance.squareform(clustering_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nb of clusters, ', 'nb of singletons, ', 'nb of data points')\n",
    "np.unique(clusterer.labels_).max(), np.sum(clusterer.labels_ == -1), clusterer.labels_.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clusterer.probabilities_)\n",
    "plt.ylabel('NOS counts')\n",
    "plt.xlabel('Certainty of assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of the hdbscan\n",
    "SAVEHDB = True\n",
    "\n",
    "short_df_hdb = df_nos_eng5.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "\n",
    "short_df_hdb['hierarchical'] = clusterer.labels_\n",
    "short_df_hdb = short_df_hdb.set_index('index')\n",
    "if SAVEHDB:\n",
    "    short_df_hdb.to_csv(os.path.join(output_dir, \n",
    "                             'hdbscan_nos_cut_labels_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)))\n",
    "\n",
    "\n",
    "hdb_dict= {}\n",
    "L = {}\n",
    "D = {}\n",
    "for ic in np.unique(clusterer.labels_):\n",
    "    if ic<0:\n",
    "        continue\n",
    "    # get ID of nos inside cluster\n",
    "    tmp_local = [t for ix,t in enumerate(standard_labels_hdb2) if clusterer.labels_[ix] == ic]\n",
    "    if len(tmp_local)<0:\n",
    "        continue\n",
    "    hdb_dict['{}'.format(ic)] = sorted(list(tmp_local))\n",
    "    A = distance.squareform(clustering_input)[(short_df_hdb['hierarchical']==ic).values,:][:,\n",
    "                        (short_df_hdb['hierarchical']==ic).values]\n",
    "    if A.sum()>0:\n",
    "        A = np.triu(A)\n",
    "        A = A[A[:]>0]\n",
    "    else:\n",
    "        A = np.ones(1)\n",
    "    D['{}'.format(ic)] = np.around(np.mean(A),3)\n",
    "    L['{}'.format(ic)] = len(tmp_local)\n",
    "L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "L = L.join(D)\n",
    "if SAVEHDB:\n",
    "    L.join(pd.DataFrame.from_dict(hdb_dict, orient = 'index')).sort_values(\n",
    "        by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                            '/hdbscan_nos_cut_clusters_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "                            cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD))\n",
    "    \n",
    "#hdb_dict = {}\n",
    "#for i in np.unique(clusterer.labels_):\n",
    "#    if i<0:\n",
    "#        continue\n",
    "#    hdb_dict[i]= [t for ix,t in enumerate(standard_labels_hdb2) if clusterer.labels_[ix] == i]\n",
    "#pd.DataFrame.from_dict(hdb_dict,orient = 'index').T.to_csv(output_dir + \n",
    "#                                                           '/hdbscan_result_all_nos_tfidf_substitute.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the two similarity matrices into a clustering input\n",
    "WARD = 'substitute'\n",
    "if WARD == 'multiply':\n",
    "    clustering_input = copy.deepcopy(eng_svq_distance)\n",
    "    clustering_input[np.isnan(clustering_input)] = 1 # replace nans with the max distance\n",
    "    clustering_input= eng_keyword_distance * clustering_input #multiply by the usual NOS distances\n",
    "else:\n",
    "    clustering_input = copy.deepcopy(eng_keyword_distance)\n",
    "    clustering_input[~np.isnan(eng_svq_distance)] = eng_svq_distance[~np.isnan(eng_svq_distance)]\n",
    "\n",
    "# get the labels information\n",
    "standard_labels_c2 = list(df_nos_eng5['NOS Title'].values)\n",
    "# use NOS titles for single NOS and group ID for grouped ones?\n",
    "cond = np.array(df_nos_eng5.index.map(lambda x: 'group' in x).values).astype(bool)\n",
    "tmp = df_nos_eng5['NOS Title'].mask(cond, df_nos_eng5.index.values)\n",
    "standard_labels_c = list(tmp.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nos_cliques = []\n",
    "for th in np.arange(0.75,0.91,0.05):\n",
    "    short_df_cliques = df_nos_eng5.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "    t0 = time.time()\n",
    "    similarity_graph = nx.convert_matrix.from_numpy_array((1 - distance.squareform(clustering_input))>th)\n",
    "    print_elapsed(t0, 'building the graph')\n",
    "    nos_cliques = list(nx.find_cliques(similarity_graph))\n",
    "    nos_cliques = [t for t in nos_cliques if len(t)>4]\n",
    "    all_nos_cliques.append(nos_cliques)\n",
    "    print_elapsed(t0, 'finding cliques')\n",
    "    print(th,len(all_nos_cliques[-1]))\n",
    "    cliques_dict = {}\n",
    "    for i, clique in enumerate(nos_cliques):\n",
    "        cliques_dict[i] = sorted([standard_labels_c2[t] for t in clique])\n",
    "        short_df_cliques[i] = False\n",
    "        short_df_cliques[i].iloc[[t for t in clique]] = True\n",
    "    pd.DataFrame.from_dict(cliques_dict, orient = 'index').T.to_csv(output_dir + \n",
    "                                                                    '/clusters_by_clique_th{:.2f}.csv'.format(th))\n",
    "    short_df_cliques.to_csv(output_dir + '/labels_by_clique_th{:.2f}.csv'.format(th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist((1 - clustering_input).ravel())\n",
    "print(np.percentile((1 - clustering_input).ravel(),99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.squareform(clustering_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loading a pre-trained glove model into gensim\n",
    "# model should have already been loaded in bg_load_prepare_and_run. \n",
    "# If not, load it here    \n",
    "\n",
    "LOADGLOVE = False\n",
    "if LOADGLOVE:\n",
    "    print('Loading glove model')\n",
    "    t0 = time.time()\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.{}.txt'.format(WHICH_GLOVE)))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors\n",
    "    # from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "    print_elapsed(t0, 'loading the glove model')\n",
    "\n",
    "    vector_matrix = model.vectors\n",
    "    list_of_terms = model.index2word\n",
    "\n",
    "    lookup_terms = [convert_from_undersc(elem) for elem in list_of_terms]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
