{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions and snippets of code that are useful for analysing text. Most of the techniques used are unsupervised. Functions are defined up front and then used in sections below.\n",
    "\n",
    "This notebook is to apply:\n",
    "- Tokenizers (based on n-grams and 'as_is')\n",
    "- Calculating distance\n",
    "- Hierarchical clustering and plotting\n",
    "- K-means clustering\n",
    "- LSH\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_with_pos(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i, pos_to_wornet_dict[p]) if i not in keep_asis else i for i,p in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_pruned(x, pofs = 'nv'):\n",
    "    if pofs == 'nv':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['V','N']]\n",
    "    elif pofs == 'n':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['N']]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return lemmatise_with_pos(tags)\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse html\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "#HTML Parser Methods\n",
    "#Initializing lists\n",
    "    lsData = list()\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.lsData.append(data)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return ''.join(self.lsData)\n",
    "\n",
    "           \n",
    "def strip_tags(some_html):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes html tags.\n",
    "    Returns a string.\n",
    "    \"\"\"\n",
    "    s = MyHTMLParser()\n",
    "    s.lsData = list()\n",
    "    s.feed(some_html)\n",
    "    data = s.get_data()\n",
    "    s.reset\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'nv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/'\n",
    "\n",
    "# to make the glove model file compatible with gensim\n",
    "#for dim in ['25','50','100','200']:\n",
    "##    glove_file = os.path.join(glove_dir,'glove.twitter.27B.{}d.txt'.format(dim))\n",
    "#    tmp_file = os.path.join(glove_dir, 'word2vec.glove.twitter.27B.{}d.txt'.format(dim) )\n",
    "#    _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "LOADGLOVE = False\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.glove.6B.100d.txt'))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "#r2 = requests.get(\"https://www.instituteforapprenticeships.org/api/fullstandards/\")\n",
    "#df_api= pd.DataFrame(r2.json())\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another column where the texts are lemmatised properly\n",
    "t0 = time.time()\n",
    "df_nos['pruned_lemmas'] = df_nos['tagged_tokens'].map(lambda x: lemmatise_pruned(x,pofs))\n",
    "print(time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„',\"'m\", \"'re\", '£',\n",
    "                    '&', '1', '@'])\n",
    "stopwords0 += tuple(set(list(df_nos['Developed By'])))\n",
    "stopwords0 += tuple(['cosvr'])\n",
    "# more stopwords from some suites of interest\n",
    "stopwords0 += tuple(['pdf','dc','db','gov','auspex','december','wa','non','go','get','ask',\n",
    "                    'thing','ha','hm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define more functions on how to create the TfIdf vectoriser and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create your TFidfVectorizer model. This doesn't depend on whether it's used on suites or NOS. However,\n",
    "# it does require that the docs collection is already given as a collection of tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, we will use a different tokenizer\n",
    "def define_tfidf(params, stopwords):\n",
    "    if params['ngrams'] == 'bi':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,2), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    elif params['ngrams'] == 'tri':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,3), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    else:\n",
    "        # unigrams is the default\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, collect the text to transform\n",
    "def combine_nos_text(df_nos, col = 'pruned'):\n",
    "    all_joint_tokens = []\n",
    "    # group by suites and concatenate all docs in it\n",
    "    row_names = []\n",
    "    for name, group in df_nos.groupby('One_suite'):\n",
    "        row_names.append(name)\n",
    "        joint_tokens = []\n",
    "        for idoc in group[col].index:\n",
    "            joint_tokens += group[col].loc[idoc]\n",
    "        all_joint_tokens.append(joint_tokens)\n",
    "    # return a dataframe\n",
    "    return pd.DataFrame({'tokens': all_joint_tokens}, index = row_names)\n",
    "\n",
    "def get_tfidf_matrix(params, df_nos, tfidf, col = 'pruned'):\n",
    "    # Note: this can simply be used to get the tfidf transform, by setting bywhich=docs and any mode\n",
    "    t0 = time.time()\n",
    "    # first, get the dataframe of tokens\n",
    "    if params['bywhich'] == 'docs':\n",
    "        textfortoken = df_nos[col]\n",
    "        \n",
    "    elif params['bywhich'] == 'suites':\n",
    "        if params['mode'] == 'meantfidf':\n",
    "            textfortoken = df_nos[col]\n",
    "                \n",
    "        elif params['mode'] == 'combinedtfidf':\n",
    "            # note that this is the only case where the tfidf min and max are computed considering the number of \n",
    "            # suites as the number of elements in the collection.\n",
    "            # TODO: allow for the alternative case, where the transform is computed on individual NOS and then \n",
    "            # applied to the joint tokens\n",
    "            textfortoken = combine_nos_text(df_nos, col)['tokens']\n",
    "    \n",
    "    # apply tfidf transform to the tokenised text\n",
    "    tfidfm = tfidf.fit_transform(textfortoken)\n",
    "    \n",
    "    # if the average is needed, compute it and overwrite the matrix. Note that the step above is still needed to\n",
    "    # initialise the tfidf transform with the proper features and stopwords\n",
    "    if (params['bywhich'] == 'suites') and (params['mode'] =='meantfidf'):\n",
    "        row_names = df_nos['One_suite'].value_counts().index.values\n",
    "        tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "        for name, group in df_nos.groupby('One_suite'):\n",
    "            tmp = get_mean_tfidf(group['pruned'], tfidf)\n",
    "            tfidfm[igroup] = tmp\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    print_elapsed(t0, 'computing the feature vector')\n",
    "    return tfidfm, feature_names, tfidf, textfortoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the file with the list of super-suites and match the suites listed inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_suites_files=  '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/NOS_Suite_Priority.xlsx'\n",
    "super_suites_names = ['Engineering','Management','FinancialServices','Construction']\n",
    "all_super_suites = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_super_suites[which_super_suite] = pd.read_excel(super_suites_files, sheet_name = which_super_suite)\n",
    "    all_super_suites[which_super_suite]['NOS Suite name'] = all_super_suites[which_super_suite]['NOS Suite name'].map(\n",
    "        lambda x: x.replace('(','').replace('(','').replace('&','and').strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "all_matches = {}\n",
    "all_match_names = {}\n",
    "#match_name = []\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_matches[which_super_suite] = []\n",
    "    for suite in all_super_suites[which_super_suite]['NOS Suite name'].values:\n",
    "        # do manually some selected suites\n",
    "        if 'insurance claims' in suite:\n",
    "            tmp = standard_labels.index('general insurance')\n",
    "            all_matches[which_super_suite].append(tmp)\n",
    "            continue\n",
    "        # for the \"management and leadership marketing 2013\" both marketing and marketing 2013 would fit,\n",
    "        # but I'm only taking the latter\n",
    "        # find a fuzzy match between \n",
    "        out = process.extract(suite, standard_labels, limit=3)\n",
    "        if len(out) and out[0][1]>89:\n",
    "            # note: most of them are above 96% similarity (only one is 90%)\n",
    "            tmp = standard_labels.index(out[0][0])\n",
    "            #print(suite, out[0])\n",
    "            if tmp not in all_matches[which_super_suite]:\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "            else:\n",
    "                if suite == 'installing domestic fascia, soffit, and bargeboards':\n",
    "                    # this suite is kind of a duplicate - I aggregated it in my suites list\n",
    "                    continue\n",
    "                tmp = standard_labels.index(out[2][0])\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "                print(out[0][0],',',out[1][0],',',out[2][0],',',suite)\n",
    "        else:\n",
    "            print(suite, ' not found')\n",
    "            print(out)\n",
    "            print('\\n')\n",
    "    print(len(all_matches[which_super_suite]),len(all_super_suites[which_super_suite]))\n",
    "    all_match_names[which_super_suite] = [standard_labels[t] for t in all_matches[which_super_suite]]\n",
    "    #print(super_suites['NOS Suite name'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships between standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import fcluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we only want to cluster one suite\n",
    "#steel_construction = df_nos[df_nos['One_suite'] == 'steelfix construction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEHC = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate cosine distance between tf-idf vectors of the documents\n",
    "\n",
    "def do_hierarch_clustering(tfidfm, DOPLOTS = True):\n",
    "    t0 = time.time()\n",
    "    N2 = 11914\n",
    "    N = 400 #400*400 = 160000 distance calls per second. For N=21500 -- > 462250000 calls --> 2900*160000 calls \n",
    "    # --> I'm guessing 2900 seconds = 48 minutes (I think it's likely to be more actually)\n",
    "    # 4000*4000 takes approximately 110 seconds. It's double for the cophenet. So, for N=22500, the three functions \n",
    "    # together will take approx 4 hours (I'll do it tonight)\n",
    "\n",
    "    try:\n",
    "        distances = distance.pdist(tfidfm.todense(), metric = 'cosine') # + np.random.randn(N,N2), metric = 'cosine')\n",
    "        sparse_flag = True\n",
    "    except:\n",
    "        distances = distance.pdist(tfidfm, metric = 'cosine')\n",
    "        sparse_flag = False\n",
    "    print_elapsed(t0, 'calculating cosine distances of tfidf vectors')\n",
    "\n",
    "    #We then build linkage matrix using the distances and specifying the method. For euclidean distances typically\n",
    "    # 'Ward' produces best results. For cosine we can only use 'average' and 'single'.\n",
    "    linkage_matrix = scipy.cluster.hierarchy.linkage(distances,\n",
    "                                                     method = 'average',\n",
    "                                                     metric = 'cosine')\n",
    "    print_elapsed(t0, 'hierarchical clustering of cosine distances')\n",
    "    #We can test how well the groupings reflect actual distances. If c > 0.75 this is considered to be sufficiently\n",
    "    #good representation\n",
    "    if sparse_flag:\n",
    "        c, coph_dists = cophenet(linkage_matrix, \n",
    "                             distance.pdist(tfidfm.todense(), metric = 'cosine'))\n",
    "    else:\n",
    "        c, coph_dists = cophenet(linkage_matrix, \n",
    "                             distance.pdist(tfidfm, metric = 'cosine'))\n",
    "\n",
    "    print_elapsed(t0, 'computing the cophenetic correlation')\n",
    "\n",
    "    if DOPLOTS:\n",
    "        fig, ax =plt.subplots(figsize = (5,5))\n",
    "        plt.imshow(scipy.spatial.distance.squareform(distances))\n",
    "        plt.title('cosine distances between suites')\n",
    "        plt.colorbar()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (5,5))\n",
    "        tmp = plt.imshow(scipy.spatial.distance.squareform(coph_dists))\n",
    "        plt.colorbar()\n",
    "    print('The cophenetic coefficient is {:.4f}'.format(c))\n",
    "    return distances, linkage_matrix, c, coph_dists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters for features extraction\n",
    "\n",
    "ngrams : uni/bi/tri\n",
    "\n",
    "tfidf thresholds: min and max percentage\n",
    "\n",
    "which parts of speech were selected before\n",
    "\n",
    "whether we are working at the level of suites or of invidual NOS, and how we aggregate NOS to form the suit level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the parameters if needed\n",
    "params = {}\n",
    "params['ngrams'] = 'bi'\n",
    "params['pofs'] = 'nv'\n",
    "params['tfidf_min'] = 3\n",
    "params['tfidf_max'] = 0.4\n",
    "\n",
    "params['bywhich'] = 'suites' #'docs' #'suites'\n",
    "params['mode'] = 'combinedtfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "\n",
    "# get the transform tfidf\n",
    "tfidf = define_tfidf(params, stopwords0)\n",
    "\n",
    "# get the matrix again (even though if the parameters stay the same, this one is the same still)\n",
    "# get the features\n",
    "tfidfm, feature_names, tfidf, textfortoken = get_tfidf_matrix(params, df_nos, tfidf, \n",
    "                                                              col = 'pruned_lemmas')\n",
    "\n",
    "# get labels\n",
    "if params['bywhich'] == 'suites':\n",
    "    standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "else:\n",
    "    standard_labels = list(df_nos['NOS Title'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check best features in one suite\n",
    "suites2check = ['pension scheme trusteeship','pension trustee board secretaryship',\n",
    "    'secretary to the trustees of pension funds', \n",
    "    'trustee and management committee', 'stop1','investment operations',\n",
    "    'investment strategy and management','providing financial advice and financial planning',\n",
    "    'generic financial advice','providing advice on savings for retirement','stop2',\n",
    "    'electricity network control engineer','gas network construction',\n",
    "    'gas networks engineering management','leakage detection and control',\n",
    "    'multi utility network construction','network construction operations',\n",
    "    'utilities network planning and management','utility infrastructure management']\n",
    "tmp_df = []\n",
    "if True:\n",
    "    standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "    for i,suite0 in enumerate(suites2check):\n",
    "        print(suite0)\n",
    "        if suite0[:4] == 'stop':\n",
    "            tmp_list = ['-']*50\n",
    "        else:\n",
    "            #standard_labels2 = textfortoken.index.values\n",
    "            s_idx = standard_labels.index(suite0) #'business continuity management 2013')\n",
    "            TF= tfidfm[s_idx,:].T.todense()\n",
    "            sort_ix = np.array(np.argsort(TF.T).ravel()).T\n",
    "            sort_ix = sort_ix[::-1]\n",
    "            tmp_list = [feature_names[ix[0]] for ix in sort_ix[:50]]\n",
    "            #print(tmp_list)\n",
    "        if i == 0:\n",
    "            tmp_df = pd.DataFrame(tmp_list, columns = [suite0])\n",
    "        else:\n",
    "            tmp_df = tmp_df.join(pd.DataFrame(tmp_list, columns = [suite0]))\n",
    "        #for ix in sort_ix[:50]: #,t in enumerate(feature_names[sort_ix]):\n",
    "        #    ix = ix[0]\n",
    "        #    if TF[ix]>0.05:\n",
    "        #        print(feature_names[ix])\n",
    "        #        break\n",
    "        print('*'*70)\n",
    "    #for it in range(len(standard_labels)):\n",
    "    #    print(standard_labels[it],',',standard_labels2[it])\n",
    "#print(np.sort(TF,axis = 0))\n",
    "#tmp_df = pd.merge(tmp_df)\n",
    "tmp_df.T.to_csv('/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/tmp_best_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.index('engineer'), len(feature_names), tfidfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collections.Counter(textfortoken.loc['animal care v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hierarchical clustering\n",
    "distances, linkage_matrix, c, _ = do_hierarch_clustering(tfidfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the dendrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 80)) # set size\n",
    "ax = dendrogram(linkage_matrix, \n",
    "                labels = [t.capitalize() for t in standard_labels], \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = 0,\n",
    "               truncate_mode = 'level', p =20,\n",
    "               above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 13)\n",
    "plt.title('Hierarchical Clustering Dendrogram of Standards', fontsize = 20)\n",
    "plt.xlabel('Distance', fontsize = 20)\n",
    "plt.ylabel('NOS suites',fontsize = 20)\n",
    "if SAVEHC and False:\n",
    "    plt.savefig(os.path.join(output_dir, 'nos_aggregated_dendrogram_{}_{}_{}.png'.format(qualifier,params['bywhich'],\n",
    "                                                    params['mode'])), bbox_inches = \"tight\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Re-plot the dendrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 160)) # set size\n",
    "ax = dendrogram(linkage_matrix, \n",
    "                labels = [t.capitalize() for t in standard_labels], \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = 0,\n",
    "               truncate_mode = 'level', p =43,\n",
    "               above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 12)\n",
    "plt.title('Hierarchical Clustering Dendrogram of Standards', fontsize = 20)\n",
    "plt.xlabel('Distance', fontsize = 20)\n",
    "plt.ylabel('NOS suites',fontsize = 20)\n",
    "T = plt.yticks()\n",
    "super_suites_colours = {'Management': nesta_colours[1], 'Engineering': nesta_colours[3],\n",
    "                        'FinancialServices':nesta_colours[6], 'Construction': nesta_colours[4]}\n",
    "for t in T[1]:\n",
    "    for which_super_suite in super_suites_names:\n",
    "        if t.get_text().lower() in all_match_names[which_super_suite]:\n",
    "            #print(t,',',which_super_suite)\n",
    "            #plt.text(t,'r')\n",
    "            t.set_color(super_suites_colours[which_super_suite])\n",
    "            break\n",
    "\n",
    "if SAVEHC and False:\n",
    "    plt.savefig(os.path.join(output_dir, 'nos_aggregated_dendrogram_{}_{}_{}.pdf'.format(qualifier,params['bywhich'],\n",
    "                                                    params['mode'])), bbox_inches = \"tight\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for t in all_matches['Engineering']:\n",
    "        TF= tfidfm[t,:].T.todense()\n",
    "        print(standard_labels[t])\n",
    "        for ix,fn in enumerate(feature_names):\n",
    "            if TF[ix]>0.08:\n",
    "                print(fn)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the super-suites subset from the cosine matrix and print average similarity matrix\n",
    "average_super_similarity = {}\n",
    "tfidf_super = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    tfidf_super[which_super_suite] = tfidfm[all_matches[which_super_suite],:]\n",
    "\n",
    "for which_super_suite in super_suites_names:\n",
    "    average_super_similarity[which_super_suite] = {}\n",
    "    for which_super_suite2 in super_suites_names:\n",
    "        A = distance.cdist(tfidf_super[which_super_suite].todense(),\n",
    "                           tfidf_super[which_super_suite2].todense(), metric = 'cosine')\n",
    "        N = A.shape\n",
    "        if which_super_suite == which_super_suite2:\n",
    "            B = A[np.triu_indices(N[0],1)]\n",
    "        else:\n",
    "            B = A#[np.triu_indices(N[0],m=N[1])]\n",
    "        average_super_similarity[which_super_suite][which_super_suite2] = (np.around(np.mean(B),2),\n",
    "                                                                           np.around(np.median(B),2),\n",
    "                                                                           np.around(np.std(B),2))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the cosine distances I computed are correct\n",
    "if False:\n",
    "    s1='Construction'\n",
    "    A = tfidfm[all_matches[s1],:].todense()\n",
    "    print(A.shape)\n",
    "    B = np.zeros((A.shape[0],A.shape[0]))\n",
    "    Bl = []\n",
    "    for ia in range(2,A.shape[0]):\n",
    "        a1 = A[ia].T\n",
    "        a1 = np.asarray(a1)\n",
    "        for ia2 in range(ia+1,A.shape[0]):\n",
    "            a2 = A[ia2].T\n",
    "            a2 = np.asarray(a2)\n",
    "            den1 = np.sqrt((np.asarray(a1.T)*np.asarray(a1.T)).sum())\n",
    "            den2 = np.sqrt((np.asarray(a2.T)*np.asarray(a2.T)).sum())\n",
    "            B[ia,ia2] = ((np.asarray(a1.T)*np.asarray(a2.T)).sum())/den1/den2\n",
    "            num = (np.asarray(a1.T)*np.asarray(a2.T)).sum()\n",
    "            Bl.append(num/den1/den2)\n",
    "            if ia2==10:\n",
    "                print(np.corrcoef(a1,a2)[0,1])\n",
    "                #plt.plot(a1,a2,'o')\n",
    "                #plt.plot(a2)\n",
    "                print(Bl[-1],num,den1,den2)\n",
    "                print(standard_labels[all_matches[s1][ia2]])\n",
    "                print([t for ix,t in enumerate(feature_names) if a2[ix]>.08])\n",
    "                print(standard_labels[all_matches[s1][ia]])\n",
    "                print([t for ix,t in enumerate(feature_names) if a1[ix]>.08])\n",
    "                break\n",
    "        break\n",
    "    plt.matshow(B)\n",
    "    plt.colorbar()\n",
    "    print(1-np.mean(Bl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_super_similarity)\n",
    "pd.DataFrame.from_dict(average_super_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to cut the dendrogram at a certain distance threshold, we would use fcluster as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_h = fcluster(linkage_matrix, 0.8, criterion='distance')\n",
    "\n",
    "if params['bywhich']== 'suites':\n",
    "    short_df = pd.DataFrame(standard_labels)\n",
    "    short_df.columns = ['Suite_names']\n",
    "else:\n",
    "    short_df = df_nos[['NOS Title', 'One_suite']].iloc\n",
    "\n",
    "short_df['hierarchical'] = labels_h\n",
    "\n",
    "n_clusters = len(collections.Counter(labels_h))\n",
    "print(n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result of the cut dendrogram\n",
    "hierarchical_dict= {}\n",
    "for ic in range(1,n_clusters+1):\n",
    "    hierarchical_dict['{}'.format(ic)] = short_df['Suite_names'][short_df['hierarchical']==ic].values\n",
    "if SAVEHC:\n",
    "    pd.DataFrame.from_dict(hierarchical_dict, orient = 'index').to_csv(output_dir +\n",
    "                            '/Hierarchical_results_{}_{}_{}.csv'.format(qualifier,params['bywhich'],params['mode']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hierarchical clustering on all suites that belong to at least one super-suite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_supersuite(x):\n",
    "    for supersuite in all_match_names.keys():\n",
    "        if x in all_match_names[supersuite]:\n",
    "            return supersuite.lower()\n",
    "    # if no match has been found\n",
    "    return 'other'\n",
    "\n",
    "def adjustsoccode(x):\n",
    "    y = re.findall(r\"[\\d']+\", str(x))\n",
    "    if len(y):\n",
    "        return y[0][1:-1]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract2digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:2])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract3digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:3])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract1digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract4digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant dataset\n",
    "df_nos['supersuite'] = df_nos['One_suite'].apply(assign_supersuite)\n",
    "df_nos_select = df_nos[~(df_nos['supersuite']=='other')]\n",
    "print(len(df_nos_select))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the parameters if needed\n",
    "params = {}\n",
    "params['ngrams'] = 'uni'\n",
    "params['pofs'] = 'nv'\n",
    "params['tfidf_min'] = 3\n",
    "params['tfidf_max'] = 0.6\n",
    "\n",
    "params['bywhich'] = 'suites' #'docs' #'suites'\n",
    "params['mode'] = 'combinedtfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "\n",
    "# get the transform tfidf\n",
    "tfidf_select = define_tfidf(params, stopwords0)\n",
    "\n",
    "# get the matrix again (even though if the parameters stay the same, this one is the same still)\n",
    "# get the features\n",
    "tfidfm_select, feature_names_select, tfidf_select, _ = get_tfidf_matrix(params, df_nos_select, \n",
    "                                                        tfidf_select, col= 'pruned_lemmas')\n",
    "\n",
    "# get labels\n",
    "if params['bywhich'] == 'suites':\n",
    "    standard_labels_select = list(df_nos_select.groupby('One_suite').groups.keys())\n",
    "else:\n",
    "    standard_labels_select = list(df_nos_select['NOS Title'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute again which suite is in which super-suite from this subsample\n",
    "tmp = np.arange(len(standard_labels_select))\n",
    "all_matches_select = {}\n",
    "for super_suite in ['Engineering','Management','Construction','FinancialServices']:\n",
    "    all_matches_select[super_suite] = tmp[[t in all_match_names[super_suite] for t in standard_labels_select]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check best features in one suite\n",
    "if False:\n",
    "    #standard_labels_select = list(df_nos_select.groupby('One_suite').groups.keys())\n",
    "    #standard_labels2 = textfortoken.index.values\n",
    "    s_idx = 0#standard_labels_select.index('business continuity management 2013')\n",
    "    TF= tfidfm_select[s_idx,:].T.todense()\n",
    "    print(standard_labels_select[s_idx])\n",
    "    for ix,t in enumerate(feature_names_select):\n",
    "        if TF[ix]>0.05:\n",
    "            print(t)\n",
    "    #for it in range(len(standard_labels)):\n",
    "    #    print(standard_labels[it],',',standard_labels2[it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_select.index('engineer'), len(feature_names_select), tfidfm_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hierarchical clustering\n",
    "distances_select, linkage_matrix_select, c_select, _ = do_hierarch_clustering(tfidfm_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_select.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the dendrogram (cutting at threshold)\n",
    "cutting_th = 0.71\n",
    "fig, ax = plt.subplots(figsize=(10, 80)) # set size\n",
    "ax = dendrogram(linkage_matrix_select, \n",
    "                labels = [t.capitalize() for t in standard_labels_select], \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = cutting_th,\n",
    "               truncate_mode = 'level', p =30)#,\n",
    "               #above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 13)\n",
    "plt.title('Hierarchical Clustering Dendrogram of Selected Suites', fontsize = 20)\n",
    "plt.xlabel('Distance', fontsize = 20)\n",
    "plt.ylabel('NOS suites',fontsize = 20)\n",
    "if SAVEHC or True:\n",
    "    plt.savefig(os.path.join(output_dir, ''.join(['suitesnos_clusters_nv_final_no_dropped/',\n",
    "                    'all_supersuites_nos_cut_dendrogram_{}_{}_{}_{}.png'.format(qualifier,\n",
    "        params['bywhich'],params['tfidf_max'],params['ngrams'])])), bbox_inches = \"tight\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Re-plot the dendrogram (no cutting)\n",
    "fig, ax = plt.subplots(figsize=(10, 80)) # set size\n",
    "ax = dendrogram(linkage_matrix_select, \n",
    "                labels = [t.capitalize() for t in standard_labels_select], \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = 0,\n",
    "               truncate_mode = 'level', p =43,\n",
    "               above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 12)\n",
    "plt.title('Hierarchical Clustering Dendrogram of Selected Suites', fontsize = 20)\n",
    "plt.xlabel('Distance', fontsize = 20)\n",
    "plt.ylabel('NOS suites',fontsize = 20)\n",
    "T = plt.yticks()\n",
    "super_suites_colours = {'Management': nesta_colours[1], 'Engineering': nesta_colours[3],\n",
    "                        'FinancialServices':nesta_colours[6], 'Construction': nesta_colours[4]}\n",
    "for t in T[1]:\n",
    "    for which_super_suite in super_suites_names:\n",
    "        if t.get_text().lower() in all_match_names[which_super_suite]:\n",
    "            #print(t,',',which_super_suite)\n",
    "            #plt.text(t,'r')\n",
    "            t.set_color(super_suites_colours[which_super_suite])\n",
    "            break\n",
    "\n",
    "if SAVEHC or True:\n",
    "    plt.savefig(os.path.join(output_dir, ''.join(['suitesnos_clusters_nv_final_no_dropped/',\n",
    "                             'all_supersuites_nos_dendrogram_{}_{}_{}_{}.pdf'.format(\n",
    "        qualifier,params['bywhich'], params['tfidf_max'],params['ngrams'])])), \n",
    "                bbox_inches = \"tight\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for t in all_matches_select['Engineering']:\n",
    "        TF= tfidfm_select[t,:].T.todense()\n",
    "        print(standard_labels_select[t])\n",
    "        for ix,fn in enumerate(feature_names_select):\n",
    "            if TF[ix]>0.08:\n",
    "                print(fn)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_h = fcluster(linkage_matrix_select, cutting_th, criterion='distance')\n",
    "\n",
    "if params['bywhich']== 'suites':\n",
    "    short_df = pd.DataFrame(standard_labels_select)\n",
    "    short_df.columns = ['Suite_names']\n",
    "else:\n",
    "    short_df = df_nos_select[['NOS Title', 'One_suite', 'supersuite']].iloc\n",
    "\n",
    "short_df['hierarchical'] = labels_h\n",
    "\n",
    "cluster_sizes = collections.Counter(labels_h)\n",
    "n_clusters = len(cluster_sizes)\n",
    "print(n_clusters, len(standard_labels_select), cluster_sizes)\n",
    "\n",
    "print(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result of the cut dendrogram\n",
    "hierarchical_dict= {}\n",
    "L = {}\n",
    "D = {}\n",
    "for ic in range(1,n_clusters+1):\n",
    "    hierarchical_dict['{}'.format(ic)] = list(short_df['Suite_names'][\n",
    "        short_df['hierarchical']==ic].values)\n",
    "    A = distance.squareform(distances_select)[(short_df['hierarchical']==ic).values,:][:,\n",
    "                        (short_df['hierarchical']==ic).values]\n",
    "    if A.sum()>0:\n",
    "        A = np.triu(A)\n",
    "        A = A[A[:]>0]\n",
    "    else:\n",
    "        A = np.ones(1)\n",
    "    D['{}'.format(ic)] = np.around(np.mean(A),3)\n",
    "    L['{}'.format(ic)] = (short_df['hierarchical']==ic).sum()\n",
    "L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "L = L.join(D)\n",
    "if SAVEHC or True:\n",
    "    short_df.to_csv(output_dir + '/suitesnos_clusters_nv_final_no_dropped' + \n",
    "                            '/all_supersuites_hierarchical_results_{}_{}_{}_{}.csv'.format(\n",
    "                            qualifier,params['bywhich'],params['tfidf_max'],params['ngrams']))\n",
    "    L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "        by='avg dist', ascending=True).to_csv(output_dir +\n",
    "                            '/suitesnos_clusters_nv_final_no_dropped'+ \n",
    "                            '/all_supersuites_clusters_{}_{}_{}_{}.csv'.format(\n",
    "                            qualifier,params['bywhich'],params['tfidf_max'],params['ngrams']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check best features in one suite\n",
    "'''\n",
    "suites2check = ['pension scheme trusteeship','pension trustee board secretaryship',\n",
    "    'secretary to the trustees of pension funds', \n",
    "    'trustee and management committee', 'stop1','investment operations',\n",
    "    'investment strategy and management','paraplanning','providing financial advice and financial planning',\n",
    "    'generic financial advice','providing advice on savings for retirement','stop2',\n",
    "    'electricity network control engineer','gas network construction',\n",
    "    'gas networks engineering management','leakage detection and control',\n",
    "    'multi utility network construction','network construction operations',\n",
    "    'utilities network planning and management','utility infrastructure management',\n",
    "    'stop3', 'glazing','fenestration, installation and surveying',\n",
    "                'solar thermal photovoltaic panel installation and surveying']\n",
    "suites2check = [['pension scheme trusteeship', 'pension trustee board secretaryship', 'secretary to the trustees of pension funds', 'trustee and management committee'],\n",
    "['providing advice on securities or derivatives in the non retail market', 'providing advice on securities or derivatives in the retail market'],\n",
    "['credit management', 'financing and credit'],\n",
    "['gas network construction', 'multi utility network construction', 'network construction operations'],\n",
    "['harbour masters', 'supervision of port operations'],\n",
    "['directors', 'live events and promotions management', 'production accounting', 'senior producers'],\n",
    "['expedition leadership and management', 'management and leadership', 'managing in road passenger transport', 'outdoor sector senior roles', 'professional skills for government psg'],\n",
    "['constructing capital plant steel structures erecting', 'design and draughting', 'fabricating steel structures plating', 'installation, testing and commissioning of electrical systems and equipment plant', 'installing plant and systems mechanical', 'installing plant and systems pipefitting', 'maintaining plant and systems electrical', 'maintaining plant and systems instrument and controls', 'maintaining plant and systems mechanical', 'monitoring engineering construction activities', 'supporting activities in engineering construction', 'welding plate and pipework', 'welding supervision'],\n",
    "['rail engineering overhead line equipment construction suite 2', 'rail engineering overhead line equipment construction suite 3'],\n",
    "['rail engineering', 'rail engineering signalling suite 2', 'rail engineering signalling suite 3', 'rail engineering telecoms suite 2', 'rail engineering telecoms suite 3'],\n",
    "['performing engineering operations suite 1 2006', 'performing engineering operations suite 2 2006'],\n",
    "['composite engineering suite 2', 'composite engineering suite 3'],\n",
    "['engineering and manufacture suite 4', 'engineering leadership and management suite 3', 'engineering leadership and management suite 4', 'engineering leadership and management suite 5', 'engineering leadership suite 3', 'engineering technical support suite 2 2007', 'engineering technical support suite 3 2009'],\n",
    "['heating and ventilating', 'plumbing and domestic heating', 'refrigeration and air conditioning'],\n",
    "['fenestration, installation and surveying', 'glazing']]\n",
    "'''\n",
    "tmp_df = []\n",
    "if True:\n",
    "    j=0\n",
    "    #standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "    for key in hierarchical_dict:\n",
    "    #for j in range(len(suites2check)):\n",
    "        suites2check = hierarchical_dict[key]\n",
    "        if len(suites2check)==1:\n",
    "            continue\n",
    "        for i,suite0 in enumerate(suites2check):\n",
    "            print(suite0)\n",
    "            if suite0[:4] == 'stop':\n",
    "                tmp_list = ['-']*50\n",
    "            else:\n",
    "                #standard_labels2 = textfortoken.index.values\n",
    "                s_idx = standard_labels_select.index(suite0) #'business continuity management 2013')\n",
    "                TF= tfidfm_select[s_idx,:].T.todense()\n",
    "                sort_ix = np.array(np.argsort(TF.T).ravel()).T\n",
    "                sort_ix = sort_ix[::-1]\n",
    "                tmp_list = [feature_names_select[ix[0]] for ix in sort_ix[:50]]\n",
    "                #print(tmp_list)\n",
    "            if isinstance(tmp_df,list):\n",
    "                tmp_df = pd.DataFrame(tmp_list, columns = [suite0])\n",
    "            else:\n",
    "                tmp_df = tmp_df.join(pd.DataFrame(tmp_list, columns = [suite0]))\n",
    "            print('*'*70)\n",
    "        tmp_list = ['-']*50\n",
    "        tmp_df = tmp_df.join(pd.DataFrame(tmp_list, columns = ['stop{}'.format(j)]))\n",
    "        j+=1\n",
    "tmp_df.to_csv(''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/',\n",
    "                'nlp_analysis/suitesnos_clusters_nv_final_no_dropped/',\n",
    "                       'tmp_best_keywords_{}_{}.csv'.format(\n",
    "                        params['tfidf_max'], params['ngrams'])]))\n",
    "\n",
    "\n",
    "# do it again just for a relevant subset\n",
    "tmp_df = []\n",
    "if True:\n",
    "    j=0\n",
    "    #standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "    for key in hierarchical_dict:\n",
    "        if D.loc[key].values>0.65:\n",
    "            continue\n",
    "    #for j in range(len(suites2check)):\n",
    "        suites2check = hierarchical_dict[key]\n",
    "        if (len(suites2check)==1) | (len(suites2check)>15):\n",
    "            continue\n",
    "        for i,suite0 in enumerate(suites2check):\n",
    "            print(suite0)\n",
    "            if suite0[:4] == 'stop':\n",
    "                tmp_list = ['-']*50\n",
    "            else:\n",
    "                #standard_labels2 = textfortoken.index.values\n",
    "                s_idx = standard_labels_select.index(suite0) #'business continuity management 2013')\n",
    "                TF= tfidfm_select[s_idx,:].T.todense()\n",
    "                sort_ix = np.array(np.argsort(TF.T).ravel()).T\n",
    "                sort_ix = sort_ix[::-1]\n",
    "                tmp_list = [feature_names_select[ix[0]] for ix in sort_ix[:50] if TF[ix]>.0001]\n",
    "                #print(tmp_list)\n",
    "            if isinstance(tmp_df,list):\n",
    "                tmp_df = pd.DataFrame(tmp_list, columns = [suite0])\n",
    "            else:\n",
    "                tmp_df = tmp_df.join(pd.DataFrame(tmp_list, columns = [suite0]))\n",
    "            print('*'*70)\n",
    "        tmp_list = ['-']*50\n",
    "        tmp_df = tmp_df.join(pd.DataFrame(tmp_list, columns = ['stop{}'.format(j)]))\n",
    "        j+=1\n",
    "tmp_df.to_csv(''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/',\n",
    "                       'nlp_analysis/suitesnos_clusters_nv_final_no_dropped/',\n",
    "                       'tmp_best_keywords_{}_{}_subset.csv'.format(\n",
    "                    params['tfidf_max'], params['ngrams'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the super-suites subset from the cosine matrix and print average similarity matrix\n",
    "avg_super_dissimilarity = {}\n",
    "tfidf_super_select = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    tfidf_super_select[which_super_suite] = tfidfm_select[all_matches_select[which_super_suite],:]\n",
    "\n",
    "for which_super_suite in super_suites_names:\n",
    "    avg_super_dissimilarity[which_super_suite] = {}\n",
    "    for which_super_suite2 in super_suites_names:\n",
    "        A = distance.cdist(tfidf_super_select[which_super_suite].todense(),\n",
    "                           tfidf_super_select[which_super_suite2].todense(), metric = 'cosine')\n",
    "        N = A.shape\n",
    "        if which_super_suite == which_super_suite2:\n",
    "            B = A[np.triu_indices(N[0],1)]\n",
    "        else:\n",
    "            B = A#[np.triu_indices(N[0],m=N[1])]\n",
    "        avg_super_dissimilarity[which_super_suite][which_super_suite2] = (np.around(np.mean(B),2),\n",
    "                                                                           np.around(np.median(B),2),\n",
    "                                                                           np.around(np.std(B),2))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_super_dissimilarity)\n",
    "pd.DataFrame.from_dict(avg_super_dissimilarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check that the cosine distances I computed are correct\n",
    "if True:\n",
    "    s1='Construction'\n",
    "    supsuite_tfidf = tfidfm_select[all_matches_select[s1],:].todense()\n",
    "    print(supsuite_tfidf.shape)\n",
    "    supsuite_simil = np.empty((supsuite_tfidf.shape[0],supsuite_tfidf.shape[0]))\n",
    "    supsuite_simil[:] = np.nan\n",
    "    supsuite_simil_l = []\n",
    "    for ia in range(supsuite_tfidf.shape[0]):\n",
    "        vector1 = supsuite_tfidf[ia].T\n",
    "        vector1 = np.asarray(vector1)\n",
    "        # only compute values for upper triangular part of the matrix\n",
    "        for ia2 in range(ia+1,supsuite_tfidf.shape[0]):\n",
    "            vector2 = supsuite_tfidf[ia2].T\n",
    "            vector2 = np.asarray(vector2)\n",
    "            # compute norm of vector 1\n",
    "            den1 = np.sqrt((np.asarray(vector1.T)*np.asarray(vector1.T)).sum())\n",
    "            # norm of vector 2\n",
    "            den2 = np.sqrt((np.asarray(vector2.T)*np.asarray(vector2.T)).sum())\n",
    "            # scalar product\n",
    "            num = (np.asarray(vector1.T)*np.asarray(vector2.T)).sum()\n",
    "            supsuite_simil[ia,ia2] = num/den1/den2\n",
    "            supsuite_simil_l.append(num/den1/den2)\n",
    "            if num>.88: #(num<.1) or (num>.9):\n",
    "                print('Correlation between vectors: ',np.corrcoef(vector1.T,vector2.T)[0,1])\n",
    "                plt.plot(vector1,vector2,'o')\n",
    "                #plt.plot(a2)\n",
    "                print('Similarity between vectors',supsuite_simil_l[-1])\n",
    "                #,', num, den1, den2: ',num,den1,den2)\n",
    "                print('Suite2: ',standard_labels_select[all_matches_select[s1][ia2]])\n",
    "                print('Suite 2 keywords: ',[t for ix,t in enumerate(feature_names_select) \n",
    "                                            if vector2[ix]>.08])\n",
    "                print('Suite 1: ',standard_labels_select[all_matches_select[s1][ia]])\n",
    "                print('Suite 1 keywords: ',[t for ix,t in enumerate(feature_names_select) \n",
    "                                            if vector1[ix]>.08])\n",
    "                #break\n",
    "        #break\n",
    "    plt.matshow(supsuite_simil)\n",
    "    plt.title('similarities')\n",
    "    plt.colorbar()\n",
    "    print('Average dissimilarity for', s1, 1-np.mean(supsuite_simil_l))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical cluster of NOS in suites clusters\n",
    "For selected clusters of suite, perform hierarchical clustering of the NOS inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some more things\n",
    "df_nos_select['SOC4str'] = df_nos_select['Clean SOC Code'].map(adjustsoccode)\n",
    "df_nos_select['SOC4'] = df_nos_select['SOC4str'].map(extract4digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subdf(SELECT_MODE, clusters2use, suites_clusters,df_nos_select):\n",
    "    if isinstance(SELECT_MODE, str):\n",
    "        tmp_dict = {'engineering': 'Engineering', 'management': 'Management',\n",
    "                    'financialservices': 'Financial services', \n",
    "                    'construction': 'Construction'}\n",
    "        # select NOS from super suite\n",
    "        cluster_name = SELECT_MODE\n",
    "        cluster_name_save = cluster_name\n",
    "        cluster_name_figs = tmp_dict[SELECT_MODE]\n",
    "        subset_nos = df_nos_select[df_nos_select['supersuite']== SELECT_MODE]\n",
    "    elif isinstance(SELECT_MODE, int):\n",
    "        cluster_name = clusters2use[SELECT_MODE][1]\n",
    "        cluster_name_save = cluster_name.replace(' ','_')\n",
    "        cluster_name_figs = cluster_name.capitalize()\n",
    "        suites2use = list(suites_clusters[suites_clusters['hierarchical'].map(\n",
    "                lambda x: x in clusters2use[SELECT_MODE][0])]['Suite_names'].values)\n",
    "        subset_nos = df_nos_select[df_nos_select['One_suite'].map(\n",
    "                lambda x: x in suites2use)]\n",
    "    print('Number of NOS selected: ', len(subset_nos))\n",
    "    #print(subset_nos.columns)\n",
    "    \n",
    "    #%\n",
    "    # only select those engineering nos with SOC codes\n",
    "    nosoc = subset_nos['SOC4'].isnull()\n",
    "    print('percentage of nos without SOC codes: ', nosoc.sum()/len(nosoc))\n",
    "    if (nosoc.sum())/len(nosoc)<0.9:\n",
    "        final_nos = subset_nos[~nosoc] #np.isnan(engineering_nos['SOC4'])]\n",
    "    else:\n",
    "        final_nos = subset_nos\n",
    "    final_groups = final_nos.groupby(by = 'One_suite')\n",
    "    larger_suites = []\n",
    "    all_lengths = final_groups.agg(len)['NOS Title'].values\n",
    "    all_lengths[::-1].sort()\n",
    "    print('Number of NOS in suites belonging to this cluster: ',all_lengths)\n",
    "    #th_supers = ['engineering': 40, 'financialservices': ]\n",
    "    for name, group in final_groups:\n",
    "        if isinstance(SELECT_MODE, int):\n",
    "            larger_suites.append(name)\n",
    "        elif len(group)> all_lengths[15]:#th_supers[SELECT_MODE]:\n",
    "            #print(name, len(group))\n",
    "            larger_suites.append(name)\n",
    "\n",
    "    return final_nos, final_groups, larger_suites, cluster_name,  \\\n",
    "                    cluster_name_save, cluster_name_figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the parameters if needed\n",
    "paramsn = {}\n",
    "paramsn['ngrams'] = 'uni'\n",
    "paramsn['pofs'] = 'nv'\n",
    "paramsn['tfidf_min'] = 3\n",
    "paramsn['tfidf_max'] = 0.4\n",
    "\n",
    "paramsn['bywhich'] = 'docs' #'docs' #'suites'\n",
    "paramsn['mode'] = 'tfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "\n",
    "# select the relevant NOS\n",
    "\n",
    "# get the transform tfidf\n",
    "tfidf_n = define_tfidf(paramsn, stopwords0)\n",
    "\n",
    "# get the matrix again (even though if the parameters stay the same, this one is the \n",
    "# same still)\n",
    "# this is to fit the transform on the entire NOS corpus (from supersuites only)\n",
    "_, feature_names_n, tfidf_n, _ = get_tfidf_matrix(paramsn, df_nos_select, \n",
    "                                                        tfidf_n, col= 'pruned_lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suites_clusters = pd.read_csv(''.join(['/Users/stefgarasto/Google Drive/Documents/results',\n",
    "            '/NOS/nlp_analysis/all_supersuites_hierarchical_results_postjoining_final',\n",
    "                                       '_no_dropped_suites_combinedtfidf_uni.csv']))\n",
    "\n",
    "clusters2use = [([17], 'port operations',3),\n",
    "                ([4], 'pension schemes',3),\n",
    "                ([54],'rail engineering',18),\n",
    "                ([51], 'temperature control engineering',5),\n",
    "                ([18], 'food and hospitality',39), # note that 39 is a local max in the knee\n",
    "                ([21], 'entertainment industry',29),\n",
    "                ([31], 'social care',22),\n",
    "                ([46], 'network engineering (utilities)',29)]\n",
    "\n",
    "# note: this clusters are kinda robust under changes of parameters for the TFIDF - that is,\n",
    "# they are roughly the same when using bi-grams, uni-grams with max=0.4 and uni-grams with \n",
    "# max = 0.6\n",
    "\n",
    "SAVEHC = False\n",
    "# so far, groups 0 and 3 don't work very well, 4 so and so, 5 needs 0.8, \n",
    "# 1,2 are decent with 0.4 (0.5?), 7 can also do with 0.8,\n",
    "# note that 3 and 6 have most NOS without SOC codes\n",
    "for SELECT_MODE in range(2,3):#8):\n",
    "    df_nos_n, final_groups, larger_suites, cluster_name, cluster_name_save, \\\n",
    "                cluster_name_figs = select_subdf(SELECT_MODE, clusters2use, \n",
    "                                                 suites_clusters,df_nos_select)\n",
    "    print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "    # remove legacy nos\n",
    "    print('nb with legacy nos: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n['NOS Title'].map(lambda x: 'legacy' not in x)]\n",
    "    print('nb without legacy nos 1: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n.index.map(lambda x: not x[-5:]=='l.pdf')]\n",
    "    print('nb without legacy nos 2: ',len(df_nos_n))\n",
    "    suites_in_clus = {}\n",
    "    groups_clus = df_nos_n.groupby('One_suite')\n",
    "    for name, group in groups_clus:\n",
    "        suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "    # this is to get the restricted corpus (to transform, not for fitting)\n",
    "    textfortoken = df_nos_n['pruned_lemmas']\n",
    "    tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "\n",
    "    # get labels\n",
    "    if paramsn['bywhich'] == 'suites':\n",
    "        standard_labels_n = list(df_nos_n.groupby('One_suite').groups.keys())\n",
    "    else:\n",
    "        standard_labels_n = list(df_nos_n['NOS Title'].values)\n",
    "\n",
    "    for ix,t in enumerate(standard_labels_n):\n",
    "        if len(t)>500:\n",
    "            # manual correction because of pdf extraction\n",
    "            standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "\n",
    "    #print('tfidfm and features shapes: ',tfidfm_n.shape, len(feature_names_n))\n",
    "\n",
    "\n",
    "    # check best features in a few NOS\n",
    "    for s_idx in range(1): #34):\n",
    "        s_idx = standard_labels_n.index(\n",
    "            'lift and move permanent way materials, components and equipment')\n",
    "        TF= tfidfm_n[s_idx,:].T.todense()\n",
    "        print(standard_labels_n[s_idx])\n",
    "        ix = np.argsort(TF, axis = 0)\n",
    "        for i in ix[-20:][::-1]: #enumerate(feature_names_n):\n",
    "            i = np.array(i)\n",
    "            print(feature_names_n[i[0][0]],np.around(TF[i[0][0]][0,0],3))\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "    #print('Where do we find the feature \\'system\\'''?: ',feature_names_n.index('system'))\n",
    "\n",
    "\n",
    "    # perform hierarchical clustering\n",
    "    distances_n, linkage_matrix_n, c_n, _ = do_hierarch_clustering(tfidfm_n, DOPLOTS= False)\n",
    "\n",
    "\n",
    "    # Plotting the distance between successive clusters: is there a knee?\n",
    "    z = linkage_matrix_n[::-1,2]\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    plt.plot(range(1, len(z)+1), z)\n",
    "    knee = np.diff(z, 2)\n",
    "    plt.plot(range(2, len(linkage_matrix_n)), knee)\n",
    "    plt.xlabel('partition')\n",
    "    plt.ylabel('cluster distance')\n",
    "    plt.title(cluster_name_figs)\n",
    "    goodness = []\n",
    "    for i in range(3,len(z)-2):\n",
    "        a1 = scipy.stats.linregress(range(1,i+1), z[:i])\n",
    "        a2 = scipy.stats.linregress(range(i, len(z)), z[i:])\n",
    "        goodness.append(np.around(a1[2]**2 + a2[2]**2,4))\n",
    "    plt.figure(figsize = (6,6))\n",
    "    #print(goodness)\n",
    "    plt.plot(np.arange(3,len(z)-2),goodness)\n",
    "    plt.title(cluster_name_figs)\n",
    "    ixg = np.array(goodness).argmax()+3\n",
    "    print('best t-point: ',ixg)\n",
    "\n",
    "    num_ideal = np.ceil(len(df_nos_n)/10)\n",
    "    print('The ideal number of clusters would be: ',num_ideal)\n",
    "    num_clust1 = knee.argmax() + 2\n",
    "    knee[knee.argmax()] = 0\n",
    "    num_clust2 = knee.argmax() + 2\n",
    "    \n",
    "    '''if num_clust1 == 2:\n",
    "        num_clust = num_clust2\n",
    "    elif num_clust2 == 2:\n",
    "        num_clust = num_clust1\n",
    "    elif num_clust2>num_ideal:\n",
    "        if num_clust1 > num_ideal:\n",
    "            num_clust = num_ideal\n",
    "        else:\n",
    "            num_clust = min([num_clust1,num_clust2])\n",
    "    else:\n",
    "        if np.abs(num_clust1-num_ideal)<np.abs(num_clust2-num_ideal):\n",
    "            num_clust = num_clust1\n",
    "        else:\n",
    "            num_clust = num_clust2'''\n",
    "    num_clust = clusters2use[SELECT_MODE][2]\n",
    "    print('The two peaks are, in order: ',num_clust1, num_clust2)\n",
    "    print('The selected num clust is ',num_clust)\n",
    "    #num_clust = max([num_clust1,num_clust2])\n",
    "\n",
    "    for t in np.arange(0,1,0.05):\n",
    "        labels_n = fcluster(linkage_matrix_n, t, criterion='distance')\n",
    "        n_clust = len(set(labels_n))\n",
    "        if n_clust <= num_clust:\n",
    "            cutting_th_n = t\n",
    "            break\n",
    "    print('cutting threshold: {}'.format(cutting_th_n))       \n",
    "    \n",
    "    #Plot the dendrogram (cutting at threshold)\n",
    "    #cutting_th_n = 0.6\n",
    "    h = .5*len(df_nos_n)\n",
    "    fig, ax = plt.subplots(figsize=(28, h)) # set size\n",
    "    ax = dendrogram(linkage_matrix_n, \n",
    "                    labels = [t.capitalize() for t in standard_labels_n], \n",
    "                    orientation = 'right', \n",
    "                    leaf_font_size=6,\n",
    "                   color_threshold = cutting_th_n,\n",
    "                   truncate_mode = 'level', p =30)#,\n",
    "                   #above_threshold_color = 'k');\n",
    "\n",
    "    plt.tick_params(axis= 'y',\n",
    "                    labelsize = 24)\n",
    "    plt.title('Hierarchical clustering for {}'.format(cluster_name_figs), fontsize = 30)\n",
    "#              'Hierarchical Clustering Dendrogram of Selected NOS', fontsize = 20)\n",
    "    plt.xlabel('Distance', fontsize = 30)\n",
    "    plt.ylabel('NOS title',fontsize = 30)\n",
    "    \n",
    "    s_patch = []\n",
    "    for ix, which_suite in enumerate(suites_in_clus):\n",
    "        s_patch.append(mpatches.Patch(color= nesta_colours[ix],label= \n",
    "                                      which_suite.capitalize()))\n",
    "        #plt.plot(0,0,color = nesta_colours[ix],label = which_suite.capitalize())\n",
    "    plt.legend(handles = s_patch, bbox_to_anchor=(1.01, .81), loc=2,\n",
    "           ncol=1, borderaxespad=0., fontsize = 24)\n",
    "    T = plt.yticks()\n",
    "    for t in T[1]:\n",
    "        for ix, which_suite in enumerate(suites_in_clus):\n",
    "            if t.get_text().lower() in suites_in_clus[which_suite]:\n",
    "                t.set_color(nesta_colours[ix])\n",
    "                break\n",
    "    plt.tight_layout()\n",
    "    if SAVEHC:\n",
    "        plt.savefig(os.path.join(output_dir, \n",
    "            'suitesnos_clusters_nv_final_no_dropped/all_nos_cut_dendrogram_in_{}_{}_{}.png'.format(\n",
    "            cluster_name_save,qualifier,params['ngrams'])), bbox_inches = \"tight\")   \n",
    "        \n",
    "    # now get and save the clusters\n",
    "    labels_n = fcluster(linkage_matrix_n, cutting_th_n, criterion='distance')\n",
    "    short_df_n = df_nos_n.reset_index()[['index','NOS Title', 'One_suite']]\n",
    "\n",
    "    short_df_n['hierarchical'] = labels_n\n",
    "    short_df_n = short_df_n.set_index('index')\n",
    "    if SAVEHC:\n",
    "        short_df_n.to_csv(os.path.join(output_dir, \n",
    "            'suitesnos_clusters_nv_final_no_dropped/all_nos_cut_labels_in_{}_{}_{}.csv'.format(\n",
    "            cluster_name_save,qualifier,params['ngrams'])))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos_n['NOS Title'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mclust = np.floor(len(standard_labels_n)/5)\n",
    "print(Mclust)\n",
    "labels_n = fcluster(linkage_matrix_n, Mclust, criterion='maxclust')\n",
    "print(labels_n)\n",
    "#labels_n = fcluster(linkage_matrix_n, cutting_th_n, criterion='distance')\n",
    "#print(labels_n)\n",
    "short_df_n = df_nos_n[['NOS Title', 'One_suite']]\n",
    "\n",
    "short_df_n['hierarchical'] = labels_n\n",
    "#print(short_df_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hierarchical clustering only on the suites in one super-suite (e.g. Management)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "which_super_suite = 'Construction'\n",
    "match_name = all_match_names[which_super_suite]\n",
    "match = all_matches[which_super_suite]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hierarchical clustering on the suites belonging to the super suite alone\n",
    "distances_super, linkage_matrix_super, c_super, c_dist_super = do_hierarch_clustering(\n",
    "                                                            tfidfm[match,:], DOPLOTS = False)\n",
    "\n",
    "#Plot the dendrogram again for this super-suite\n",
    "standard_labels_super = [standard_labels[t].capitalize() for t in match]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix_super, \n",
    "                labels = standard_labels_super, \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = 0,\n",
    "               truncate_mode = 'level', p =40,\n",
    "               above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 14)\n",
    "plt.title('Hierarchical Clustering Dendrogram of NOS (in super-suite {})'.format(\n",
    "    which_super_suite), fontsize = 18)\n",
    "plt.xlabel('Distance', fontsize = 18)\n",
    "plt.ylabel('NOS suites',fontsize = 18)\n",
    "if SAVEHC:\n",
    "    plt.savefig(os.path.join(output_dir, 'nos_dendrogram_{}_{}_{}_{}.png'.format(\n",
    "            which_super_suite, qualifier,params['bywhich'],params['mode'])), \n",
    "                bbox_inches = \"tight\")     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h = fcluster(linkage_matrix_super, 0.9, criterion='distance')\n",
    "\n",
    "if params['bywhich']== 'suites':\n",
    "    short_df = pd.DataFrame(standard_labels_super)\n",
    "    short_df.columns = ['Suite_names']\n",
    "else:\n",
    "    short_df = df_nos[['NOS Title', 'One_suite']].iloc\n",
    "\n",
    "short_df['hierarchical'] = labels_h\n",
    "\n",
    "n_clusters = len(collections.Counter(labels_h))\n",
    "print(n_clusters,len(labels_h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and save data for graph visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_soc(x):\n",
    "    if isinstance(x,list):\n",
    "        try:\n",
    "            return x[0]\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def get_one_occupation(x):\n",
    "    if isinstance(x,list):\n",
    "        y= ''.join(x)\n",
    "    else:\n",
    "        y= x\n",
    "    if isinstance(y,str):\n",
    "        return y.split(';')\n",
    "    else:\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect metadata for all suites in the super-suite\n",
    "metadata = {}\n",
    "nb_of_nos = []\n",
    "datadriven_keywords = []\n",
    "expert_keywords = []\n",
    "top_developed = []\n",
    "top_originating = []\n",
    "top_approved_year = []\n",
    "top_SOC = []\n",
    "top_occupation = []\n",
    "# group by suite\n",
    "groups = df_nos.groupby('One_suite')\n",
    "# cycle through suites\n",
    "for suite in match_name:\n",
    "    group = groups.get_group(suite)\n",
    "    # how many nos in this suite\n",
    "    nb_of_nos.append(len(group))\n",
    "    # suite keywords (identified before)\n",
    "    datadriven_keywords.append(top_terms_dict[name + ' (top features)'])\n",
    "    # list of actual keywords (ordered by popularity)\n",
    "    expert_keywords.append(top_terms_dict[name + ' (keywords)'])\n",
    "    # most common developing and originating organisation\n",
    "    top_originating.append(group['Originating_organisation'].value_counts().index[0])\n",
    "    top_developed.append(group['Developed By'].value_counts().index[0])\n",
    "    # most common year approved\n",
    "    top_approved_year.append(group['Date_approved_year'].value_counts().index[0])\n",
    "    # top SOC code (in order)\n",
    "    try:\n",
    "        top_SOC.append(group['Clean SOC Code'].map(get_one_soc).value_counts().index[0])\n",
    "    except:\n",
    "        top_SOC.append(np.nan)\n",
    "    # most common occupations\n",
    "    tmp = []\n",
    "    for t in group['Occupations'].map(get_one_occupation):\n",
    "        tmp += t\n",
    "    top_occupation.append(pd.DataFrame(tmp)[0].value_counts().index[0].strip())\n",
    "    if not top_occupation[-1]:\n",
    "        top_occupation[-1] = np.nan\n",
    "\n",
    "# now make dictionary\n",
    "metadata['nb_of_nos'] = nb_of_nos\n",
    "metadata['datadriven_keywords'] = datadriven_keywords\n",
    "metadata['expert_keywords'] = expert_keywords\n",
    "metadata['top_originating_organisation'] = top_originating\n",
    "metadata['top_developed_by'] = top_developed\n",
    "metadata['top_approved_year'] = top_approved_year\n",
    "metadata['top_soc'] = top_SOC\n",
    "metadata['top_occupation'] = top_occupation\n",
    "#metadata['suite_name'] = match_name\n",
    "# change to dataframe\n",
    "df_metadata = pd.DataFrame.from_dict(metadata)\n",
    "#df_metadata = df_metadata.set_index('suite_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_management_cluster = df_metadata.join(short_df).set_index('Suite_names')#.drop('Suite_names')\n",
    "print(df_management_cluster.head(n=3))\n",
    "df_management_cluster.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDATAHC = False\n",
    "if SAVEDATAHC:\n",
    "    with open(os.path.join(output_dir, 'for_graph_visualisation/data_from_management_clustering.pickle'), 'wb') as f:\n",
    "        pickle.dump((df_management_cluster, distances_super, linkage_matrix_super, c_super, c_dist_super), f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each suite in the management super-suite, collect the same information as for the whole management supersuite and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {}\n",
    "params2['ngrams'] = 'uni'\n",
    "params2['pofs'] = 'nv'\n",
    "params2['tfidf_min'] = 3\n",
    "params2['tfidf_max'] = 0.4\n",
    "\n",
    "params2['bywhich'] = 'docs' #'docs' #'suites'\n",
    "params2['mode'] = 'tfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "\n",
    "# get the transform tfidf\n",
    "tfidf2 = define_tfidf(params2, stopwords0)\n",
    "\n",
    "# get the matrix again (even though if the parameters stay the same, this one is the same still)\n",
    "# get the features\n",
    "tfidfm2, feature_names2 = get_tfidf_matrix(params2, df_nos, tfidf2, col = 'pruned_lemmas')\n",
    "\n",
    "# get labels\n",
    "if params['bywhich'] == 'suites':\n",
    "    standard_labels2 = list(df_nos.groupby('One_suite').groups.keys())\n",
    "else:\n",
    "    standard_labels2 = list(df_nos['NOS Title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_nos.groupby('One_suite')\n",
    "metadata_cols = ['NOS Title', 'URN',\n",
    "       'Originating_organisation','Date_approved_year',\n",
    "       'Clean Ind Review Year', 'Version_number', 'Developed By', 'Validity',\n",
    "       'Keywords', 'Clean SOC Code', 'NOS Document Status', 'NOSCategory', 'Occupations',\n",
    "       'One_suite']\n",
    "fields_to_check = ['Overview',\n",
    "       'Knowledge_and_understanding', 'Performance_criteria', 'Scope_range',\n",
    "       'Glossary', 'Behaviours', 'Skills', 'Values', 'External_Links' ,\n",
    "       'Links_to_other_NOS']\n",
    "SAVEDATAHC = False\n",
    "COLLECTDATAHC = False\n",
    "t0 = time.time()\n",
    "if COLLECTDATAHC:\n",
    "    for suite in df_management_cluster.index[::-1]:\n",
    "        print(suite)\n",
    "        idx = (df_nos['One_suite'] == suite).values\n",
    "        if idx.sum()>19:\n",
    "            tfidfm_red = tfidfm2[idx]\n",
    "            group = groups.get_group(suite)\n",
    "            df_metadata_nos = group[metadata_cols]\n",
    "            df_metadata_nos['fields present'] = 1\n",
    "            for nos in group.index:\n",
    "                fields_present = []\n",
    "                for field in fields_to_check:\n",
    "                    if not group[field].loc[nos] == np.nan:\n",
    "                        fields_present.append(field)\n",
    "                df_metadata_nos['fields present'].loc[nos] = fields_present\n",
    "            # now get the rest, that is the cosine distances\n",
    "            distances_suite, linkage_matrix_suite, c_suite, c_dist_suite = do_hierarch_clustering(tfidfm_red, \n",
    "                                                                                          DOPLOTS = False)\n",
    "    #        if suite == 'supply chain management':\n",
    "    #            distances_scm, linkage_matrix_scm, c_scm, c_dist_scm = \n",
    "    #            distances_suite, linkage_matrix_suite, c_suite, c_dist_suite\n",
    "    #            break\n",
    "            if SAVEDATAHC:\n",
    "                with open(os.path.join(output_dir, \n",
    "                        'for_graph_visualisation/data_from_{}_suite_clustering.pickle'.format(suite)), \n",
    "                          'wb') as f:\n",
    "                    pickle.dump((df_metadata_nos, distances_suite, linkage_matrix_suite, c_suite, c_dist_suite), f)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the clustering for the supply chain management suite\n",
    "suite = 'supply chain management'\n",
    "groups = df_nos.groupby('One_suite')\n",
    "idx = (df_nos['One_suite'] == suite).values\n",
    "tfidfm_red = tfidfm2[idx]\n",
    "group = groups.get_group(suite)\n",
    "nos_names = group['NOS Title'].values\n",
    "nos_names = [t.capitalize() for t in nos_names]\n",
    "# now get the rest, that is the cosine distances\n",
    "distances_scm, linkage_matrix_scm, c_scm, c_dist_scm = do_hierarch_clustering(tfidfm_red, \n",
    "                                                                              DOPLOTS = False)\n",
    "\n",
    "#Plot the dendrogram again for this suite\n",
    "fig, ax = plt.subplots(figsize=(10, 50)) # set size\n",
    "ax = dendrogram(linkage_matrix_scm, \n",
    "                labels = nos_names, \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = 0,\n",
    "               truncate_mode = 'level', p =40,\n",
    "               above_threshold_color = 'k');\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 14)\n",
    "plt.title('Hierarchical Clustering Dendrogram of NOS (in the {} suite)'.format(suite), fontsize = 18)\n",
    "plt.xlabel('Distance', fontsize = 18)\n",
    "plt.ylabel('NOS titles',fontsize = 18)\n",
    "if SAVEHC or True:\n",
    "    plt.savefig(os.path.join(output_dir, 'nos_dendrogram_{}_{}_{}_{}_new.pdf'.format(suite.replace(' ','_'),\n",
    "                                            qualifier,params['bywhich'],params['mode'])), bbox_inches = \"tight\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad hoc requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance.squareform(distances_scm).shape)\n",
    "groups = df_nos.groupby('One_suite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = groups.get_group('welding supervision')\n",
    "print(group['tagged_tokens'].loc['eciws01.pdf'][:20])\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = groups.get_group('engineering leadership and management suite 5')\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = groups.get_group('engineering leadership and management suite 3')\n",
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is very informative, but does not scale well since we are calculating pairwise distances. So if we had many more standards than 400+ apprenticeship standards, we would have to explore other options.\n",
    "One solution would be to pre-cluster standards into a large number of smaller clusters using a faster method, such as k-means and then do the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEKM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['ngrams'] = 'uni'\n",
    "params['pofs'] = 'nv'\n",
    "params['tfidf_min'] = 3\n",
    "params['tfidf_max'] = 0.4\n",
    "\n",
    "params['bywhich'] = 'suites' #'docs' #'suites'\n",
    "params['mode'] = 'combinetfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "\n",
    "# get the transform tfidf\n",
    "tfidf = define_tfidf(params, stopwords0)\n",
    "\n",
    "# get the matrix again (even though if the parameters stay the same, this one is the same still)\n",
    "# get the features\n",
    "tfidfm, feature_names = get_tfidf_matrix(params, df_nos, tfidf, col = 'pruned_lemmas')\n",
    "\n",
    "# get labels\n",
    "if params['bywhich'] == 'suites':\n",
    "    standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "else:\n",
    "    standard_labels = list(df_nos['NOS Title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_k(df_row):\n",
    "    #distance = pairwise_distances(df_row.values[:4103].reshape(1, -1), \n",
    "    #                              centroids[int(df_row['k_cluster'])].reshape(1, -1)) # why until 4103?\n",
    "    L = len(df_row.values) # last column is the cluster class\n",
    "    distance = pairwise_distances(df_row.values[:L-1].reshape(1, -1), \n",
    "                                  centroids[int(df_row['k_cluster'])].reshape(1, -1))\n",
    "    distance = distance[0][0]\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 5 seconds for N = 400, 20 seconds for N = 1000, 50 seconds for N = 2000, 110 seconds for 4000\n",
    "N = 400\n",
    "t0 = time.time()\n",
    "# use approx the number of super suites SDS gave you\n",
    "k = 40 \n",
    "#use the number of clusters detected by the hierarchical algorithm above\n",
    "#k = n_clusters \n",
    "km = KMeans(n_clusters = k, random_state = 111)\n",
    "km.fit(tfidfm.toarray())\n",
    "k_clusters = km.labels_.tolist()\n",
    "print_elapsed(t0, task = 'kmean clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df['k_means'] = k_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfm_df = pd.DataFrame(tfidfm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfm_df['k_cluster'] = k_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df['k_distance'] = tfidfm_df.apply(get_distance_k, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the centroids (that is, the suite closest to the centroid) and print the result of the clustering\n",
    "kmeans_dict= {}\n",
    "most_central = []\n",
    "igroup = 0\n",
    "for name, group in short_df.groupby('k_means'):\n",
    "    kmeans_dict['{}'.format(name)] = group['Suite_names'].values\n",
    "    if igroup < 10:\n",
    "        print(name, group.sort_values(by = 'k_distance').head(3))\n",
    "    igroup += 1\n",
    "    most_central.append(group.sort_values(by = 'k_distance').head(1))\n",
    "if SAVEKM:\n",
    "    pd.DataFrame.from_dict(kmeans_dict, orient = 'index').to_csv(output_dir +\n",
    "                                            '/Kmeans_results_{}_{}_{}.csv'.format(qualifier,bywhich,mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIERARCHICAL 2\n",
    "\n",
    "We can now select representatives from each small cluster and perform hierarchical clustering again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = pd.concat(most_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(set(select_df['hierarchical'])))\n",
    "# print the most representative suite for each cluster\n",
    "select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indices of the central suites/docs\n",
    "select_ix = np.array([short_df.index.get_loc(ix) for ix in select_df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_standards = tfidfm.toarray()[select_ix, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_standards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate cosine distance between tf-idf vectors of the documents/suites\n",
    "distances_s, linkage_matrix_s, c_s = do_hierarch_clustering(select_standards, DOPLOTS = False)\n",
    "'''\n",
    "distances_s = distance.pdist(select_standards, metric = 'cosine')\n",
    "\n",
    "#We then build linkage matrix using the distances and specifying the method. For euclidean distances typically 'Ward'\n",
    "#produces best results. For cosine we can only use 'average' and 'single'.\n",
    "linkage_matrix_s = scipy.cluster.hierarchy.linkage(distances_s,\n",
    "                                                 method = 'average',\n",
    "                                                 metric = 'cosine')\n",
    "#We can test how well the groupings reflect actual distances. If c > 0.75 this is considered to be sufficiently\n",
    "#good representation\n",
    "c, coph_dists = cophenet(linkage_matrix_s, \n",
    "                         distance.pdist(select_standards, metric = 'cosine'))\n",
    "'''\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if params['bywhich'] == 'suites':\n",
    "    standard_labels = list(select_df['Suite_names'].values)\n",
    "else:\n",
    "    standard_labels = list(select_df['NOS Title'].values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 15)) # set size\n",
    "ax = dendrogram(linkage_matrix_s, \n",
    "                labels = standard_labels, \n",
    "                orientation = 'right', \n",
    "                leaf_font_size=6,\n",
    "               color_threshold = 0.8);\n",
    "\n",
    "plt.tick_params(axis= 'y',\n",
    "                labelsize = 12)\n",
    "plt.title('Hierarchical Clustering Dendrogram of Pre-clustered Standards', fontsize = 12)\n",
    "plt.xlabel('Distance', fontsize = 12)\n",
    "if params['bywhich'] == 'suites':\n",
    "    plt.ylabel('NOS suites',fontsize = 12)\n",
    "else:\n",
    "    plt.ylabel('NOS titles',fontsize = 12)\n",
    "if SAVEKM:\n",
    "    plt.savefig(os.path.join(output_dir, 'nos_dendrogram_centroids_{}_{}_{}.svg'.format(qualifier,bywhich,mode)), \n",
    "            bbox_inches = \"tight\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below is just backup, with code that is now being replaced, but the replacement has not been fully tested yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The next three lines compute the tfidf matrix for the keywords extraction\n",
    "'''\n",
    "\n",
    "textfortoken= df_nos['pruned']\n",
    "tfidfm = tfidf.fit_transform(textfortoken)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "'''\n",
    "The code below was used to compute the tfidf matrix to feed to the clustering algorithm. However, I want to uniform \n",
    "the way the matrix is computed for both the keywords extraction and the clustering\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def get_tfidfm(bywhich, mode, df_nos)\n",
    "    t0 = time.time()\n",
    "    if bywhich == 'suites':\n",
    "        if mode == 'meantfidf':\n",
    "            # this is the case where I group by suite and use the average tfidf vectore as the features to cluster\n",
    "            row_names = df_nos['One_suite'].value_counts().index.values\n",
    "            tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "            igroup = 0\n",
    "            for name, group in df_nos.groupby('One_suite'):\n",
    "                tmp = get_mean_tfidf(group['pruned'], tfidf)\n",
    "                tfidfm[igroup] = tmp\n",
    "                igroup += 1\n",
    "        elif mode == 'combinedtfidf':\n",
    "            # this is the case where I group by suite, concatenate all tokens and compute the tfidf vectors\n",
    "            row_names = df_nos['One_suite'].value_counts().index.values\n",
    "            tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "            igroup = 0\n",
    "            for name, group in df_nos.groupby('One_suite'):\n",
    "                joint_tokens = []\n",
    "                for idoc in group['pruned'].index:\n",
    "                    joint_tokens += group['pruned'].loc[idoc]\n",
    "                tmp = tfidf.transform([joint_tokens])\n",
    "                tfidfm[igroup] = tmp\n",
    "                igroup += 1\n",
    "        # TODO: top word embedding modes + t\n",
    "    elif bywhich == 'docs':\n",
    "        if mode == 'tfidf':\n",
    "            # this is where I keep everything the same\n",
    "            tfidfm = tfidf.fit_transform(df_nos['pruned'])\n",
    "    print_elapsed(t0, 'computing the feature vector')\n",
    "    tfidfm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
