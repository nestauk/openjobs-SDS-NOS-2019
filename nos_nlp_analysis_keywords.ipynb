{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions and snippets of code that are useful for analysing text. Most of the techniques used are unsupervised. Functions are defined up front and then used in sections below.\n",
    "\n",
    "This notebook is to apply:\n",
    "- Tokenizers (based on n-grams and 'as_is')\n",
    "- Calculating distance\n",
    "- Hierarchical clustering and plotting\n",
    "- K-means clustering\n",
    "- LSH\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('-')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list): #, stopwords):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]# if elem.lower() not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'nv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/glove.twitter.27B'\n",
    "\n",
    "# to make the glove model file compatible with gensim\n",
    "#for dim in ['25','50','100','200']:\n",
    "##    glove_file = os.path.join(glove_dir,'glove.twitter.27B.{}d.txt'.format(dim))\n",
    "#    tmp_file = os.path.join(glove_dir, 'word2vec.glove.twitter.27B.{}d.txt'.format(dim) )\n",
    "#    _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "LOADGLOVE = False\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.glove.twitter.27B.100d.txt'))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "#r2 = requests.get(\"https://www.instituteforapprenticeships.org/api/fullstandards/\")\n",
    "#df_api= pd.DataFrame(r2.json())\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„',\"'m\", \"'re\", '£'])\n",
    "stopwords0 += tuple(set(list(df_nos['Developed By'])))\n",
    "stopwords0 += tuple(['cosvr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get raw data and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the NOS text data, together with the clean text and the tokens with only selected parts of speech.\n",
    "\n",
    "Then, there are several strategies we could use to convert the corpus into a document-term matrix. Some of the most common ones are using:\n",
    "- Counts\n",
    "- Term frequency - inverse document frequency metric (tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters for features extraction\n",
    "\n",
    "ngrams : uni/bi/tri\n",
    "\n",
    "tfidf thresholds: min and max percentage\n",
    "\n",
    "which parts of speech were selected before\n",
    "\n",
    "whether we are working at the level of suites or of invidual NOS, and how we aggregate NOS to form the suit level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create your TFidfVectorizer model. This doesn't depend on whether it's used on suites or NOS. However,\n",
    "# it does require that the docs collection is already given as a collection of tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, we will use a different tokenizer\n",
    "def define_tfidf(params, stopwords):\n",
    "    if params['ngrams'] == 'bi':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,2), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    elif params['ngrams'] == 'tri':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,3), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    else:\n",
    "        # unigrams is the default\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis, #lambda x: tokenize_asis(x,stopwords),\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, collect the text to transform\n",
    "def combine_nos_text(df_nos):\n",
    "    all_joint_tokens = []\n",
    "    # group by suites and concatenate all docs in it\n",
    "    row_names = []\n",
    "    for name, group in df_nos.groupby('One_suite'):\n",
    "        row_names.append(name)\n",
    "        joint_tokens = []\n",
    "        for idoc in group['pruned'].index:\n",
    "            joint_tokens += group['pruned'].loc[idoc]\n",
    "        all_joint_tokens.append(joint_tokens)\n",
    "    # return a dataframe\n",
    "    return pd.DataFrame({'tokens': all_joint_tokens}, index = row_names)\n",
    "\n",
    "def get_tfidf_matrix(params, df_nos, tfidf):\n",
    "    # Note: this can simply be used to get the tfidf transform, by setting bywhich=docs and any mode\n",
    "    t0 = time.time()\n",
    "    # first, get the dataframe of tokens\n",
    "    if params['bywhich'] == 'docs':\n",
    "        textfortoken = df_nos['pruned']\n",
    "        \n",
    "    elif params['bywhich'] == 'suites':\n",
    "        if params['mode'] == 'meantfidf':\n",
    "            textfortoken = df_nos['pruned']\n",
    "                \n",
    "        elif params['mode'] == 'combinedtfidf':\n",
    "            # note that this is the only case where the tfidf min and max are computed considering the number of \n",
    "            # suites as the number of elements in the collection.\n",
    "            # TODO: allow for the alternative case, where the transform is computed on individual NOS and then \n",
    "            # applied to the joint tokens\n",
    "            textfortoken = combine_nos_text(df_nos)['tokens']\n",
    "    \n",
    "    # apply tfidf transform to the tokenised text\n",
    "    tfidfm = tfidf.fit_transform(textfortoken)\n",
    "    \n",
    "    # if the average is needed, compute it and overwrite the matrix. Note that the step above is still needed to\n",
    "    # initialise the tfidf transform with the proper features and stopwords\n",
    "    if (params['bywhich'] == 'suites') and (params['mode'] =='meantfidf'):\n",
    "        row_names = df_nos['One_suite'].value_counts().index.values\n",
    "        tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "        for name, group in df_nos.groupby('One_suite'):\n",
    "            tmp = get_mean_tfidf(group['pruned'], tfidf)\n",
    "            tfidfm[igroup] = tmp\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    print_elapsed(t0, 'computing the feature vector')\n",
    "    return tfidfm, feature_names, tfidf, textfortoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(df, name, stopwords, top_n = 20):\n",
    "    all_keywords = []\n",
    "    count_keywords = {}\n",
    "    for ix in df.index:\n",
    "        if isinstance(df.loc[ix], list):\n",
    "            for ik in df.loc[ix]:\n",
    "                # I think that ik can be a collection of words separated by \";\"\n",
    "                #ik_elems = ik.split(';')\n",
    "                ik_elems = re.findall(r\"[\\w']+\", ik.replace('-',''))\n",
    "                # remove extra spaces\n",
    "                ik_elems = [elem.strip() for elem in ik_elems]\n",
    "                # remove digits\n",
    "                ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "                for elem in ik_elems:\n",
    "                    if elem not in stopwords:\n",
    "                        if elem not in all_keywords:\n",
    "                            all_keywords.append(elem)\n",
    "                            count_keywords[elem] = 1\n",
    "                        else:\n",
    "                            count_keywords[elem] += 1\n",
    "        elif isinstance(df.loc[ix],str):\n",
    "            ik_elems = re.findall(r\"[\\w']+\", df.loc[ix].replace('-',''))\n",
    "            #ik_elems = re.split('; |, ', df.loc[ix])\n",
    "            # remove extra spaces\n",
    "            ik_elems = [elem.strip() for elem in ik_elems]\n",
    "            # remove digits\n",
    "            ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "            for elem in ik_elems:\n",
    "                if elem not in stopwords:\n",
    "                    if elem not in all_keywords:\n",
    "                        all_keywords.append(elem)\n",
    "                        count_keywords[elem] = 1\n",
    "                    else:\n",
    "                        count_keywords[elem] += 1\n",
    "    n_repeated = np.sum(np.array(list(count_keywords.values()))>1)\n",
    "    n_keywords = len(all_keywords)\n",
    "    #print('Number of keywords repeated more than once for suite {} is {}. \\n'.format(name,\n",
    "    #                                                            n_repeated))\n",
    "    # get the top 20 keywords in terms of count\n",
    "    top_kw_indices = np.argsort(list(count_keywords.values()))[::-1][:top_n]\n",
    "    top_keywords = [k for t,k in enumerate(all_keywords) if t in top_kw_indices]\n",
    "    for _ in range(len(top_keywords),top_n):\n",
    "        top_keywords.append('-')\n",
    "    return top_keywords, n_keywords, n_repeated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['ngrams'] = 'uni'\n",
    "params['pofs'] = 'nv'\n",
    "params['tfidf_min'] = 3\n",
    "params['tfidf_max'] = 0.4\n",
    "\n",
    "params['bywhich'] = 'suites' #'docs' #'suites'\n",
    "params['mode'] = 'combinedtfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transform: this one can easily be the same for both keywords and the clustering\n",
    "tfidf = define_tfidf(params, []) #stopwords0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting important terms that describe NOS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at some of the terms with highest tf-idf score in each NOS suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEKW= False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features\n",
    "tfidfm, feature_names, tfidf, textfortokens = get_tfidf_matrix(params, df_nos, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('How many features: ')\n",
    "print(tfidfm.shape, len(feature_names))\n",
    "N = 2100\n",
    "print('\\n')\n",
    "print('Feature names: ')\n",
    "print(feature_names[N : N+50])\n",
    "print('\\n')\n",
    "print('Possible noise that escaped the TFIDF thresholds')\n",
    "print(set(no_idea_why_here_stopwords).intersection(set(feature_names)))\n",
    "print(set(more_stopwords).intersection(set(feature_names)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_terms_dict = {}\n",
    "#for name, group in ifa_df.groupby('Route'):\n",
    "igroup = 0\n",
    "n_keywords =[]\n",
    "n_repeated = []\n",
    "for name, group in df_nos.groupby('One_suite'):\n",
    "    top_terms = get_top_words(group['pruned'], feature_names, tfidf, n = 20)\n",
    "    top_keywords, n1, n2  = get_top_keywords(group['Keywords'], name, stopwords0, top_n = 20)\n",
    "    top_keywords = [t for t in top_keywords if t != '-']\n",
    "    n_keywords.append(n1)\n",
    "    n_repeated.append(n2)\n",
    "    if igroup<10:\n",
    "        print(name, top_terms, top_keywords)\n",
    "        print('**************************************')\n",
    "    top_terms_dict[name] = {}\n",
    "    top_terms_dict[name]['data-driven keywords'] = top_terms\n",
    "    top_terms_dict[name]['expert-driven keywords'] = top_keywords\n",
    "    top_terms_dict[name]['intersection of data- and expert-driven keywords'] = list(set(top_terms).intersection(\n",
    "        top_keywords))\n",
    "    top_terms_dict[name]['data-driven only keywords']=list(set(top_terms)-set(top_terms).intersection(top_keywords))\n",
    "    igroup += 1\n",
    "# save them all as csv\n",
    "if SAVEKW:\n",
    "    pd.DataFrame.from_dict(top_terms_dict, orient = 'index').to_csv(output_dir +\n",
    "                                                                '/Suites_top_terms_{}_{}.csv'.format(qualifier,pofs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tfidf.transform(group['pruned']),axis = 0).shape\n",
    "print(list(top_terms_dict.keys())[885:887])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we wanted to get not just the words, but their tf-idf scores too, we'll do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df_nos.groupby('One_suite'):    \n",
    "    if name in ['steelfixing construction','wood occupations construction','heritage skills construction',\n",
    "               'applied waterproof membranes construction']:\n",
    "        top_terms_weights = get_top_words_weights(group['pruned'], feature_names, tfidf, n = 20)\n",
    "        print(top_terms_weights.sort_values(by = 'tfidf', ascending = False).head(n=20))\n",
    "        print('-'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check keywords at the NOS level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
