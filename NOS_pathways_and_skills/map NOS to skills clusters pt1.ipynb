{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.decomposition.pca import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_bg import * #nesta_colours, nesta_colours_combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extract the top keywords for each NOS - it will need to be merged with another notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('-')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def get_mean_vec(skill_list, model):\n",
    "    skill_list_conv = [convert_to_undersc(elem) for elem in skill_list]\n",
    "    vector_list = [model[elem] for elem in skill_list_conv if elem in model]\n",
    "    vec_array = np.asarray(vector_list)\n",
    "    avg_vec = np.mean(vec_array, axis=0)\n",
    "    return avg_vec\n",
    "\n",
    "def get_average_skill_category(list_of_skills, reference_dict):\n",
    "    \"\"\"\n",
    "    Returns top 10 categories in the averaged cosine sim array.\n",
    "    \"\"\"\n",
    "    pruned_skills = [elem for elem in list_of_skills if elem in reference_dict]\n",
    "    if len(pruned_skills):\n",
    "        vec_list = [reference_dict[skill] for skill in pruned_skills]\n",
    "        vec_array = np.asarray(vec_list)\n",
    "        avg_vec = np.mean(vec_array, axis=0)\n",
    "        sorted_vecs = np.argsort(avg_vec)[0, -10:]\n",
    "        scores = [avg_vec[0,i] for i in sorted_vecs]\n",
    "        categories_values = zip(sorted_vecs, scores)\n",
    "        res = list(categories_values)\n",
    "    else:\n",
    "        res = []\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_best_skill_category(list_of_skills, reference_dict, transversal):\n",
    "    top10 = get_average_skill_category(list_of_skills, reference_dict)\n",
    "    if len(top10):\n",
    "        dom_specific = [elem for elem in top10 if elem[0] not in transversal]\n",
    "        best = max(dom_specific, key = lambda x: x[1])\n",
    "    else:\n",
    "        best = (999, 0.0)\n",
    "    return best\n",
    "    \n",
    "lookup_dir = ''\n",
    "output_dir = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_gensim(list_of_terms, some_model):\n",
    "    # replace space with underscore\n",
    "    new_terms = [convert_to_undersc(elem) for elem in list_of_terms]\n",
    "    # check if each element in the list is in the model\n",
    "    is_in = [elem for elem in new_terms if elem in some_model]\n",
    "    # only return the element in the model\n",
    "    return is_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_with_pos(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i, pos_to_wornet_dict[p]) if i not in keep_asis else i for i,p in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def stem_features(s, ps):\n",
    "    return ps.stem(s)\n",
    "    \n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    #Â it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('â-â¢âââÂµÂ¾Ã¢Ã£Ã©Ëï¬[â¬â¢Â¢Â±Ã¯â¦Ë')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list): #, stopwords):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]# if elem.lower() not in stopwords]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for BG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgsoc_is_newsoc(data):\n",
    "    return np.floor(data['SOC']/10) == data['new_soc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'nv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ps = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tags = [(t,p) for t,p in df_nos['tagged_tokens'].iloc[0] if p[:1] in ['V','N']]\n",
    "#A = [stem_features(t,ps) for t in df_nos['pruned'].iloc[0]]\n",
    "A2 = lemmatise_with_pos(tags) #[stem_features(t,stemmer) for t in df_nos['pruned'].iloc[0]]\n",
    "#A = list(set(A))\n",
    "#A2 = list(set(A2))\n",
    "#B = [t in model for t in A]\n",
    "B2 = [t in model for t in A2]\n",
    "#C = [t for t in A if t in model]\n",
    "C2 = [t for t in A2 if t in model]\n",
    "print(len(df_nos['pruned'].iloc[0]),len(A2))\n",
    "print(A2)\n",
    "#print(sum(B),sorted(A))\n",
    "#print(sum(B2),sorted(A2))\n",
    "#plt.scatter(model['valu'],model['value'])\n",
    "'''\n",
    "print('playground')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "WHICH_GLOVE = 'glove.6B.100d' #'glove.6B.100d' #'glove.840B.300d', \n",
    "#glove.twitter.27B.100d\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/'\n",
    "\n",
    "LOADGLOVE = True\n",
    "if LOADGLOVE:\n",
    "    print('Loading glove model')\n",
    "    t0 = time.time()\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.{}.txt'.format(WHICH_GLOVE)))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors\n",
    "    # from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "    print(time.time() - t0)\n",
    "\n",
    "vector_matrix = model.vectors\n",
    "list_of_terms = model.index2word\n",
    "\n",
    "lookup_terms = [convert_from_undersc(elem) for elem in list_of_terms]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['Â¤', 'Â¨', 'Î¼', 'à¸', 'à¸¢', 'á¶', 'â°', 'Â©', 'Æ', 'Â°', 'â'])\n",
    "stopwords0 += tuple(['Â¤', 'Â¨', 'Î¼', 'à¸', 'à¸¢', 'á¶', 'â°', 'Â©', 'Æ', 'Â°', 'â',\"'m\", \"'re\", 'Â£'])\n",
    "stopwords0 += tuple(set(list(df_nos['Developed By'])))\n",
    "stopwords0 += tuple(['cosvr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another column where the texts are lemmatised properly\n",
    "def lemmatise_pruned(x, pofs):\n",
    "    if pofs == 'nv':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['V','N']]\n",
    "    elif pofs == 'n':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['N']]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return lemmatise_with_pos(tags)\n",
    "\n",
    "t0 = time.time()\n",
    "df_nos['pruned_lemmas'] = df_nos['tagged_tokens'].map(lambda x: lemmatise_pruned(x,pofs))\n",
    "print(time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep NOS from a super-suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_suites_files=  '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/NOS_Suite_Priority.xlsx'\n",
    "super_suites_names = ['Engineering','Management','FinancialServices','Construction']\n",
    "all_super_suites = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_super_suites[which_super_suite] = pd.read_excel(super_suites_files, sheet_name = which_super_suite)\n",
    "    all_super_suites[which_super_suite]['NOS Suite name'] = all_super_suites[which_super_suite]['NOS Suite name'].map(\n",
    "        lambda x: x.replace('(','').replace('(','').replace('&','and').strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "all_matches = {}\n",
    "all_match_names = {}\n",
    "#match_name = []\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_matches[which_super_suite] = []\n",
    "    for suite in all_super_suites[which_super_suite]['NOS Suite name'].values:\n",
    "        # do manually some selected suites\n",
    "        if 'insurance claims' in suite:\n",
    "            tmp = standard_labels.index('general insurance')\n",
    "            all_matches[which_super_suite].append(tmp)\n",
    "            continue\n",
    "        # for the \"management and leadership marketing 2013\" both marketing and marketing 2013 would fit,\n",
    "        # but I'm only taking the latter\n",
    "        # find a fuzzy match between \n",
    "        out = process.extract(suite, standard_labels, limit=3)\n",
    "        if len(out) and out[0][1]>89:\n",
    "            # note: most of them are above 96% similarity (only one is 90%)\n",
    "            tmp = standard_labels.index(out[0][0])\n",
    "            #print(suite, out[0])\n",
    "            if tmp not in all_matches[which_super_suite]:\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "            else:\n",
    "                if suite == 'installing domestic fascia, soffit, and bargeboards':\n",
    "                    # this suite is kind of a duplicate - I aggregated it in my suites list\n",
    "                    continue\n",
    "                tmp = standard_labels.index(out[2][0])\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "                print(out[0][0],',',out[1][0],',',out[2][0],',',suite)\n",
    "        else:\n",
    "            print(suite, ' not found')\n",
    "            print(out)\n",
    "            print('\\n')\n",
    "    print(len(all_matches[which_super_suite]),len(all_super_suites[which_super_suite]))\n",
    "    all_match_names[which_super_suite] = [standard_labels[t] for t in all_matches[which_super_suite]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_supersuite(x):\n",
    "    for supersuite in all_match_names.keys():\n",
    "        if x in all_match_names[supersuite]:\n",
    "            return supersuite.lower()\n",
    "    # if no match has been found\n",
    "    return 'other'\n",
    "\n",
    "def adjustsoccode(x):\n",
    "    y = re.findall(r\"[\\d']+\", str(x))\n",
    "    if len(y):\n",
    "        return y[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract2digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:2])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract3digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:3])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract1digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df_nos['supersuite'] = df_nos['One_suite'].apply(assign_supersuite)\n",
    "# extract 2 digit soc\n",
    "df_nos['SOC4'] = df_nos['Clean SOC Code'].map(adjustsoccode)\n",
    "df_nos['SOC1'] = df_nos['SOC4'].map(extract1digits)\n",
    "df_nos['SOC2'] = df_nos['SOC4'].map(extract2digits)\n",
    "df_nos['SOC3'] = df_nos['SOC4'].map(extract3digits)\n",
    "print(df_nos['supersuite'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(max(df_nos['Clean SOC Code'].map(lambda x: len(x) if isinstance(x,list) else 0).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select NOS in super-suites of interest\n",
    "df_nos_select = df_nos[~(df_nos['supersuite']=='other')]\n",
    "print(len(df_nos_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get raw data and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters for features extraction\n",
    "\n",
    "ngrams : uni/bi/tri\n",
    "\n",
    "tfidf thresholds: min and max percentage\n",
    "\n",
    "which parts of speech were selected before\n",
    "\n",
    "whether we are working at the level of suites or of invidual NOS, and how we aggregate NOS to form the suit level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create your TFidfVectorizer model. This doesn't depend on whether it's used on suites or NOS. However,\n",
    "# it does require that the docs collection is already given as a collection of tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, we will use a different tokenizer\n",
    "def define_tfidf(params, stopwords):\n",
    "    if params['ngrams'] == 'bi':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,2), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    elif params['ngrams'] == 'tri':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,3), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    else:\n",
    "        # unigrams is the default\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis, #lambda x: tokenize_asis(x,stopwords),\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, collect the text to transform\n",
    "def combine_nos_text(df_nos_select, col = 'pruned'):\n",
    "    all_joint_tokens = []\n",
    "    # group by suites and concatenate all docs in it\n",
    "    row_names = []\n",
    "    for name, group in df_nos_select.groupby('One_suite'):\n",
    "        row_names.append(name)\n",
    "        joint_tokens = []\n",
    "        for idoc in group[col].index:\n",
    "            joint_tokens += group[col].loc[idoc]\n",
    "        all_joint_tokens.append(joint_tokens)\n",
    "    # return a dataframe\n",
    "    return pd.DataFrame({'tokens': all_joint_tokens}, index = row_names)\n",
    "\n",
    "def get_tfidf_matrix(params, df_nos_select, tfidf, col = 'pruned'):\n",
    "    # Note: this can simply be used to get the tfidf transform, by setting bywhich=docs and any mode\n",
    "    t0 = time.time()\n",
    "    # first, get the dataframe of tokens\n",
    "    if params['bywhich'] == 'docs':\n",
    "        textfortoken = df_nos_select[col]\n",
    "        \n",
    "    elif params['bywhich'] == 'suites':\n",
    "        if params['mode'] == 'meantfidf':\n",
    "            textfortoken = df_nos_select[col]\n",
    "                \n",
    "        elif params['mode'] == 'combinedtfidf':\n",
    "            # note that this is the only case where the tfidf min and max are computed considering the number of \n",
    "            # suites as the number of elements in the collection.\n",
    "            # TODO: allow for the alternative case, where the transform is computed on individual NOS and then \n",
    "            # applied to the joint tokens\n",
    "            textfortoken = combine_nos_text(df_nos_select, col = col)['tokens']\n",
    "    \n",
    "    # apply tfidf transform to the tokenised text\n",
    "    tfidfm = tfidf.fit_transform(textfortoken)\n",
    "    \n",
    "    # if the average is needed, compute it and overwrite the matrix. Note that the step above is still needed to\n",
    "    # initialise the tfidf transform with the proper features and stopwords\n",
    "    if (params['bywhich'] == 'suites') and (params['mode'] =='meantfidf'):\n",
    "        row_names = df_nos_select['One_suite'].value_counts().index.values\n",
    "        tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "        for name, group in df_nos_select.groupby('One_suite'):\n",
    "            tmp = get_mean_tfidf(group[col], tfidf)\n",
    "            tfidfm[igroup] = tmp\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    print_elapsed(t0, 'computing the feature vector')\n",
    "    return tfidfm, feature_names, tfidf, textfortoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(df, name, stopwords, top_n = 20):\n",
    "    all_keywords = []\n",
    "    count_keywords = {}\n",
    "    for ix in df.index:\n",
    "        if isinstance(df.loc[ix], list):\n",
    "            for ik in df.loc[ix]:\n",
    "                # I think that ik can be a collection of words separated by \";\"\n",
    "                #ik_elems = ik.split(';')\n",
    "                ik_elems = re.findall(r\"[\\w']+\", ik.replace('-',''))\n",
    "                # remove extra spaces\n",
    "                ik_elems = [elem.strip() for elem in ik_elems]\n",
    "                # remove digits\n",
    "                ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "                for elem in ik_elems:\n",
    "                    if elem not in stopwords:\n",
    "                        if elem not in all_keywords:\n",
    "                            all_keywords.append(elem)\n",
    "                            count_keywords[elem] = 1\n",
    "                        else:\n",
    "                            count_keywords[elem] += 1\n",
    "        elif isinstance(df.loc[ix],str):\n",
    "            ik_elems = re.findall(r\"[\\w']+\", df.loc[ix].replace('-',''))\n",
    "            #ik_elems = re.split('; |, ', df.loc[ix])\n",
    "            # remove extra spaces\n",
    "            ik_elems = [elem.strip() for elem in ik_elems]\n",
    "            # remove digits\n",
    "            ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "            for elem in ik_elems:\n",
    "                if elem not in stopwords:\n",
    "                    if elem not in all_keywords:\n",
    "                        all_keywords.append(elem)\n",
    "                        count_keywords[elem] = 1\n",
    "                    else:\n",
    "                        count_keywords[elem] += 1\n",
    "    n_repeated = np.sum(np.array(list(count_keywords.values()))>1)\n",
    "    n_keywords = len(all_keywords)\n",
    "    #print('Number of keywords repeated more than once for suite {} is {}. \\n'.format(name,\n",
    "    #                                                            n_repeated))\n",
    "    # get the top 20 keywords in terms of count\n",
    "    top_kw_indices = np.argsort(list(count_keywords.values()))[::-1][:top_n]\n",
    "    top_keywords = [k for t,k in enumerate(all_keywords) if t in top_kw_indices]\n",
    "    for _ in range(len(top_keywords),top_n):\n",
    "        top_keywords.append('-')\n",
    "    return top_keywords, n_keywords, n_repeated\n",
    "\n",
    "def get_top_keywords_nos(nos,stopwords, top_n = 20):\n",
    "    all_keywords = []\n",
    "    count_keywords = {}\n",
    "    if isinstance(nos, list):\n",
    "        for ik in nos:\n",
    "            # I think that ik can be a collection of words separated by \";\"\n",
    "            #ik_elems = ik.split(';')\n",
    "            ik_elems = re.findall(r\"[\\w']+\", ik.replace('-',''))\n",
    "            # remove extra spaces\n",
    "            ik_elems = [elem.strip() for elem in ik_elems]\n",
    "            # remove digits\n",
    "            ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "            for elem in ik_elems:\n",
    "                if elem not in stopwords:\n",
    "                    if elem not in all_keywords:\n",
    "                        all_keywords.append(elem)\n",
    "                        count_keywords[elem] = 1\n",
    "                    else:\n",
    "                        count_keywords[elem] += 1\n",
    "    elif isinstance(nos,str):\n",
    "        ik_elems = re.findall(r\"[\\w']+\", nos.replace('-',''))\n",
    "        #ik_elems = re.split('; |, ', nos)\n",
    "        # remove extra spaces\n",
    "        ik_elems = [elem.strip() for elem in ik_elems]\n",
    "        # remove digits\n",
    "        ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "        for elem in ik_elems:\n",
    "            if elem not in stopwords:\n",
    "                if elem not in all_keywords:\n",
    "                    all_keywords.append(elem)\n",
    "                    count_keywords[elem] = 1\n",
    "                else:\n",
    "                    count_keywords[elem] += 1\n",
    "    n_repeated = np.sum(np.array(list(count_keywords.values()))>1)\n",
    "    n_keywords = len(all_keywords)\n",
    "    #print('Number of keywords repeated more than once for suite {} is {}. \\n'.format(name,\n",
    "    #                                                            n_repeated))\n",
    "    # get the top 20 keywords in terms of count\n",
    "    top_kw_indices = np.argsort(list(count_keywords.values()))[::-1][:top_n]\n",
    "    top_keywords = [k for t,k in enumerate(all_keywords) if t in top_kw_indices]\n",
    "    for _ in range(len(top_keywords),top_n):\n",
    "        top_keywords.append('-')\n",
    "    return top_keywords, n_keywords, n_repeated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['ngrams'] = 'uni'\n",
    "params['pofs'] = 'nv'\n",
    "params['tfidf_min'] = 3\n",
    "params['tfidf_max'] = 0.5\n",
    "\n",
    "params['bywhich'] = 'docs' #'docs' #'suites'\n",
    "params['mode'] = 'tfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transform: this one can easily be the same for both keywords and the clustering\n",
    "tfidf = define_tfidf(params, stopwords0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check keywords at the NOS level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at some of the terms with highest tf-idf score in each NOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEKW= False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features\n",
    "tfidfm, feature_names, tfidf, textfortokens = get_tfidf_matrix(params, df_nos_select, tfidf, col = 'pruned_lemmas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of features: {}'.format(len(feature_names)))\n",
    "N = 2000\n",
    "print('Some features:')\n",
    "print(feature_names[N:N+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import timeit\n",
    "#timeit.timeit(lambda: tfidfm.todense(), number = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_terms_dict = {}\n",
    "top_keywords_dict = {}\n",
    "#for name, group in ifa_df.groupby('Route'):\n",
    "igroup = 0\n",
    "n_keywords =[]\n",
    "n_repeated = []\n",
    "#top_terms = {}\n",
    "t0 = time.time()\n",
    "tfidfm_dense = tfidfm.todense()\n",
    "for ix,name in enumerate(df_nos_select.index):\n",
    "    #top_terms = get_top_words(df_nos_select.loc[name]['pruned'], feature_names, tfidf, n = 20)\n",
    "    top_ngrams = np.argsort(tfidfm_dense[ix,:])\n",
    "    top_ngrams = top_ngrams.tolist()[0][-20:]\n",
    "    top_ngrams = top_ngrams[::-1]\n",
    "    # only retain the ones with non zero features\n",
    "    top_ngrams = [elem for elem in top_ngrams if tfidfm_dense[ix,elem]>0]    \n",
    "    top_features = [feature_names[elem] for elem in top_ngrams]\n",
    "    top_terms_dict[name] = {}\n",
    "    top_terms_dict[name] = top_features\n",
    "    if ix<4:\n",
    "        print(name, top_features) #, top_keywords)\n",
    "        print('**************************************')\n",
    "    \n",
    "    #top_keywords, n1, n2  = get_top_keywords_nos(df_nos_select.loc[name]['Keywords'], stopwords0, top_n = 20)\n",
    "    #top_keywords = [t for t in top_keywords if t != '-']\n",
    "    #n_keywords.append(n1)\n",
    "    #n_repeated.append(n2)\n",
    "    #top_keywords_dict[name] = {}\n",
    "    #top_keywords_dict[name] = top_keywords\n",
    "    if ix % 1000 == 999:\n",
    "        print('Got to NOS nb {}. Total time elapsed: {:.4f} s'.format(ix,time.time()-t0))\n",
    "# save them all as csv\n",
    "if SAVEKW or True:\n",
    "    pd.DataFrame.from_dict(top_terms_dict, orient = 'index').to_csv(output_dir +\n",
    "                                                '/NOS_from_supersuites_top_terms_{}_{}.csv'.format(qualifier,pofs))\n",
    "tfidfm_dense = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to check results\n",
    "'''\n",
    "print(list(top_terms_dict.keys())[885:887])\n",
    "top_terms_weights = get_top_words_weights([df_nos_select.iloc[0]['pruned_lemmas']], feature_names, tfidf, n = 20)\n",
    "print(top_terms_weights.sort_values(by = 'tfidf', ascending = False).head(n=20))\n",
    "'''\n",
    "# note that the get_top_words_weights function is probably wrong - but it doesn't matter now\n",
    "print('not now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove top terms that are not in the chosen gensim model\n",
    "new_top_terms_dict = {}\n",
    "for k,v in top_terms_dict.items():\n",
    "    # check if the top terms for each document are in the gensim model\n",
    "    new_top_terms = prep_for_gensim(v, model)\n",
    "    # only retains the ones in the model\n",
    "    new_top_terms_dict[k] = new_top_terms\n",
    "    if np.random.randn(1)>3:\n",
    "        print(k, new_top_terms, len(new_top_terms), len(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find average vector per skill cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the file with the skills taxonomy lower layer\n",
    "with open('/Users/stefgarasto/Google Drive/Documents/scripts/NOS/bottom_cluster_membership.pkl', 'rb') as infile:\n",
    "    bottom_layer = pickle.load(infile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect skills in clusters\n",
    "skill_cluster_membership = {}\n",
    "for clus in collections.Counter(bottom_layer.values()):\n",
    "    cluster_skills = [elem for elem in bottom_layer if \\\n",
    "                      bottom_layer[elem] == clus]\n",
    "    skill_cluster_membership[clus] = cluster_skills\n",
    "\n",
    "print(list(skill_cluster_membership.keys())[::10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lookup vecs using pre-trained GloVe model\n",
    "#Â I guess this is to get a mean vector for each skills cluster\n",
    "skill_cluster_vecs = {}\n",
    "for clus in skill_cluster_membership:\n",
    "    cluster_skills = skill_cluster_membership[clus]\n",
    "    new_skills = [convert_to_undersc(elem) for elem in cluster_skills]\n",
    "    other_skills = [elem.split() for elem in cluster_skills if len(elem)>1]\n",
    "    flat_other_skills = [item for sublist in other_skills for item in sublist]\n",
    "    all_skills = new_skills + list(set(flat_other_skills))\n",
    "    skills_in = [elem for elem in all_skills if elem in model]\n",
    "    print(clus, len(cluster_skills), len(skills_in))\n",
    "    skill_cluster_vecs[clus] = get_mean_vec(skills_in, \n",
    "                            model)\n",
    "\n",
    "# check all skill clusters have a vector: check has to be empty\n",
    "check = [k for k,v in skill_cluster_vecs.items() if len(v.shape) == 0]\n",
    "print('This should be empty.',check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join(output_dir, 'skill_cluster_vecs_pretrained.pkl'), 'wb') as f:\n",
    "#    pickle.dump(skill_cluster_vecs, f)\n",
    " \n",
    "# show words in the model that are closest to average vector for each skill cluster\n",
    "for clus in list(skill_cluster_vecs.keys())[:10]:\n",
    "    print(clus)\n",
    "    print(model.similar_by_vector(skill_cluster_vecs[clus]))\n",
    "    print('***********')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange all mean skill vectors in a matrix\n",
    "comparison_vecs = np.vstack(list(skill_cluster_vecs.values()))\n",
    "clus_names = list(skill_cluster_vecs.keys())\n",
    "\n",
    "print(clus_names[:10], comparison_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign each NOS to a skill cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_similarity(test_skills, comparison_vecs, clus_names):\n",
    "    sims = cosine_similarity(test_skills.reshape(1,-1), comparison_vecs)\n",
    "    \n",
    "    top_sims = np.argsort(sims)[:, -5:].tolist()[0]\n",
    "    top_sim_vals = [sims[0, elem] for elem in top_sims]\n",
    "    top_sim_clus = [clus_names[elem] for elem in top_sims]\n",
    "    top_sims_res = list(zip(reversed(top_sim_clus), reversed(top_sim_vals)))\n",
    "    if 'dental assistance' == top_sim_clus[0]: #np.random.randn(1)>3:\n",
    "        #print(df_nos_select['NOS Title'].loc[k], new_top_terms_dict[k], top_sim_clus)\n",
    "        #counter +=1\n",
    "        # do manual adjustment\n",
    "        top_sims_res = top_sims_res[1:]\n",
    "    return top_sims_res\n",
    "\n",
    "def highest_similarity(test_skills, comparison_vecs, clus_names):\n",
    "    sims = cosine_similarity(test_skills.reshape(1,-1), comparison_vecs)\n",
    "    \n",
    "    top_sims = np.argsort(sims)[:, -2:].tolist()[0]\n",
    "    top_sim_vals = [sims[0, elem] for elem in top_sims]\n",
    "    top_sim_clus = [clus_names[elem] for elem in top_sims]\n",
    "    #top_sims_res = list(zip(reversed(top_sim_clus), reversed(top_sim_vals)))\n",
    "    if 'dental assistance' == top_sim_clus[0]: #np.random.randn(1)>3:\n",
    "        #print(df_nos_select['NOS Title'].loc[k], new_top_terms_dict[k], top_sim_clus)\n",
    "        #counter +=1\n",
    "        # do manual adjustment\n",
    "        top_sim_clus = top_sim_clus[1]\n",
    "    else:\n",
    "        top_sim_clus = top_sim_clus[0]\n",
    "    return top_sim_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_v_clus = {}\n",
    "counter = 0\n",
    "for ix,k in enumerate(new_top_terms_dict):\n",
    "    test_skills = get_mean_vec(new_top_terms_dict[k], model)\n",
    "\n",
    "    #sims = cosine_similarity(test_skills.reshape(1,-1), comparison_vecs)\n",
    "    #top_sims = np.argsort(sims)[:, -5:].tolist()[0]\n",
    "    #top_sim_vals = [sims[0, elem] for elem in top_sims]\n",
    "    #top_sim_clus = [clus_names[elem] for elem in top_sims]\n",
    "    #top_sims_res = list(zip(reversed(top_sim_clus), reversed(top_sim_vals)))\n",
    "    #if ('dental assistance' == top_sim_clus[0]) & (counter<6): #np.random.randn(1)>3:\n",
    "    #    print(df_nos_select['NOS Title'].loc[k], new_top_terms_dict[k], top_sim_clus)\n",
    "    #    counter +=1\n",
    "    #    # do manual adjustment\n",
    "    #    top_sims_res = top_sims_res[1:]\n",
    "    #st_v_clus[k] = top_sims_res \n",
    "    \n",
    "    #st_v_clus[k] = high_similarity(test_skills, comparison_vecs, clus_names)\n",
    "    st_v_clus[k] = highest_similarity(test_skills, comparison_vecs, clus_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the best clusters to the nos dataframe\n",
    "tmp = pd.DataFrame.from_dict(st_v_clus, orient = 'index')\n",
    "tmp = tmp.rename(columns = {0: 'best_cluster_nos'})\n",
    "df_nos['best_cluster_nos'] = tmp['best_cluster_nos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign each job advert to a skill cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vectors_nofile(x,model):\n",
    "    # compute a word embedding for a sentence as the average over the words\n",
    "    # that make up the job title\n",
    "    counter_oov = 0\n",
    "    # first check whether the whole word is in the model\n",
    "    y = convert_to_undersc(x)\n",
    "    if y in model.vocab:\n",
    "        return model[y], 1\n",
    "    # if not, reconvert to discrete words and take the average of them\n",
    "    y = convert_from_undersc(y)\n",
    "    # split the sentence into words after removing hyphens: THERE IS NONE\n",
    "    y = y.split()\n",
    "    # remove extra spaces, genitives \"'s\"\n",
    "    y = [t.strip().lower().replace('\\'s','') for t in y]\n",
    "\n",
    "    # initialise word embedding and we counter\n",
    "    we = np.zeros((100), dtype = np.float32)\n",
    "    we_counter = 0\n",
    "    missed = []\n",
    "    for t in y:\n",
    "        if t in model.vocab:\n",
    "            we += model[t]\n",
    "            we_counter += 1\n",
    "        else:\n",
    "            we_tmp, flag_oov, recovered_words = oov_to_vectors(t, model)\n",
    "            if flag_oov:\n",
    "                we += we_tmp\n",
    "                we_counter += 1\n",
    "            else:\n",
    "                missed.append(t)\n",
    "                counter_oov += 1\n",
    "    # normalise by the number of embeddings\n",
    "    if we_counter>0:\n",
    "        we = we/we_counter\n",
    "    return we, we_counter\n",
    "\n",
    "#%%\n",
    "def jt_to_vectors_nofile(x, model):\n",
    "    # transform a job title into a word embedding\n",
    "    we, we_counter = sentence_to_vectors_nofile(x,model)\n",
    "    if we_counter>0:\n",
    "        return np.float32(we)\n",
    "    else:\n",
    "        # if nothing has been turned to word embedding then this is 0\n",
    "        return np.zeros((100), dtype = np.float32)\n",
    "    \n",
    "#%%\n",
    "def skills_to_vectors_nofile(x, model):\n",
    "    # compute a word embedding for the list of skills as an average of averages\n",
    "    if isinstance(x, str):\n",
    "        skills = eval(x)\n",
    "    else:\n",
    "        skills = eval(x.values[0])\n",
    "    we= np.zeros((100), dtype = np.float32)\n",
    "    we_counter = 0\n",
    "    for z in skills:\n",
    "        we_skill, skill_counter = sentence_to_vectors_nofile(z, model)\n",
    "        # note that we_skill is already normalised by skill_counter\n",
    "        if skill_counter>0:\n",
    "            # if at least one word making up this skill was in the vocabulary,\n",
    "            # add it to the overall word embedding:\n",
    "            we += we_skill\n",
    "            we_counter += 1\n",
    "    if we_counter>0:\n",
    "        we = we/we_counter\n",
    "    return np.float32(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole dataset\n",
    "filename= os.path.join('/Users/stefgarasto/Local-Data/Burning-glass/',\n",
    "     'Job_ads_2012_2018/{}_reliable_soc_ads.csv')\n",
    "\n",
    "all_years = ['2012','2013','2014','2015','2016','2017','2018']\n",
    "\n",
    "print('Loading the dataset')\n",
    "t0 = time.time()\n",
    "\n",
    "FULLDS = False\n",
    "if FULLDS:\n",
    "    for ix,year in enumerate(all_years):\n",
    "        if ix == 0:\n",
    "            bgdata = pd.read_csv(filename.format(year))\n",
    "        else:\n",
    "            bgdata = pd.concat((bgdata, pd.read_csv(filename.format(year))))\n",
    "            print(len(bgdata))\n",
    "else:\n",
    "    bgdata = pd.read_csv(filename.format('2012'))\n",
    "    \n",
    "print('Time in minutes: {:.4f}'.format((time.time()- t0)/60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract good portions of data\n",
    "bgdata['empty_clusters'] = bgdata['clusters'].map(lambda x: len(x)==2)\n",
    "print(bgdata['empty_clusters'].sum()/len(bgdata))\n",
    "\n",
    "soc_flag = bgsoc_is_newsoc(bgdata)\n",
    "print(((~bgdata['MinEdu'].isnull()) & (soc_flag) & (~bgdata['empty_clusters'])).sum()/len(bgdata))\n",
    "\n",
    "edu_or_exp = (~bgdata['MinEdu'].isnull()) | (~bgdata['MinExp'].isnull())\n",
    "\n",
    "bgdata_select = bgdata[edu_or_exp & (soc_flag) & (~bgdata['empty_clusters'])]\n",
    "#bgdata_exp = bgdata[(~bgdata['MinExp'].isnull()) & (soc_flag) & (~bgdata['empty_clusters'])]\n",
    "\n",
    "bgdata_select['Eduv2'] = bgdata_select['MinEdu'].map(group_eduv2)\n",
    "bgdata_select['Exp3'] = bgdata_select['MinExp'].map(group_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average word embedding for the skills\n",
    "t0 = time.time()\n",
    "print('Computing the word embedding for the skills')\n",
    "bgdata_select['skills_embedding'] = bgdata_select['converted_skills'].map(\n",
    "        lambda x: skills_to_vectors_nofile(x, model))\n",
    "print_elapsed(t0, 'above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute average word embedding for the skills\n",
    "#t0 = time.time()\n",
    "#print('Computing the word embedding for the skills')\n",
    "#bgdata_select['skills_embedding'] = bgdata_exp['converted_skills'].map(\n",
    "#        lambda x: skills_to_vectors_nofile(x, model))\n",
    "#print_elapsed(t0, 'above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match average skills embedding to closest cluster\n",
    "t0 = time.time()\n",
    "#ja_edu_v_clus = {}\n",
    "bgdata_select['best_cluster'] = bgdata_select['skills_embedding'].map(lambda x: highest_similarity(x, \n",
    "                                                                                    comparison_vecs, clus_names))\n",
    "#for ix,k in enumerate(bgdata_edu.index):\n",
    "#    test_skills = bgdata_edu['skills_embedding'].loc[k]\n",
    "#\n",
    "#    ja_edu_v_clus[k] = high_similarity(test_skills, comparison_vecs, clus_names)\n",
    "\n",
    "print_elapsed(t0,'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "##ja_exp_v_clus = {}\n",
    "##for ix,k in enumerate(bgdata_exp.index):\n",
    "##    test_skills = bgdata_exp['skills_embedding'].loc[k]\n",
    "##\n",
    "##    ja_exp_v_clus[k] = high_similarity(test_skills, comparison_vecs, clus_names)\n",
    "#bgdata_exp['best_cluster'] = bgdata_exp['skills_embedding'].map(lambda x: highest_similarity(x, \n",
    "#                                                                                comparison_vecs, clus_names))\n",
    "#print_elapsed(t0,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing the average salary')\n",
    "bgdata_select['MeanSalary'] = (bgdata_select['MinSalary'] + bgdata_select['MaxSalary'])/2.0\n",
    "#bgdata_select['MeanSalary'] = bgdata_select['MeanSalary'].fillna(\n",
    "#        bgdata_select['MeanSalary'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_groups = bgdata_select.groupby(by = 'best_cluster')\n",
    "#exp_groups = bgdata_exp.groupby(by = 'best_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Quick way: just pick the cluster to which skills are most associated\n",
    "'''\n",
    "cluster_counters = []\n",
    "t0 = time.time()\n",
    "for ix, k in enumerate(bgdata_edu.index):\n",
    "    cluster_counters.append(collections.Counter(eval(bgdata_edu['clusters'].loc[k])))\n",
    "    if ix%200000 == 199999:\n",
    "        print(ix)\n",
    "print((time.time()- t0)//60)\n",
    "'''\n",
    "print('not now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def check_dominant_cluster(x):\n",
    "    tmp = sorted(list(x.values()))[::-1]\n",
    "    if len(tmp)>1:\n",
    "        return tmp[0] - tmp[1]\n",
    "    else:\n",
    "        return tmp[0]\n",
    "\n",
    "t0 =time.time()\n",
    "check = [check_dominant_cluster(elem) for elem in cluster_counters]\n",
    "print_elapsed(t0)'''\n",
    "print('not now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.hist(check)\n",
    "print((sum([elem>0 for elem in check])))\n",
    "print((sum([elem>0 for elem in check]))/len(check))'''\n",
    "print('not now')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOS are now mapped to job adverts via the skill cluster membership\n",
    "#### extract the exp/edu/salary requirements\n",
    "\n",
    "That is: \n",
    "1. for each skill cluster, find the distribution of experience, education, SOC (first digit) and salary from the job advert dataset\n",
    "2. For each distribution, find peak education/experience/SOC1 and median salary\n",
    "3. Assign the correct distribution and peak requirements to NOS according to skill cluster membership\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "\n",
    "cols_v_clus = {}\n",
    "\n",
    "for name,group in bg_groups:\n",
    "    cols_v_clus[name] = {}\n",
    "    if not len(group):\n",
    "        print(name)\n",
    "        continue\n",
    "    for col in ['Exp3','Eduv2','MeanSalary','SOC']:\n",
    "        cols_v_clus[name][col] = group[col].value_counts()\n",
    "        if col == 'MeanSalary':\n",
    "            cols_v_clus[name][col + '-peak'] = np.nanmean(group[col])\n",
    "        else:\n",
    "            try:\n",
    "                cols_v_clus[name][col + '-peak'] = cols_v_clus[name][col].idxmax()\n",
    "            except:\n",
    "                cols_v_clus[name][col + '-peark'] = 'unknown'\n",
    "    ix+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgdata_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select NOS from engineering\n",
    "engineering_nos = df_nos[df_nos['supersuite']== 'engineering']\n",
    "print(len(engineering_nos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nos_to_req(x,col,cols_v_clus):\n",
    "    return cols_v_clus[x][col + '-peak']\n",
    "\n",
    "for col in ['Exp3','Eduv2','MeanSalary','SOC']:\n",
    "    engineering_nos[col + '-peak'] = engineering_nos['best_cluster_nos'].map(\n",
    "                            lambda x: map_nos_to_req(x,col,cols_v_clus))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineering_nos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_groups = engineering_nos.groupby(by = 'One_suite')\n",
    "for name, group in eng_groups:\n",
    "    print(name, len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = eng_groups.get_group('aeronautical engineering suite 3')\n",
    "group = group.rename(columns = {'Eduv2-peak': 'Qualification requirements'})\n",
    "sns.swarmplot(data = group, \n",
    "                x = 'Exp3-peak', y ='MeanSalary-peak', hue = 'Qualification requirements',\n",
    "              order = ['Entry-level', 'Mid-level','Senior-level'])\n",
    "#               x_bins = ['Entry-level', 'Mid-level','Senior-level'],\n",
    "#               y_bins = ['Pregraduate','Graduate','Postgraduate'])\n",
    "plt.ylabel('Average salary', fontsize = 18)\n",
    "plt.xlabel('Experience requirements', fontsize = 18)\n",
    "plt.savefig(output_dir + '/NOS_progression_pathway_auronautical_engineering_suite_3_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgdata['converted_skills'].map(lambda x: isinstance(x, str)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgdata['clusters'].map(lambda x: len(x)>2).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineering_nos.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_groups.get_group('aeronautical engineering suite 3')[\n",
    "    ['Title','best_cluster_nos','Exp3-peak','Eduv2-peak','MeanSalary-peak','Clean SOC Code','SOC-peak']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['Exp3','Eduv2']:\n",
    "    print(cols_v_clus['welding and machining'][key]/cols_v_clus['welding and machining'][key].sum())\n",
    "    print('-'*30)\n",
    "    \n",
    "print('*'*90)\n",
    "for key in ['Exp3','Eduv2']:\n",
    "    print(cols_v_clus['electrical engineering'][key]/cols_v_clus['electrical engineering'][key].sum())\n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
