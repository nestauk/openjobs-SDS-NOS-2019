{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefgarasto/miniconda3/envs/nlp/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.7215686274509804, 0.09803921568627451], [1, 0, 0.2549019607843137], [0, 0, 0], [1, 0.35294117647058826, 0], [0.6078431372549019, 0, 0.7647058823529411], [0.6470588235294118, 0.5803921568627451, 0.5098039215686274], [0.6274509803921569, 0.5686274509803921, 0.1568627450980392], [0.7686274509803922, 0.6901960784313725, 0], [0.9647058823529412, 0.49411764705882355, 0], [0.7843137254901961, 0.1568627450980392, 0.5725490196078431], [0.23529411764705882, 0.07058823529411765, 0.3215686274509804]] [[0, 1, 2, 3, 4, 5], [0, 6, 7], [1, 3, 8], [4, 9, 10], [8, 5], [1, 11]]\n"
     ]
    }
   ],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seaborn-dark', 'seaborn-darkgrid', 'seaborn-ticks', 'fivethirtyeight', 'seaborn-whitegrid', 'classic', '_classic_test', 'fast', 'seaborn-talk', 'seaborn-dark-palette', 'seaborn-bright', 'seaborn-pastel', 'grayscale', 'seaborn-notebook', 'ggplot', 'seaborn-colorblind', 'seaborn-muted', 'seaborn', 'Solarize_Light2', 'seaborn-paper', 'bmh', 'tableau-colorblind10', 'seaborn-white', 'dark_background', 'seaborn-poster', 'seaborn-deep']\n"
     ]
    }
   ],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_with_pos(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i, pos_to_wornet_dict[p]) if i not in keep_asis else i for i,p in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_pruned(x, pofs = 'nv'):\n",
    "    if pofs == 'nv':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['V','N']]\n",
    "    elif pofs == 'n':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['N']]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return lemmatise_with_pos(tags)\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "#df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define more functions on how to create the TfIdf vectoriser and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create your TFidfVectorizer model. This doesn't depend on whether it's used on suites or NOS. However,\n",
    "# it does require that the docs collection is already given as a collection of tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, we will use a different tokenizer\n",
    "def define_tfidf(params, stopwords):\n",
    "    if params['ngrams'] == 'bi':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,2), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    elif params['ngrams'] == 'tri':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,3), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    else:\n",
    "        # unigrams is the default\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, collect the text to transform\n",
    "def combine_nos_text(df_nos, col = 'pruned'):\n",
    "    all_joint_tokens = []\n",
    "    # group by suites and concatenate all docs in it\n",
    "    row_names = []\n",
    "    for name, group in df_nos.groupby('One_suite'):\n",
    "        row_names.append(name)\n",
    "        joint_tokens = []\n",
    "        for idoc in group[col].index:\n",
    "            joint_tokens += group[col].loc[idoc]\n",
    "        all_joint_tokens.append(joint_tokens)\n",
    "    # return a dataframe\n",
    "    return pd.DataFrame({'tokens': all_joint_tokens}, index = row_names)\n",
    "\n",
    "def get_tfidf_matrix(params, df_nos, tfidf, col = 'pruned'):\n",
    "    # Note: this can simply be used to get the tfidf transform, by setting bywhich=docs and any mode\n",
    "    t0 = time.time()\n",
    "    # first, get the dataframe of tokens\n",
    "    if params['bywhich'] == 'docs':\n",
    "        textfortoken = df_nos[col]\n",
    "        \n",
    "    elif params['bywhich'] == 'suites':\n",
    "        if params['mode'] == 'meantfidf':\n",
    "            textfortoken = df_nos[col]\n",
    "                \n",
    "        elif params['mode'] == 'combinedtfidf':\n",
    "            # note that this is the only case where the tfidf min and max are computed considering the number of \n",
    "            # suites as the number of elements in the collection.\n",
    "            # TODO: allow for the alternative case, where the transform is computed on individual NOS and then \n",
    "            # applied to the joint tokens\n",
    "            textfortoken = combine_nos_text(df_nos, col)['tokens']\n",
    "    \n",
    "    # apply tfidf transform to the tokenised text\n",
    "    tfidfm = tfidf.fit_transform(textfortoken)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    \n",
    "    # if the average is needed, compute it and overwrite the matrix. Note that the step above is still needed to\n",
    "    # initialise the tfidf transform with the proper features and stopwords\n",
    "    if (params['bywhich'] == 'suites') and (params['mode'] =='meantfidf'):\n",
    "        row_names = df_nos['One_suite'].value_counts().index.values\n",
    "        tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "        for name, group in df_nos.groupby('One_suite'):\n",
    "            tmp = get_mean_tfidf(group['pruned'], tfidf)\n",
    "            tfidfm[igroup] = tmp\n",
    "\n",
    "    print_elapsed(t0, 'computing the tfidf matrix')\n",
    "    return tfidfm, feature_names, tfidf, textfortoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships between standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import fcluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate cosine distance between tf-idf vectors of the documents\n",
    "\n",
    "def do_hierarch_clustering(tfidfm, method='average', metric = 'cosine', DOPLOTS = True):\n",
    "    t0 = time.time()\n",
    "    N2 = 11914\n",
    "    N = 400 #400*400 = 160000 distance calls per second. For N=21500 -- > 462250000 calls --> 2900*160000 calls \n",
    "    # --> I'm guessing 2900 seconds = 48 minutes (I think it's likely to be more actually)\n",
    "    # 4000*4000 takes approximately 110 seconds. It's double for the cophenet. So, for N=22500, the three functions \n",
    "    # together will take approx 4 hours (I'll do it tonight)\n",
    "\n",
    "    try:\n",
    "        distances = distance.pdist(tfidfm.todense(), metric = metric) # + np.random.randn(N,N2), metric = 'cosine')\n",
    "        sparse_flag = True\n",
    "    except:\n",
    "        distances = distance.pdist(tfidfm, metric = 'cosine')\n",
    "        sparse_flag = False\n",
    "    print_elapsed(t0, 'calculating cosine distances of tfidf vectors')\n",
    "\n",
    "    #We then build linkage matrix using the distances and specifying the method. For euclidean distances typically\n",
    "    # 'Ward' produces best results. For cosine we can only use 'average' and 'single'.\n",
    "    linkage_matrix = scipy.cluster.hierarchy.linkage(distances,\n",
    "                                                     method = method,\n",
    "                                                     metric = metric)\n",
    "    print_elapsed(t0, 'hierarchical clustering of cosine distances')\n",
    "    #We can test how well the groupings reflect actual distances. If c > 0.75 this is considered to be sufficiently\n",
    "    #good representation\n",
    "    if sparse_flag:\n",
    "        c, coph_dists = cophenet(linkage_matrix, \n",
    "                             distance.pdist(tfidfm.todense(), metric = 'cosine'))\n",
    "    else:\n",
    "        c, coph_dists = cophenet(linkage_matrix, \n",
    "                             distance.pdist(tfidfm, metric = 'cosine'))\n",
    "\n",
    "    print_elapsed(t0, 'computing the cophenetic correlation')\n",
    "\n",
    "    if DOPLOTS:\n",
    "        fig, ax =plt.subplots(figsize = (5,5))\n",
    "        plt.imshow(scipy.spatial.distance.squareform(distances))\n",
    "        plt.title('cosine distances between suites')\n",
    "        plt.colorbar()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (5,5))\n",
    "        tmp = plt.imshow(scipy.spatial.distance.squareform(coph_dists))\n",
    "        plt.colorbar()\n",
    "    print('The cophenetic coefficient is {:.4f}'.format(c))\n",
    "    return distances, linkage_matrix, c, coph_dists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters for features extraction\n",
    "\n",
    "ngrams : uni/bi/tri\n",
    "\n",
    "tfidf thresholds: min and max percentage\n",
    "\n",
    "which parts of speech were selected before\n",
    "\n",
    "whether we are working at the level of suites or of invidual NOS, and how we aggregate NOS to form the suit level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hierarchical clustering on all NOS from one super-suite at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_supersuite(x):\n",
    "    for supersuite in all_match_names.keys():\n",
    "        if x in all_match_names[supersuite]:\n",
    "            return supersuite.lower()\n",
    "    # if no match has been found\n",
    "    return 'other'\n",
    "\n",
    "def adjustsoccode(x):\n",
    "    y = re.findall(r\"[\\d']+\", str(x))\n",
    "    if len(y):\n",
    "        return y[0][1:-1]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract2digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:2])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract3digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:3])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract1digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract4digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_gensim(list_of_terms, some_model, weights = None):\n",
    "    # replace space with underscore\n",
    "    new_terms = [convert_to_undersc(elem) for elem in list_of_terms]\n",
    "    # check if each element in the list is in the model\n",
    "    is_in = [elem for elem in new_terms if elem in some_model]\n",
    "    # also check the weights\n",
    "    if weights:\n",
    "        weights_in = [weights[ix] for ix,elem in enumerate(new_terms) \n",
    "                        if elem in some_model]\n",
    "    # only return the element in the model\n",
    "    return is_in, weights_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vec(skill_list, model, weights= None):\n",
    "    if not weights:\n",
    "        weights = np.ones(len(skill_list))\n",
    "    skill_list_conv = [convert_to_undersc(elem) for elem in skill_list]\n",
    "    wvector_list = [model[elem]*weights[ix] for ix,elem in enumerate(skill_list_conv) \n",
    "                if elem in model]\n",
    "    vector_list = [model[elem] for ix,elem in enumerate(skill_list_conv) \n",
    "                if elem in model]\n",
    "    vec_array = np.asarray(vector_list)\n",
    "    wvec_array = np.asarray(wvector_list)\n",
    "    avg_vec = np.mean(wvec_array, axis=0)\n",
    "    return avg_vec, vec_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subdf(SELECT_MODE, clusters2use, suites_clusters, df_nos_select):\n",
    "    if isinstance(SELECT_MODE, str):\n",
    "        tmp_dict = {'engineering': 'Engineering', 'management': 'Management',\n",
    "                    'financialservices': 'Financial services', \n",
    "                    'construction': 'Construction'}\n",
    "        # select NOS from super suite\n",
    "        cluster_name = SELECT_MODE\n",
    "        cluster_name_save = cluster_name\n",
    "        cluster_name_figs = tmp_dict[SELECT_MODE]\n",
    "        subset_nos = df_nos_select[df_nos_select['supersuite']== SELECT_MODE]\n",
    "    elif isinstance(SELECT_MODE, int):\n",
    "        cluster_name = clusters2use[SELECT_MODE][1]\n",
    "        cluster_name_save = cluster_name.replace(' ','_')\n",
    "        cluster_name_figs = cluster_name.capitalize()\n",
    "        suites2use = list(suites_clusters[suites_clusters['hierarchical'].map(\n",
    "                lambda x: x in clusters2use[SELECT_MODE][0])]['Suite_names'].values)\n",
    "        subset_nos = df_nos_select[df_nos_select['One_suite'].map(\n",
    "                lambda x: x in suites2use)]\n",
    "    print('Number of NOS selected: ', len(subset_nos))\n",
    "    #print(subset_nos.columns)\n",
    "    \n",
    "    #%\n",
    "    # only select those engineering nos with SOC codes\n",
    "    nosoc = subset_nos['SOC4'].isnull()\n",
    "    print('percentage of nos without SOC codes: ', nosoc.sum()/len(nosoc))\n",
    "    if (nosoc.sum())/len(nosoc)<0.9:\n",
    "        final_nos = subset_nos[~nosoc] #np.isnan(engineering_nos['SOC4'])]\n",
    "    else:\n",
    "        final_nos = subset_nos\n",
    "    final_groups = final_nos.groupby(by = 'One_suite')\n",
    "    larger_suites = []\n",
    "    all_lengths = final_groups.agg(len)['NOS Title'].values\n",
    "    all_lengths[::-1].sort()\n",
    "    print('Number of NOS in suites belonging to this cluster: ',all_lengths)\n",
    "    #th_supers = ['engineering': 40, 'financialservices': ]\n",
    "    for name, group in final_groups:\n",
    "        if isinstance(SELECT_MODE, int):\n",
    "            larger_suites.append(name)\n",
    "        elif len(group)> all_lengths[15]:#th_supers[SELECT_MODE]:\n",
    "            #print(name, len(group))\n",
    "            larger_suites.append(name)\n",
    "\n",
    "    return final_nos, final_groups, larger_suites, cluster_name,  \\\n",
    "                    cluster_name_save, cluster_name_figs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part: setup\n",
    "the next cell is the setup and it should be the same as the script to detect levels via GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seaborn-dark', 'seaborn-darkgrid', 'seaborn-ticks', 'fivethirtyeight', 'seaborn-whitegrid', 'classic', '_classic_test', 'fast', 'seaborn-talk', 'seaborn-dark-palette', 'seaborn-bright', 'seaborn-pastel', 'grayscale', 'seaborn-notebook', 'ggplot', 'seaborn-colorblind', 'seaborn-muted', 'seaborn', 'Solarize_Light2', 'seaborn-paper', 'bmh', 'tableau-colorblind10', 'seaborn-white', 'dark_background', 'seaborn-poster', 'seaborn-deep']\n",
      "/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/nos_clusters_nv_final_no_dropped\n",
      "Loading glove model\n",
      "Done with loading the glove model. Elapsed time: 38.589213\n"
     ]
    }
   ],
   "source": [
    "WHICH_GLOVE = 'glove.6B.100d' #'glove.6B.100d' #'glove.840B.300d', \n",
    "#glove.twitter.27B.100d\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/'\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])\n",
    "pofs = 'nv'\n",
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n",
    "output_dir += 'nos_clusters_{}_final_no_dropped'.format(pofs)\n",
    "print(output_dir)\n",
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n",
    "\n",
    "#Loading a pre-trained glove model into gensim\n",
    "# model should have already been loaded in bg_load_prepare_and_run. \n",
    "# If not, load it here    \n",
    "\n",
    "LOADGLOVE = True\n",
    "if LOADGLOVE:\n",
    "    print('Loading glove model')\n",
    "    t0 = time.time()\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.{}.txt'.format(WHICH_GLOVE)))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors\n",
    "    # from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "    print_elapsed(t0, 'loading the glove model')\n",
    "\n",
    "    vector_matrix = model.vectors\n",
    "    list_of_terms = model.index2word\n",
    "\n",
    "    lookup_terms = [convert_from_undersc(elem) for elem in list_of_terms]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with loading the dataset\n",
      "48.87467122077942\n",
      "83 83\n",
      "management and leadership , marketing , marketing 2013 , management and leadership marketing 2013)\n",
      "59 59\n",
      "29 29\n",
      "75 76\n",
      "other                16350\n",
      "engineering           3038\n",
      "construction          1394\n",
      "management            1376\n",
      "financialservices      459\n",
      "Name: supersuite, dtype: int64\n",
      "Nb of NOS in all supersuites 6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefgarasto/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1', '@'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with computing the tfidf matrix. Elapsed time: 0.940246\n",
      "Number of features: 4847\n",
      "Some features:\n",
      "['half', 'halve', 'hand', 'handbook', 'handing', 'handover', 'hangar', 'harbour', 'hardening', 'hardware', 'harmful', 'harvest', 'haswa', 'hatching', 'header', 'headphone', 'hear', 'heat', 'heatproof', 'heaving', 'helicopter', 'helmet', 'hexagonal', 'hfo', 'hierarchy', 'hindrance', 'historic', 'hob', 'holder', 'holiday', 'homing', 'honeycomb', 'hood', 'hopper']\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "'''  Start of the main script   '''\n",
    "\n",
    "#%% set up main parameters\n",
    "#from set_params_thematic_groups import qualifier, qualifier0, pofs, WHICH_GLOVE, \n",
    "#from set_params_thematic_groups import glove_dir, paramsn\n",
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Get the NOS data \n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + \n",
    "                    'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "\n",
    "# remove p and k\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)\n",
    "print('Done with loading the dataset')\n",
    "\n",
    "\n",
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„',\n",
    "                     \"'m\", \"'re\", '£','—','‚°','●'])\n",
    "stopwords0 += tuple(set(list(df_nos['Developed By'])))\n",
    "stopwords0 += tuple(['cosvr','unit','standard','sfl','paramount','tp','il','al','ad','hoc',\n",
    "                    'lanleo','ireland','something'])\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# create another column where the texts are lemmatised properly\n",
    "t0 = time.time()\n",
    "df_nos['pruned_lemmas'] = df_nos['tagged_tokens'].map(lambda x: lemmatise_pruned(x,pofs))\n",
    "print(time.time()-t0)\n",
    "\n",
    "\n",
    "# ### Only keep NOS from a super-suite\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "super_suites_files=  ''.join(['/Users/stefgarasto/Google Drive/Documents/data/',\n",
    "                              'NOS_meta_data/NOS_Suite_Priority.xlsx'])\n",
    "super_suites_names = ['Engineering','Management','FinancialServices','Construction']\n",
    "all_super_suites = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_super_suites[which_super_suite] = pd.read_excel(super_suites_files, \n",
    "                    sheet_name = which_super_suite)\n",
    "    all_super_suites[which_super_suite]['NOS Suite name'] = all_super_suites[\n",
    "        which_super_suite]['NOS Suite name'].map(\n",
    "        lambda x: x.replace('(','').replace('(','').replace('&','and').strip().lower())\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "all_matches = {}\n",
    "all_match_names = {}\n",
    "#match_name = []\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_matches[which_super_suite] = []\n",
    "    for suite in all_super_suites[which_super_suite]['NOS Suite name'].values:\n",
    "        # do manually some selected suites\n",
    "        if 'insurance claims' in suite:\n",
    "            tmp = standard_labels.index('general insurance')\n",
    "            all_matches[which_super_suite].append(tmp)\n",
    "            continue\n",
    "        # for the \"management and leadership marketing 2013\" both marketing \n",
    "        # and marketing 2013 would fit,\n",
    "        # but I'm only taking the latter\n",
    "        # find a fuzzy match between \n",
    "        out = process.extract(suite, standard_labels, limit=3)\n",
    "        if len(out) and out[0][1]>89:\n",
    "            # note: most of them are above 96% similarity (only one is 90%)\n",
    "            tmp = standard_labels.index(out[0][0])\n",
    "            #print(suite, out[0])\n",
    "            if tmp not in all_matches[which_super_suite]:\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "            else:\n",
    "                if suite == 'installing domestic fascia, soffit, and bargeboards':\n",
    "                    # this suite is kind of a duplicate - I aggregated it in my suites list\n",
    "                    continue\n",
    "                tmp = standard_labels.index(out[2][0])\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "                print(out[0][0],',',out[1][0],',',out[2][0],',',suite)\n",
    "        else:\n",
    "            print(suite, ' not found')\n",
    "            print(out)\n",
    "            print('\\n')\n",
    "    print(len(all_matches[which_super_suite]),len(all_super_suites[which_super_suite]))\n",
    "    all_match_names[which_super_suite] = [standard_labels[t] \n",
    "                    for t in all_matches[which_super_suite]]\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "# assign supersuite and SOC codes\n",
    "df_nos['supersuite'] = df_nos['One_suite'].apply(assign_supersuite)\n",
    "# extract 2 digit soc\n",
    "df_nos['SOC4str'] = df_nos['Clean SOC Code'].map(adjustsoccode)\n",
    "df_nos['SOC1'] = df_nos['SOC4str'].map(extract1digits)\n",
    "df_nos['SOC2'] = df_nos['SOC4str'].map(extract2digits)\n",
    "df_nos['SOC3'] = df_nos['SOC4str'].map(extract3digits)\n",
    "df_nos['SOC4'] = df_nos['SOC4str'].map(extract4digits)\n",
    "print(df_nos['supersuite'].value_counts())\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "# select NOS in super-suites of interest\n",
    "df_nos_select = df_nos[~(df_nos['supersuite']=='other')]\n",
    "print('Nb of NOS in all supersuites' ,len(df_nos_select))\n",
    "\n",
    "\n",
    "#%%\n",
    "'''\n",
    "# ## Get raw data and tokenize\n",
    "\n",
    "# ## Choosing parameters for features extraction\n",
    "# \n",
    "# ngrams : uni/bi/tri\n",
    "# \n",
    "# tfidf thresholds: min and max percentage\n",
    "# \n",
    "# which parts of speech were selected before\n",
    "# \n",
    "# whether we are working at the level of suites or of invidual NOS, \n",
    "# and how we aggregate NOS to form the suit level\n",
    "# \n",
    "'''\n",
    "\n",
    "#\n",
    "\n",
    "'''\n",
    "# First, create your TFidfVectorizer model. This doesn't depend on whether \n",
    "it's used on suites or NOS. However,\n",
    "it does require that the docs collection is already given as a collection of\n",
    "tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, \n",
    "we will use a different tokenizer\n",
    "'''\n",
    "\n",
    "paramsn = {}\n",
    "paramsn['ngrams'] = 'uni'\n",
    "paramsn['pofs'] = pofs #'nv'\n",
    "paramsn['tfidf_min'] = 3\n",
    "paramsn['tfidf_max'] = 0.5\n",
    "\n",
    "paramsn['bywhich'] = 'docs' #'docs' #'suites'\n",
    "paramsn['mode'] = 'tfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n",
    "# define the transform: this one can easily be the same for both \n",
    "# keywords and the clustering\n",
    "tfidf_n = define_tfidf(paramsn, stopwords0)\n",
    "\n",
    "# get the transform from the whole NOS corpus\n",
    "FULL_CORPUS = False\n",
    "if FULL_CORPUS:\n",
    "    _, feature_names_n, tfidf_n, _ = get_tfidf_matrix(\n",
    "            paramsn, df_nos, tfidf_n, col = 'pruned_lemmas')\n",
    "else:\n",
    "    df_nos_select = df_nos[df_nos['supersuite']=='engineering']\n",
    "    _, feature_names_n, tfidf_n, _ = get_tfidf_matrix(\n",
    "            paramsn, df_nos_select, tfidf_n, col = 'pruned_lemmas')\n",
    "\n",
    "\n",
    "print('Number of features: {}'.format(len(feature_names_n)))\n",
    "N = 2000\n",
    "print('Some features:')\n",
    "print(feature_names_n[N:N+100:3])\n",
    "print('*'*70)\n",
    "\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "# first transform via tfidf all the NOS in one supersuite because you need the top keywords\n",
    "textfortoken = df_nos_select['pruned_lemmas']\n",
    "tfidfm = tfidf_n.transform(textfortoken)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got to NOS nb 999. Total time elapsed: 0.3645 s\n",
      "semaut3083.pdf producing motorsport composite assemblies\n",
      "['assembly', 'composite', 'motorsport', 'produce', 'join', 'joining', 'mould', 'duct', 'fairings', 'panel', 'floor', 'moulding', 'body', 'adhesive', 'rivet', 'trial', 'incorporate', 'shroud', 'tub', 'wing']\n",
      "**************************************\n",
      "sembit33.pdf applying central limit theorem and confidence intervals\n",
      "['confidence', 'interval', 'mean', 'calculate', 'limit', 'equation', 'cpk', 'cp', 'error', 'population', 'distribute', 'utilise', 'deviation', 'estimate', 'size', 'highlight', 'measurement', 'finding', 'data', 'improvement']\n",
      "**************************************\n",
      "semfwe344.pdf forming pipework by machine bending\n",
      "['bend', 'bending', 'pipe', 'machine', 'form', 'forming', 'produce', 'pipework', 'section', 'angle', 'bridge', 'operate', 'expansion', 'diameter', 'template', 'deformation', 'perform', 'power', 'offset', 'size']\n",
      "**************************************\n",
      "Got to NOS nb 1999. Total time elapsed: 0.6668 s\n",
      "semme3029.pdf maintaining marine sensor equipment and systems\n",
      "['maintenance', 'sensor', 'marine', 'sonar', 'radar', 'replace', 'receiver', 'surveillance', 'schedule', 'adjust', 'circuit', 'warn', 'transducer', 'traywork', 'sense', 'isolation', 'power', 'transmitter', 'rating', 'conduit']\n",
      "**************************************\n",
      "Got to NOS nb 2999. Total time elapsed: 0.9586 s\n",
      "not now\n",
      "semaut3067.pdf ['rectification', 'vehicle', 'motorsport', 'fault', 'diagnosis', 'competition', 'replace', 'pressure', 'team', 'remove', 'release', 'torque', 'fastener', 'sport', 'diagnosing', 'rectify', 'suspension', 'refit', 'damage', 'voltmeter'] 20 20\n",
      "Done with computing average word embedding. Elapsed time: 1.054078\n",
      "assess-the-configuration-of-metered-areas-eusldc1.pdf\n",
      "assess the configuration of metered areas\n",
      "['respond', 'presentation', 'person', 'supplier', 'duty', 'ignore', 'perceive', 'drug', 'name', 'reduce', 'likelihood', 'endanger', 'eu', 'water', 'manufacturer', 'hygiene', 'suggestion', 'pose', 'introduce', 'harm']\n",
      "[('any', 0.8227744102478027), ('because', 0.8154299259185791), ('not', 0.8147847652435303), ('should', 0.8092131614685059), ('need', 0.8066936731338501), ('whether', 0.8010623455047607), ('possible', 0.7993483543395996), ('make', 0.7979775667190552), ('certain', 0.797663688659668), ('rather', 0.7959783673286438)]\n",
      "\n",
      "astatt1.pdf\n",
      "prepare to carry out air tightness testing \n",
      "['air', 'building', 'tightness', 'fan', 'client', 'conduct', 'envelope', 'ventilation', 'explain', 'leakage', 'testing', 'calculate', 'permeability', 'impact', 'pressurisation', 'establish', 'volume', 'dwell', 'location', 'flow']\n",
      "[('air', 0.8053557872772217), ('building', 0.7535480260848999), ('the', 0.7394852638244629), ('system', 0.7344543933868408), ('it', 0.7342804074287415), ('because', 0.7318680882453918), ('control', 0.729941725730896), ('same', 0.7254292368888855), ('this', 0.7233279943466187), ('that', 0.7232325673103333)]\n",
      "\n",
      "astatt10.pdf\n",
      "carry out air tightness testing on large and complex, high rise and phased hand over zonal buildings\n",
      "['building', 'air', 'rise', 'envelope', 'pressure', 'tightness', 'result', 'floor', 'seal', 'site', 'period', 'expire', 'stabilisation', 'remain', 'average', 'permeability', 'establish', 'comment', 'fan', 'state']\n",
      "[('building', 0.8641065359115601), ('the', 0.8125848174095154), ('this', 0.7876595854759216), ('it', 0.7821349501609802), ('which', 0.7811872363090515), ('same', 0.7769464254379272), ('where', 0.7737154960632324), ('well', 0.7728615999221802), ('its', 0.7723281383514404), ('new', 0.7676297426223755)]\n",
      "\n",
      "astatt2.pdf\n",
      "prepare building for air tightness testing\n",
      "['building', 'air', 'seal', 'ventilation', 'fan', 'location', 'testing', 'trap', 'site', 'variance', 'impact', 'tightness', 'pressure', 'thermometer', 'compromise', 'calculation', 'conduct', 'opening', 'measure', 'roof']\n",
      "[('building', 0.8175748586654663), ('the', 0.7881037592887878), ('it', 0.775310754776001), ('air', 0.7659105658531189), ('which', 0.7576380372047424), ('ground', 0.7551987767219543), ('well', 0.7551064491271973), ('same', 0.753849983215332), ('this', 0.7532176971435547), ('where', 0.7509032487869263)]\n",
      "\n",
      "astatt3.pdf\n",
      "carry out air tightness testing for single dwellings and other small buildings  \n",
      "['air', 'building', 'remain', 'seal', 'dwelling', 'pressure', 'result', 'establish', 'site', 'duration', 'equalisation', 'period', 'build', 'airflow', 'value', 'differentiation', 'ventilation', 'expire', 'volume', 'average']\n",
      "[('this', 0.8014422655105591), ('same', 0.7982402443885803), ('the', 0.7979630827903748), ('which', 0.7976223230361938), ('its', 0.7902200222015381), ('it', 0.7894993424415588), ('only', 0.7856019735336304), ('well', 0.780990719795227), ('addition', 0.7770465612411499), ('because', 0.772983193397522)]\n",
      "\n",
      "astatt4.pdf\n",
      "carry out air tightness testing for non-simple buildings\n",
      "['building', 'air', 'pressure', 'calculate', 'result', 'testing', 'fan', 'stabilisation', 'remain', 'envelope', 'define', 'calculation', 'seal', 'target', 'period', 'airflow', 'value', 'energy', 'differentiation', 'expire']\n",
      "[('this', 0.8047895431518555), ('the', 0.8037049174308777), ('same', 0.8022733926773071), ('it', 0.7991722226142883), ('building', 0.7926275730133057), ('which', 0.7867507338523865), ('well', 0.7860069274902344), ('that', 0.7854025363922119), ('its', 0.7810043096542358), ('because', 0.7789366841316223)]\n",
      "\n",
      "astatt5.pdf\n",
      "prepare and issue the air tightness test report  \n",
      "['building', 'methodology', 'permeability', 'air', 'result', 'envelope', 'data', 'post', 'registration', 'calculation', 'pre', 'equation', 'value', 'photograph', 'interpretation', 'scheme', 'average', 'pressure', 'wa', 'fan']\n",
      "[('same', 0.7946382164955139), ('example', 0.7924999594688416), ('this', 0.7904510498046875), ('system', 0.7766608595848083), ('result', 0.7710034847259521), ('change', 0.7709980607032776), ('that', 0.7597221732139587), ('the', 0.7507261037826538), ('which', 0.7498671412467957), ('data', 0.7490862011909485)]\n",
      "\n",
      "astatt6.pdf\n",
      "calculate the area of building envelopes\n",
      "['envelope', 'building', 'calculation', 'measurement', 'exclude', 'calculate', 'value', 'wall', 'roof', 'software', 'photograph', 'input', 'sketch', 'build', 'field', 'observation', 'site', 'volume', 'geometry', 'ceiling']\n",
      "[('structure', 0.7643742561340332), ('example', 0.755991518497467), ('same', 0.7260786890983582), ('this', 0.7167790532112122), ('using', 0.7167298793792725), ('value', 0.7110070586204529), ('data', 0.7068643569946289), ('design', 0.7046613097190857), ('actual', 0.7041364312171936), ('it', 0.7013126611709595)]\n",
      "\n",
      "astatt7.pdf\n",
      "manage all calibrated equipment\n",
      "['calibration', 'business', 'inventory', 'remain', 'database', 'accredit', 'availability', 'staff', 'laboratory', 'calibrate', 'competent', 'demand', 'way', 'compliance', 'monitor', 'trader', 'air', 'despatch', 'permeability', 'facility']\n",
      "[('data', 0.7572908401489258), ('measurement', 0.7236288785934448), ('quality', 0.7174516320228577), ('required', 0.7156268954277039), ('monitoring', 0.711164653301239), ('testing', 0.7068874835968018), ('requirements', 0.7060242891311646), ('calibration', 0.7057995200157166), ('evaluation', 0.696022629737854), ('systems', 0.6947113275527954)]\n",
      "\n",
      "astatt8.pdf\n",
      "initiate and progress contracts with clients to carry out air tightness testing \n",
      "['client', 'building', 'quotation', 'air', 'contract', 'methodology', 'conduct', 'personnel', 'query', 'payment', 'progress', 'clarify', 'cost', 'quantify', 'issue', 'resource', 'draft', 'approval', 'invoice', 'debt']\n",
      "[('this', 0.772860050201416), ('any', 0.7714681625366211), ('that', 0.7648391723632812), ('same', 0.7628040909767151), ('not', 0.7571265697479248), ('because', 0.7563966512680054), ('for', 0.7562602758407593), ('would', 0.7538049817085266), ('possible', 0.7519775032997131), ('work', 0.7486711740493774)]\n",
      "\n",
      "astatt9.pdf\n",
      "quality assure the air tightness testing process \n",
      "['complaints', 'air', 'quotation', 'conformance', 'dispute', 'personnel', 'document', 'client', 'data', 'effectiveness', 'investigate', 'conduct', 'agree', 'accreditation', 'accredit', 'statement', 'way', 'monitor', 'comment', 'durability']\n",
      "[('information', 0.7660167813301086), ('regarding', 0.7566043138504028), ('any', 0.7512701153755188), ('whether', 0.745772123336792), ('officials', 0.7384834289550781), ('complaints', 0.7383577823638916), ('reports', 0.7368653416633606), ('response', 0.7313218116760254), ('security', 0.7275611162185669), ('that', 0.7265994548797607)]\n",
      "\n",
      "carry-out-the-maintenance-of-treatment-processing-plant-and-equipment-eustpc03.pdf\n",
      "carry out the maintenance of treatment processing plant and equipment\n",
      "['treatment', 'maintenance', 'plant', 'malfunction', 'shut', 'instrumentation', 'designate', 'organisation', 'computer', 'processing', 'specific', 'request', 'catchment', 'tabular', 'relates', 'failure', 'isolate', 'cyber', 'site', 'sludge']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('maintenance', 0.7866942882537842), ('system', 0.7516964673995972), ('treatment', 0.7326031923294067), ('facilities', 0.7323371171951294), ('facility', 0.7322425842285156), ('equipment', 0.7315930724143982), ('systems', 0.7310415506362915), ('use', 0.7290605902671814), ('plant', 0.7239134907722473), ('required', 0.7171631455421448)]\n",
      "\n",
      "cogesa1.1.pdf\n",
      "create the complex specification for explosive substances and/or articles\n",
      "['customer', 'underlie', 'article', 'research', 'address', 'create', 'satisfaction', 'success', 'solution', 'constraint', 'feasibility', 'interest', 'substance', 'confidentiality', 'confirm', 'affect', 'improvement', 'adaptation', 'manager', 'criterion']\n",
      "[('particular', 0.7937981486320496), ('change', 0.7895011901855469), ('example', 0.7835838794708252), ('this', 0.7800945043563843), ('fact', 0.7716935873031616), ('certain', 0.7714309692382812), ('terms', 0.7700669765472412), ('that', 0.7676703929901123), ('specific', 0.7643022537231445), ('need', 0.7584383487701416)]\n",
      "\n",
      "cogesa1.10 .pdf\n",
      "carry out the research strategy and analyse the information collected on explosive substances and/or articles\n",
      "['research', 'article', 'evaluation', 'investigation', 'substance', 'organization', 'confidentiality', 'deadline', 'strategy', 'party', 'analyse', 'objective', 'programme', 'minimize', 'peer', 'colleague', 'evaluate', 'budget', 'adapt', 'delay']\n",
      "[('research', 0.8023743629455566), ('study', 0.7985717058181763), ('scientific', 0.7847696542739868), ('review', 0.776728093624115), ('analysis', 0.7721195816993713), ('information', 0.7604241371154785), ('report', 0.7596075534820557), ('policy', 0.7506483793258667), ('concluded', 0.7447044849395752), ('according', 0.7432854771614075)]\n",
      "\n",
      "cogesa1.11 .pdf\n",
      "carry out investigations and analyse the information collected on explosive substances and/or articles\n",
      "['research', 'article', 'investigation', 'substance', 'evaluation', 'confidentiality', 'party', 'analyse', 'minimize', 'deadline', 'manager', 'evaluate', 'analysis', 'nature', 'team', 'organization', 'plan', 'objective', 'programme', 'correctness']\n",
      "[('study', 0.7905646562576294), ('research', 0.7856747508049011), ('analysis', 0.7785893082618713), ('scientific', 0.7755963206291199), ('information', 0.7703495025634766), ('review', 0.7633264660835266), ('report', 0.7595019340515137), ('that', 0.747862696647644), ('according', 0.7475697994232178), ('concluded', 0.7449434995651245)]\n",
      "\n",
      "cogesa1.12 .pdf\n",
      "contribute to carrying out investigations and analysing the information collected on explosive substances and/or articles\n",
      "['research', 'article', 'investigation', 'contribute', 'substance', 'confidentiality', 'analyse', 'team', 'deadline', 'analysis', 'objective', 'collaborate', 'appropriateness', 'collect', 'result', 'relevance', 'researcher', 'manager', 'line', 'designer']\n",
      "[('study', 0.8046624660491943), ('research', 0.785769522190094), ('information', 0.7789937257766724), ('analysis', 0.7755856513977051), ('scientific', 0.7698013782501221), ('report', 0.7619048357009888), ('work', 0.7531548738479614), ('this', 0.752042293548584), ('according', 0.7499550580978394), ('that', 0.7494171261787415)]\n",
      "\n",
      "cogesa1.13.pdf\n",
      "evaluate and document complex research and findings on explosive substances and/or articles\n",
      "['research', 'dissemination', 'article', 'strategy', 'evaluate', 'substance', 'result', 'finding', 'evaluation', 'objective', 'recommendation', 'sponsor', 'party', 'effectiveness', 'find', 'specialism', 'weakness', 'audience', 'deadline', 'meeting']\n",
      "[('research', 0.8488433361053467), ('scientific', 0.8183245658874512), ('study', 0.8176252841949463), ('analysis', 0.7958711385726929), ('focus', 0.7755220532417297), ('critical', 0.7720077037811279), ('development', 0.7632455229759216), ('information', 0.7619121074676514), ('particular', 0.7553917169570923), ('specific', 0.748378336429596)]\n",
      "\n",
      "cogesa1.14.pdf\n",
      "assess and document research and findings on explosive substances and/or articles\n",
      "['research', 'article', 'substance', 'result', 'find', 'dissemination', 'sponsor', 'effectiveness', 'audience', 'objective', 'recommendation', 'deadline', 'meeting', 'document', 'explain', 'finding', 'format', 'confidentiality', 'programme', 'distinction']\n",
      "[('study', 0.8308645486831665), ('research', 0.8278442621231079), ('scientific', 0.8051879405975342), ('analysis', 0.7744254469871521), ('critical', 0.771973729133606), ('information', 0.7700482606887817), ('particular', 0.7691866159439087), ('example', 0.764061689376831), ('this', 0.7628527879714966), ('that', 0.7592617273330688)]\n",
      "\n",
      "cogesa1.15.pdf\n",
      "develop a dissemination plan for explosive substances and/or articles\n",
      "['dissemination', 'plan', 'research', 'audience', 'strategy', 'sponsor', 'target', 'article', 'property', 'approval', 'substance', 'collaborator', 'develop', 'manager', 'reach', 'disadvantage', 'affect', 'right', 'expert', 'leader']\n",
      "[('information', 0.7827171087265015), ('development', 0.7572556138038635), ('research', 0.749344527721405), ('knowledge', 0.7417855262756348), ('scientific', 0.7369899749755859), ('specific', 0.7147936820983887), ('focus', 0.7145334482192993), ('particular', 0.7091487646102905), ('program', 0.709040105342865), ('strategy', 0.704269528388977)]\n",
      "\n",
      "cogesa1.16.pdf\n",
      "carry out small scale processing for explosive substances and/or articles\n",
      "['processing', 'scale', 'article', 'event', 'suitability', 'substance', 'researcher', 'inherent', 'designer', 'developer', 'consign', 'resource', 'establish', 'deviation', 'correct', 'recommend', 'confirm', 'reinstate', 'modification', 'occurrence']\n",
      "[('example', 0.7998494505882263), ('particular', 0.7773665189743042), ('this', 0.7724037170410156), ('specific', 0.7687644362449646), ('instance', 0.761712908744812), ('possible', 0.7527121305465698), ('use', 0.7511153221130371), ('change', 0.746310830116272), ('product', 0.7412328720092773), ('any', 0.7408649921417236)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVEKW= False\n",
    "top_terms_dict = {}\n",
    "top_weights_dict = {}\n",
    "top_keywords_dict = {}\n",
    "#for name, group in ifa_df.groupby('Route'):\n",
    "igroup = 0\n",
    "n_keywords =[]\n",
    "n_repeated = []\n",
    "#top_terms = {}\n",
    "t0 = time.time()\n",
    "tfidfm_dense = tfidfm.todense()\n",
    "for ix,name in enumerate(df_nos_select.index):\n",
    "    #top_terms = get_top_words(df_nos_select.loc[name]['pruned'], feature_names_n, tfidf, n = 20)\n",
    "    top_ngrams = np.argsort(tfidfm_dense[ix,:])\n",
    "    top_ngrams = top_ngrams.tolist()[0][-20:]\n",
    "    top_ngrams = top_ngrams[::-1]\n",
    "    # only retain the ones with non zero features\n",
    "    top_ngrams = [elem for elem in top_ngrams if tfidfm_dense[ix,elem]>0]\n",
    "    top_weights = [tfidfm_dense[ix,elem] for elem in top_ngrams]\n",
    "    top_features = [feature_names_n[elem] for elem in top_ngrams]\n",
    "    top_terms_dict[name] = {}\n",
    "    top_terms_dict[name] = top_features\n",
    "    top_weights_dict[name] = {}\n",
    "    top_weights_dict[name] = top_weights\n",
    "    if np.random.randn(1)>3:\n",
    "        print(name, df_nos_select.loc[name]['NOS Title'])\n",
    "        print(top_features) #, top_keywords)\n",
    "        print('**************************************')\n",
    "    if ix % 1000 == 999:\n",
    "        print('Got to NOS nb {}. Total time elapsed: {:.4f} s'.format(ix,time.time()-t0))\n",
    "\n",
    "# save them all as csv\n",
    "if SAVEKW:\n",
    "    pd.DataFrame.from_dict(top_terms_dict, orient = 'index').to_csv(output_dir +\n",
    "                        '/NOS_from_supersuites_top_terms_{}_{}.csv'.format(qualifier,pofs))\n",
    "    pd.DataFrame.from_dict(top_weights_dict, orient = 'index').to_csv(output_dir +\n",
    "                '/NOS_from_supersuites_top_terms_weights_{}_{}.csv'.format(qualifier,pofs))\n",
    "tfidfm_dense = None\n",
    "\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "# just to check results\n",
    "'''\n",
    "print(list(top_terms_dict.keys())[885:887])\n",
    "top_terms_weights = get_top_words_weights([df_nos_select.iloc[0]['pruned_lemmas']], feature_names_n, tfidf, n = 20)\n",
    "print(top_terms_weights.sort_values(by = 'tfidf', ascending = False).head(n=20))\n",
    "'''\n",
    "# note that the get_top_words_weights function is probably wrong - but it doesn't matter now\n",
    "print('not now')\n",
    "\n",
    "\n",
    "# remove top terms that are not in the chosen gensim model\n",
    "new_top_terms_dict = {}\n",
    "new_top_weights_dict = {}\n",
    "for k,v in top_terms_dict.items():\n",
    "    # check if the top terms for each document are in the gensim model\n",
    "    new_top_terms, weights = prep_for_gensim(v, model, weights = top_weights_dict[k])\n",
    "    # only retains the ones in the model\n",
    "    new_top_terms_dict[k] = new_top_terms\n",
    "    new_top_weights_dict[k] = weights\n",
    "    if np.random.randn(1)>3.5:\n",
    "        print(k, new_top_terms, len(new_top_terms), len(v))\n",
    "        \n",
    "'''\n",
    "Compute average word embedding for each NOS\n",
    "'''\n",
    "t0 = time.time()\n",
    "we_p_st = {}\n",
    "counter = 0\n",
    "for ix,k in enumerate(new_top_terms_dict):\n",
    "    avg_features, _  = get_mean_vec(new_top_terms_dict[k], model, \n",
    "                                                  weights = new_top_weights_dict[k])\n",
    "    we_p_st[k] = avg_features\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# add the best clusters to the nos dataframe\n",
    "tmp = pd.DataFrame.from_dict(we_p_st, orient = 'index')\n",
    "all_we_nb = range(len(avg_features))\n",
    "all_we_cols = []\n",
    "all_we_changes = {}\n",
    "for col in all_we_nb:\n",
    "    all_we_changes[col] = 'weighted_avg_we{}'.format(col)\n",
    "    all_we_cols.append('weighted_avg_we{}'.format(col))\n",
    "\n",
    "tmp = tmp.rename(columns = all_we_changes) #{col: 'weighted_avg_we{}'.format(col)})\n",
    "if 'weighted_avg_we1' in df_nos_select.columns:\n",
    "    df_nos_select = df_nos[df_nos['supersuite']=='engineering']\n",
    "df_nos_select = df_nos_select.join(tmp) #[all_we_cols] = tmp[all_we_cols]\n",
    "print_elapsed(t0, 'computing average word embedding')\n",
    "\n",
    "for ix in range(20):\n",
    "    nos = df_nos_select.index[ix]\n",
    "    print(nos)\n",
    "    print(df_nos_select.loc[nos]['NOS Title'])\n",
    "    print(new_top_terms_dict[nos])\n",
    "    print(model.similar_by_vector(we_p_st[nos]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['installation',\n",
       " 'flight',\n",
       " 'aircraft',\n",
       " 'guidance',\n",
       " 'fastener',\n",
       " 'gyro',\n",
       " 'secure',\n",
       " 'fly',\n",
       " 'aviation',\n",
       " 'autopilot',\n",
       " 'director',\n",
       " 'instal',\n",
       " 'install',\n",
       " 'earth',\n",
       " 'damage',\n",
       " 'align',\n",
       " 'load',\n",
       " 'position',\n",
       " 'socket']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_top_terms_dict['semae3068.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organise and supervise the delivery of specified targets search plans\n",
      "provide advice on munition clearance and search related matters\n",
      "evaluate the effectiveness of specified targets search operations\n",
      "provide advice on specified targets search matters\n",
      "confirm the presence of munition(s)\n",
      "detect and locate specified targets and other risks\n",
      "contribute to locating and gaining access to specified targets\n",
      "make presentations on explosives matters\n",
      "prepare and care for equipment in an explosives environment\n",
      "manage explosives safety\n",
      "manage your own resources \n",
      "develop your competence in working with munitions or specified targets\n",
      "manage your own resources and professional development \n",
      "manage continuous improvement in activities for explosive substances and articles\n",
      "allocate and check work in your team \n",
      "provide leadership for your team\n",
      "provide leadership in your area of responsibility \n",
      "conduct an assessment of the risks in the workplace\n",
      "develop the organizational safety policy and/or strategy for explosives\n",
      "implement the organizational safety policy and/or strategy for explosives\n",
      "analyse and identify aggregated hazards and risks for explosives\n",
      "determine and implement aggregated risk control measures for explosives\n",
      "implement risk control measures for explosive substances and/or articles safety\n",
      "develop and implement assurance systems for explosives safety\n",
      "carry out assurance audit of systems for explosives safety\n",
      "develop emergency response systems and procedures for explosives safety\n",
      "investigate explosives-related safety incidents\n",
      "contribute to the investigation of explosives safety incidents\n",
      "assess explosives licence applications\n",
      "prepare and submit an explosives licence application\n",
      "review the factors affecting the safety of specific explosive substances and/or articles\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for ix in range(len(df_nos_select)):\n",
    "    nos = df_nos_select.index[ix]\n",
    "    tmp_sim = model.similar_by_vector(we_p_st[nos])\n",
    "    tmp_sim = [t[0] for t in tmp_sim]\n",
    "    if ix<100:\n",
    "        continue\n",
    "    if set(tmp_sim).intersection(set(new_top_terms_dict[nos])):\n",
    "        print(df_nos_select.loc[nos]['NOS Title'])\n",
    "        counter +=1\n",
    "    if counter>30:\n",
    "        break\n",
    "    #print(nos)\n",
    "    ##print(df_nos_select.loc[nos]['NOS Title'])\n",
    "    #print(new_top_terms_dict[nos])\n",
    "    #print()\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform hierarchical clustering on each super-suite separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# super-suites:\n",
    "SELECT_MODES = ['engineering','management','construction','financialservices']\n",
    "\n",
    "SAVEHC = False\n",
    "STRATEGY = 'tfidf' #'we' or 'tfidf'\n",
    "WARD = 'a'\n",
    "for SELECT_MODE in SELECT_MODES[:1]:\n",
    "    df_nos_n, final_groups, larger_suites, cluster_name, cluster_name_save, \\\n",
    "                cluster_name_figs = select_subdf(SELECT_MODE, 1, \n",
    "                                                 1, df_nos_select)\n",
    "    print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "    # remove legacy nos\n",
    "    print('nb with legacy nos: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n['NOS Title'].map(lambda x: 'legacy' not in x)]\n",
    "    print('nb without legacy nos 1: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n.index.map(lambda x: not x[-5:]=='l.pdf')]\n",
    "    print('nb without legacy nos 2: ',len(df_nos_n))\n",
    "    suites_in_clus = {}\n",
    "    groups_clus = df_nos_n.groupby('One_suite')\n",
    "    for name, group in groups_clus:\n",
    "        suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "    if STRATEGY == 'tfidf':\n",
    "        # this is to get the restricted corpus (to transform, not for fitting)\n",
    "        textfortoken = df_nos_n['pruned_lemmas']\n",
    "        tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "    elif STRATEGY == 'we':\n",
    "        tfidfm_n = df_nos_n[all_we_cols].values\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # get labels\n",
    "    if paramsn['bywhich'] == 'suites':\n",
    "        standard_labels_n = list(df_nos_n.groupby('One_suite').groups.keys())\n",
    "    else:\n",
    "        standard_labels_n = list(df_nos_n['NOS Title'].values)\n",
    "\n",
    "    for ix,t in enumerate(standard_labels_n):\n",
    "        if len(t)>500:\n",
    "            # manual correction because of pdf extraction\n",
    "            standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "\n",
    "    # check best features in a few NOS\n",
    "    if STRATEGY == 'tfidf':\n",
    "        for s_idx in range(1): #34):\n",
    "            s_idx = 0#standard_labels_n.index(\n",
    "                #'lift and move permanent way materials, components and equipment')\n",
    "            TF= tfidfm_n[s_idx,:].T.todense()\n",
    "            print(standard_labels_n[s_idx])\n",
    "            ix = np.argsort(TF, axis = 0)\n",
    "            for i in ix[-20:][::-1]: #enumerate(feature_names_n):\n",
    "                i = np.array(i)\n",
    "                print(feature_names_n[i[0][0]],np.around(TF[i[0][0]][0,0],3))\n",
    "            print()\n",
    "\n",
    "\n",
    "    # perform hierarchical clustering\n",
    "    distances_n, linkage_matrix_n, c_n, _ = do_hierarch_clustering(tfidfm_n,\n",
    "                                                                   method = 'average',\n",
    "                                                                   metric = 'cosine',\n",
    "                                                                   DOPLOTS= False)\n",
    "\n",
    "\n",
    "    # Plotting the distance between successive clusters: is there a knee?\n",
    "    z = linkage_matrix_n[::-1,2]\n",
    "    knee = np.diff(z, 2)\n",
    "    \n",
    "    #fig = plt.figure(figsize = (6,6))\n",
    "    fig, ax1 = plt.subplots(figsize = (12,6))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(range(1, len(z)+1), z, 'g-')\n",
    "    ax2.plot(range(2, len(linkage_matrix_n)), knee, 'b-')\n",
    "    plt.xlim([0,500])\n",
    "    ax1.set_ylabel('cluster distance', color='g')\n",
    "    ax2.set_ylabel('derivative', color='b')\n",
    "\n",
    "    #plt.plot(range(1, len(z)+1), z)\n",
    "    #plt.plot(range(2, len(linkage_matrix_n)), knee)\n",
    "    #plt.xlabel('partition')\n",
    "    #plt.ylabel('cluster distance')\n",
    "    plt.title(cluster_name_figs)\n",
    "    goodness = []\n",
    "    for i in range(3,100): #len(z)-2):\n",
    "        lr = LinearRegression(normalize = True)\n",
    "        lr = lr.fit(np.arange(1,i+1).reshape(-1, 1), z[:i].reshape(-1, 1))\n",
    "        a1 = lr.score(np.arange(1,i+1).reshape(-1, 1), z[:i].reshape(-1, 1))\n",
    "        lr = LinearRegression(normalize = True)\n",
    "        lr = lr.fit(np.arange(i, len(z)).reshape(-1, 1), z[i:].reshape(-1, 1))\n",
    "        a2 = lr.score(np.arange(i, len(z)).reshape(-1, 1), z[i:].reshape(-1, 1))\n",
    "        goodness.append(np.around(a1 + a2,4))\n",
    "        #a1 = scipy.stats.linregress(range(1,i+1), z[:i])\n",
    "        #a2 = scipy.stats.linregress(range(i, len(z)), z[i:])\n",
    "        #goodness.append(np.around(a1[2]**2 + a2[2]**2,4))\n",
    "    \n",
    "    plt.figure(figsize = (12,6))\n",
    "    #print(goodness)\n",
    "    plt.plot(np.arange(3,100), goodness)#len(z)-2),goodness)\n",
    "    plt.title(cluster_name_figs)\n",
    "    ixg = np.array(goodness).argmax()+3\n",
    "    print('best t-point: ',ixg)\n",
    "\n",
    "    num_ideal = np.ceil(len(df_nos_n)/10)\n",
    "    print('The ideal number of clusters would be: ',num_ideal)\n",
    "    num_clust1 = knee.argmax() + 2\n",
    "    knee[knee.argmax()] = 0\n",
    "    num_clust2 = knee.argmax() + 2\n",
    "    \n",
    "    if SELECT_MODE == 'engineering':\n",
    "        num_clust = 50 #170 #ixg #max([num_clust1,num_clust2]) #clusters2use[SELECT_MODE][2]\n",
    "    else:\n",
    "        if num_clust1 == 2:\n",
    "            num_clust = num_clust2 #2000\n",
    "        elif num_clust2 == 2:\n",
    "            num_clust = num_clust1 #2000\n",
    "        else:\n",
    "            num_clust = min([num_clust1,num_clust2])\n",
    "            \n",
    "    print('The two peaks are, in order: ',num_clust1, num_clust2)\n",
    "    print('The selected num clust is ',num_clust)\n",
    "    #num_clust = max([num_clust1,num_clust2])\n",
    "\n",
    "    for t in np.arange(0,1,0.05):\n",
    "        labels_n = fcluster(linkage_matrix_n, t, criterion='distance')\n",
    "        n_clust = len(set(labels_n))\n",
    "        if n_clust <= num_clust:\n",
    "            cutting_th_n = t\n",
    "            break\n",
    "    print('cutting threshold: {}'.format(cutting_th_n))       \n",
    "    \n",
    "    #Plot the dendrogram (cutting at threshold)\n",
    "    #cutting_th_n = 0.6\n",
    "    h = .05*len(df_nos_n)\n",
    "    fig, ax = plt.subplots(figsize=(28, h)) # set size\n",
    "    ax = dendrogram(linkage_matrix_n, \n",
    "                    labels = [t.capitalize() for t in standard_labels_n], \n",
    "                    orientation = 'right', \n",
    "                    leaf_font_size=6,\n",
    "                   color_threshold = cutting_th_n,\n",
    "                   truncate_mode = 'level', p =15)#,\n",
    "                   #above_threshold_color = 'k');\n",
    "\n",
    "    plt.tick_params(axis= 'y',\n",
    "                    labelsize = 24)\n",
    "    plt.title('Hierarchical clustering for {}'.format(cluster_name_figs), fontsize = 30)\n",
    "#              'Hierarchical Clustering Dendrogram of Selected NOS', fontsize = 20)\n",
    "    plt.xlabel('Distance', fontsize = 30)\n",
    "    plt.ylabel('NOS title',fontsize = 30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if SAVEHC:\n",
    "        plt.savefig(os.path.join(output_dir, \n",
    "                                 'all_nos_cut_dendrogram_in_{}_{}_{}_{}_{}.png'.format(\n",
    "            cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)), \n",
    "                    bbox_inches = \"tight\")   \n",
    "        \n",
    "    # now get and save the clusters\n",
    "    labels_n = fcluster(linkage_matrix_n, cutting_th_n, criterion='distance')\n",
    "    short_df_n = df_nos_n.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "\n",
    "    short_df_n['hierarchical'] = labels_n\n",
    "    short_df_n = short_df_n.set_index('index')\n",
    "    if SAVEHC:\n",
    "        short_df_n.to_csv(os.path.join(output_dir, \n",
    "                                 'all_nos_cut_labels_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "            cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD)))\n",
    "        \n",
    "    # print the result of the cut dendrogram\n",
    "    hierarchical_dict= {}\n",
    "    L = {}\n",
    "    D = {}\n",
    "    for ic in range(1,num_clust+1):\n",
    "        tmp_local = short_df_n['NOS Title'][\n",
    "            short_df_n['hierarchical']==ic].values\n",
    "        if len(tmp_local)<3:\n",
    "            continue\n",
    "        hierarchical_dict['{}'.format(ic)] = list(tmp_local)\n",
    "        A = distance.squareform(distances_n)[(short_df_n['hierarchical']==ic).values,:][:,\n",
    "                            (short_df_n['hierarchical']==ic).values]\n",
    "        if A.sum()>0:\n",
    "            A = np.triu(A)\n",
    "            A = A[A[:]>0]\n",
    "        else:\n",
    "            A = np.ones(1)\n",
    "        D['{}'.format(ic)] = np.around(np.mean(A),3)\n",
    "        L['{}'.format(ic)] = (short_df_n['hierarchical']==ic).sum()\n",
    "    L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "    D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "    L = L.join(D)\n",
    "    if SAVEHC:\n",
    "        L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "            by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                                '/all_nos_cut_clusters_in_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY,WARD))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def do_kmean(xx, ks=np.arange(2,4), N=100, N2=100):\n",
    "        stab = []\n",
    "        for k in ks:\n",
    "            t0 = time.time()\n",
    "            # do N iterations\n",
    "            stab0 = []\n",
    "            A = np.empty((xx.shape[0],N))\n",
    "            for i in range(N):\n",
    "                k_clus = KMeans(k, n_init = 1, random_state = np.random.randint(1e7))\n",
    "                A[:,i] = k_clus.fit_predict(xx)\n",
    "                for j in range(i):\n",
    "                    stab0.append(adjusted_rand_score(A[:,i],A[:,j]))\n",
    "            # get stability of clusters for this nb of clusters\n",
    "            stab.append(np.mean(stab0))\n",
    "            print_elapsed(t0,'kmeans for k={}'.format(k))\n",
    "        # what number of clusters has highest stability?\n",
    "        kmax = ks[np.array(stab).argmax()]\n",
    "        # redo one last clustering with kmax \n",
    "        # and lots of iteration to get the stable versions\n",
    "        k_clus = KMeans(kmax, n_init= N2)\n",
    "        labels = k_clus.fit_predict(xx)\n",
    "        return labels, k_clus, kmax, stab\n",
    "\n",
    "def get_distance_k(df_row, centroids):\n",
    "    L = len(df_row.values) # last column is the cluster class\n",
    "    distance = pairwise_distances(df_row.values[:L-1].reshape(1, -1), \n",
    "                                  centroids[int(df_row['k_cluster'])].reshape(1, -1))\n",
    "    distance = distance[0][0]\n",
    "    return distance\n",
    "\n",
    "def get_distance_k2(df_row, labels, centroids):\n",
    "    Nf = df_row.shape[0] # last column is the cluster class\n",
    "    N = centroids.shape[0]\n",
    "    distance = np.empty(N)\n",
    "    for ix in range(N):\n",
    "        tmp = df_row[labels == ix]\n",
    "        tmp2 = pairwise_distances(tmp, centroids[ix].reshape(1,-1))\n",
    "        distance[ix] = tmp2[0][0]\n",
    "    #distance = distance[0][0]\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_MODES = ['engineering','management','construction','financialservices']\n",
    "\n",
    "SAVEKM = True\n",
    "for SELECT_MODE in SELECT_MODES[:1]:\n",
    "    df_nos_n, final_groups, larger_suites, cluster_name, cluster_name_save, \\\n",
    "                cluster_name_figs = select_subdf(SELECT_MODE, 1, \n",
    "                                                 1,df_nos_select)\n",
    "    print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "    # remove legacy nos\n",
    "    print('nb with legacy nos: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n['NOS Title'].map(lambda x: 'legacy' not in x)]\n",
    "    print('nb without legacy nos 1: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n.index.map(lambda x: not x[-5:]=='l.pdf')]\n",
    "    print('nb without legacy nos 2: ',len(df_nos_n))\n",
    "    suites_in_clus = {}\n",
    "    groups_clus = df_nos_n.groupby('One_suite')\n",
    "    for name, group in groups_clus:\n",
    "        suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "    if STRATEGY == 'tfidf':\n",
    "        # this is to get the restricted corpus (to transform, not for fitting)\n",
    "        textfortoken = df_nos_n['pruned_lemmas']\n",
    "        tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "    elif STRATEGY == 'we':\n",
    "        tfidfm_n = df_nos_n[all_we_cols].values\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # get labels\n",
    "    if paramsn['bywhich'] == 'suites':\n",
    "        standard_labels_n = list(df_nos_n.groupby('One_suite').groups.keys())\n",
    "    else:\n",
    "        standard_labels_n = list(df_nos_n['NOS Title'].values)\n",
    "\n",
    "    for ix,t in enumerate(standard_labels_n):\n",
    "        if len(t)>500:\n",
    "            # manual correction because of pdf extraction\n",
    "            standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "            \n",
    "    #\n",
    "    try:\n",
    "        print(type(tfidfm_n.toarray()))\n",
    "        tfidfm_n = tfidfm_n.toarray()#StandardScaler.fit_transform(tfidfm_n.toarray())\n",
    "        #tfidfm_n = tfidfm_n.toarray()\n",
    "    except:\n",
    "        print(type(tfidfm_n))\n",
    "        tfidfm_n = tfidfm_n\n",
    "    \n",
    "    xx = StandardScaler().fit_transform(tfidfm_n)\n",
    "    out = do_kmean(xx, np.arange(140,221,10), N=100, N2= 100)\n",
    "    #k_clusters = out[1].labels_.tolist()\n",
    "    #centroids = out[1].cluster_centers_\n",
    "    #short_df['k_distance'] = tfidfm_df.apply(get_distance_k, axis =1)\n",
    "    short_df_n = df_nos_n.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "    short_df_n['kmeans'] = out[0]\n",
    "    short_df_n = short_df_n.set_index('index')\n",
    "    if SAVEKM or True:\n",
    "        short_df_n.to_csv(os.path.join(output_dir, \n",
    "                             'all_nos_cut_labels_kmeans_in_{}_{}_{}_{}.csv'.format(\n",
    "                cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY)))\n",
    "    \n",
    "    print(out[2],out[3])\n",
    "    # get the clusters\n",
    "    labels2 = np.empty(len(short_df_n), dtype = np.int)\n",
    "    for ix in range(len(short_df_n)):\n",
    "        labels2[ix] = int(short_df_n['kmeans'].iloc[ix])\n",
    "    out_d = get_distance_k2(xx, labels2, out[1].cluster_centers_)\n",
    "    \n",
    "    ## collect the centroids (that is, the suite closest to the centroid) \n",
    "    ## and print the result of the clustering\n",
    "    #kmeans_dict= {}\n",
    "    #most_central = []\n",
    "    #igroup = 0\n",
    "    #for name, group in short_df.groupby('k_means'):\n",
    "    #    kmeans_dict['{}'.format(name)] = group['Suite_names'].values\n",
    "    #    if igroup < 10:\n",
    "    #        print(name, group.sort_values(by = 'k_distance').head(3))\n",
    "    #    igroup += 1\n",
    "    #    most_central.append(group.sort_values(by = 'k_distance').head(1))\n",
    "    #if SAVEKM:\n",
    "    #    pd.DataFrame.from_dict(kmeans_dict, orient = 'index').to_csv(output_dir +\n",
    "    #                            '/Kmeans_results_{}_{}_{}.csv'.format(qualifier,bywhich,mode))\n",
    "    \n",
    "    # print the result of the cut dendrogram\n",
    "    cluster_dict= {}\n",
    "    L = {}\n",
    "    D = {}\n",
    "    N = max(short_df_n['kmeans'].values)\n",
    "    print(N)\n",
    "    for ic in range(N+1):\n",
    "        tmp_local = short_df_n['NOS Title'][short_df_n['kmeans']==ic].values\n",
    "        if len(tmp_local)<3:\n",
    "            continue\n",
    "        cluster_dict['{}'.format(ic)] = list(tmp_local)\n",
    "        #A = distance.squareform(distances_n)[(short_df_n['hierarchical']==ic).values,:][:,\n",
    "        #                    (short_df_n['hierarchical']==ic).values]\n",
    "        #if A.sum()>0:\n",
    "        #    A = np.triu(A)\n",
    "        #    A = A[A[:]>0]\n",
    "        #else:\n",
    "        #    A = np.ones(1)\n",
    "        D['{}'.format(ic)] = out_d[ic]#np.around(np.mean(A),3)\n",
    "        L['{}'.format(ic)] = (short_df_n['kmeans']==ic).sum()\n",
    "    L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "    D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "    L = L.join(D)\n",
    "    if SAVEKM or True:\n",
    "        L.join(pd.DataFrame.from_dict(cluster_dict, orient = 'index')).sort_values(\n",
    "            by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                                '/all_nos_cut_clusters_kmeans_in_{}_{}_{}_{}.csv'.format(\n",
    "                                cluster_name_save,qualifier,paramsn['ngrams'],STRATEGY))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp =pd.read_csv(os.path.join(output_dir, \n",
    "#                                 'all_nos_cut_labels_in_{}_{}_{}_v2.csv'.format(\n",
    "#            cluster_name_save,qualifier,paramsn['ngrams'])))\n",
    "#tmp2 = pd.read_csv(os.path.join(output_dir, \n",
    "#                             'all_nos_cut_labels_kmeans_in_{}_{}_{}.csv'.format(\n",
    "#        cluster_name_save,qualifier,paramsn['ngrams'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted_rand_score(tmp['hierarchical'].values,tmp2['kmeans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
