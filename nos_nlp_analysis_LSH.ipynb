{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions and snippets of code that are useful for analysing text. Most of the techniques used are unsupervised. Functions are defined up front and then used in sections below.\n",
    "\n",
    "This notebook is to apply:\n",
    "- Tokenizers (based on n-grams and 'as_is')\n",
    "- LSH\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse html\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "#HTML Parser Methods\n",
    "#Initializing lists\n",
    "    lsData = list()\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.lsData.append(data)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return ''.join(self.lsData)\n",
    "\n",
    "           \n",
    "def strip_tags(some_html):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes html tags.\n",
    "    Returns a string.\n",
    "    \"\"\"\n",
    "    s = MyHTMLParser()\n",
    "    s.lsData = list()\n",
    "    s.feed(some_html)\n",
    "    data = s.get_data()\n",
    "    s.reset\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/glove.twitter.27B'\n",
    "\n",
    "# to make the glove model file compatible with gensim\n",
    "#for dim in ['25','50','100','200']:\n",
    "##    glove_file = os.path.join(glove_dir,'glove.twitter.27B.{}d.txt'.format(dim))\n",
    "#    tmp_file = os.path.join(glove_dir, 'word2vec.glove.twitter.27B.{}d.txt'.format(dim) )\n",
    "#    _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "LOADGLOVE = False\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.glove.twitter.27B.100d.txt'))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "#r2 = requests.get(\"https://www.instituteforapprenticeships.org/api/fullstandards/\")\n",
    "#df_api= pd.DataFrame(r2.json())\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHashLSHEnsemble, MinHash, MinHashLSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(text, char_ngram=5):\n",
    "    '''\n",
    "    This function splits strings into continuous sets of characters of length n. In the current example n = 5.\n",
    "    '''\n",
    "    if len(text) == 5:\n",
    "        res = set([text, text])\n",
    "    else:\n",
    "        res = set(text[head:head + char_ngram] \\\n",
    "               for head in range(0, len(text) - char_ngram))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "shingled_desc = [shingles(desc) for desc in df_nos['clean_full_text']]\n",
    "print_elapsed(t0, 'splitting the text into groups of characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create hash signatures for shingles\n",
    "t0 = time.time()\n",
    "hash_objects = []\n",
    "for i in range(len(shingled_desc)):\n",
    "    m = MinHash(num_perm=200)\n",
    "    hash_objects.append(m)\n",
    "print_elapsed(t0, 'creating hash signatures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVELSH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    for d in desc:\n",
    "        hash_objects[ix].update(d.encode('utf8'))\n",
    "print_elapsed(t0, 'encoding hash objects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "standard_labels = list(df_nos.index.values) #df_nos['URN'].values)\n",
    "title_labels = list(df_nos['NOS Title'].values)\n",
    "urn_labels = list(df_nos['URN'].values)\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    content.append((standard_labels[ix], hash_objects[ix]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSH and Jaccard similarity threshold\n",
    "LSH_th = 0.8\n",
    "lsh = MinHashLSH(threshold=LSH_th, num_perm=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,elem in enumerate(content):\n",
    "    #lsh.insert('{}'.format(ix), elem[1]) #elem[0], elem[1])\n",
    "    lsh.insert(elem[0], elem[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For each standard search all signatures and identify potential clashes (e.g. other standards \n",
    "# with Jaccard similarity of shingle sets greater or equal to the threshold). \n",
    "# Note: some of the candidates might be false positives.\n",
    "candidates = {}\n",
    "singletons = []\n",
    "candidates2 = []\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "    if len(result) >1:\n",
    "        full_result = []\n",
    "        for res in result:\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        \n",
    "        candidates[standard_labels[ix]] = full_result\n",
    "        candidates2.append(result)\n",
    "        if ix<40:\n",
    "            print(urn_labels[ix], ': ', full_result)\n",
    "            print('***************')\n",
    "    else:\n",
    "        singletons.append(standard_labels[ix])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of NOS that found at least a match = total NOS - number of singletons\n",
    "print('Nb. of NOS that were not matched with anything: {}'.format(len(singletons)))\n",
    "print('Nb. of NOS that matched: {}'.format(len(title_labels) - len(singletons)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''# If we make the assumptions that groups are closed and don't form chains, then:\n",
    "# nb of groups of N = nb of matches of length N / N\n",
    "all_lengths = np.array([len(t) for t in candidates2])\n",
    "unique_lengths = list(set(all_lengths))\n",
    "len_counts = []\n",
    "for ln in unique_lengths:\n",
    "    len_counts.append((ln,np.sum(all_lengths == ln)))\n",
    "print(np.sum([t[1] for t in len_counts]))\n",
    "print(len(candidates2),len(candidates))\n",
    "print(len_counts)\n",
    "'''\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think that the first thing should be to create an adjacency matrix\n",
    "t0 = time.time()\n",
    "Nmatched = len(title_labels) - len(singletons)\n",
    "Adj_matrix = np.zeros((Nmatched,Nmatched))\n",
    "# create dictionary of indices\n",
    "indices = {}\n",
    "indices_reverse = {}\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    indices[candidate] = ix\n",
    "    indices_reverse[ix] = candidate\n",
    "# now cycle again through the matched NOS and populate the adjacency matrix\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    idx1 = ix\n",
    "    for k in candidates[candidate]:\n",
    "        # now this is a list of tuples, where the first element is the urn label\n",
    "        idx2 = indices[k[2]]\n",
    "        Adj_matrix[idx1,idx2] = 1\n",
    "\n",
    "print_elapsed(t0,'creating the adjacency matrix')\n",
    "#plt.figure(figsize = (5,5))\n",
    "#plt.imshow(Adj_matrix[:200,:200])\n",
    "print('The highest degree in the adjacency matrix is: ', np.max(np.sum(Adj_matrix,axis=1)))\n",
    "print('The number of matched couples are: ', np.sum(np.sum(Adj_matrix, axis = 1)==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the NOS that were matched as similar\n",
    "t0 = time.time()\n",
    "matched_groups = []\n",
    "matched_indices = []\n",
    "for ix in range(Adj_matrix.shape[0]):\n",
    "    idx_used = []\n",
    "    # find the adjacent nodes\n",
    "    where_list = list(np.where(Adj_matrix[ix])[0])\n",
    "    where_list_cumul = []\n",
    "    # don't go into the rabbit hole of nodes with very high degree - also, don't use indices already matched\n",
    "    if len(where_list)<60000 and (ix not in matched_indices):\n",
    "        for ix2 in where_list:\n",
    "            # if the neighborhood has connections to indices that we haven't included yet, \n",
    "            # add them to the list to be analysed later\n",
    "            where_list_cumul += list(np.where(Adj_matrix[ix2])[0])\n",
    "            idx_used.append(ix2)\n",
    "            # grow the neighbourhood by adding the new connections\n",
    "            new_list = [t for t in where_list_cumul if t not in where_list]\n",
    "            # don't go into the rabbit hole of nodes with very high degree\n",
    "            if len(new_list)>60000:\n",
    "                break # this one tells it to break the inner for cycle - it goes to the next if\n",
    "            if len(new_list):\n",
    "                # if the length is zero it means there are no new connected nodes\n",
    "                where_list+=new_list\n",
    "        # if it has never gone into a rabbit hole then add the group just found\n",
    "        # if and only if the neighbourhood is self-contained, that is if the nodes for which we have collected the\n",
    "        if (set(idx_used) == set(where_list)) and (len(new_list)<6):\n",
    "            if len(new_list)>6:\n",
    "                print('got here after breaking', idx_used)\n",
    "            matched_groups.append(tuple(idx_used))\n",
    "            matched_indices += idx_used #[t for t in idx_used if t not in matched_indices]\n",
    "print_elapsed(t0, 'grouping the similar NOS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in matched_couples:\n",
    "#    if not (t in matched_groups):\n",
    "#        print(t)\n",
    "# show some of the groups\n",
    "groups_length = [len(t) for t in matched_groups]\n",
    "print('Number of groups of size up to 3: ', np.sum([t<4 for t in groups_length]))\n",
    "nbprint = 0\n",
    "for t in matched_groups:\n",
    "    if len(t)>4 and len(t)<7:\n",
    "        print(t)\n",
    "        for it in t:\n",
    "            print(np.where(Adj_matrix[it])[0])\n",
    "        nbprint+=1\n",
    "    if nbprint>2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all the groups are not overlapping: if it prints something is bad\n",
    "print('If something gets printed next, then the groups indentified have overlaps')\n",
    "matched_groups2 = []\n",
    "for it,t in enumerate(matched_groups):\n",
    "    if it>40000:\n",
    "        break\n",
    "    flag = 0\n",
    "    for t2 in matched_groups:\n",
    "        if t != t2:\n",
    "            if set(t).intersection(set(t2)):\n",
    "                print(t,t2)\n",
    "                flag += 1\n",
    "                #if (set(t)-set(t2)) and (set(t2) - set(t)):\n",
    "                #    print(t,t2)\n",
    "    if flag == 0:\n",
    "        matched_groups2.append(t)\n",
    "    else:\n",
    "        print(flag)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "N = Adj_matrix.shape[0]\n",
    "Nmatched= len(matched_indices)\n",
    "print('number of grouped NOS', N, 'number of matched NOS', Nmatched)\n",
    "# finally, check that the grouped and the non-grouped indices have a separate adjacency matrix\n",
    "Adj_matrix2 = copy.deepcopy(Adj_matrix)\n",
    "leftout_indices = list(set(range(N)) - set(matched_indices))\n",
    "neworder_indices = np.array(matched_indices + leftout_indices)\n",
    "# rearrange rows and columns of the adjacency matrix\n",
    "Adj_matrix2 = Adj_matrix2[:, neworder_indices][neworder_indices]\n",
    "print('For the two parts of the Adjacency matrix to be separate, there has to be no edges between the two.')\n",
    "print('The number of edges is: ')\n",
    "print(np.sum(Adj_matrix2[:Nmatched,Nmatched:]))\n",
    "# check the matrix is symmetric\n",
    "print('if 1 the matrix is symmetric: ', np.mean(Adj_matrix2 == Adj_matrix2.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leftout_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now redo the dictionary to save, with placing the self-contained groups first and then the unpaired ones\n",
    "from collections import OrderedDict\n",
    "candidates_grouped = OrderedDict()\n",
    "candidates_paired = OrderedDict()\n",
    "\n",
    "#print(indices_reverse)\n",
    "pair_counter = 0\n",
    "group_counter = 0\n",
    "for ix, group in enumerate(matched_groups):\n",
    "    full_result = []\n",
    "    if len(group)==2:\n",
    "        for ig in group:\n",
    "            res = indices_reverse[ig]\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        candidates_paired['Pair {}'.format(pair_counter)] = full_result\n",
    "        pair_counter+=1\n",
    "for ix, group in enumerate(matched_groups):\n",
    "    full_result = []\n",
    "    if len(group)>2:\n",
    "        for ig in group:\n",
    "            res = indices_reverse[ig]\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        candidates_grouped['Group {}'.format(group_counter)] = full_result\n",
    "        group_counter+=1\n",
    "    \n",
    "#candidates_grouped['Finished with groups'] = '-'\n",
    "\n",
    "'''# now add the ones that weren't matched\n",
    "for ix in leftout_indices:\n",
    "    res = indices_reverse[ig]\n",
    "    candidates_grouped[res] = candidates[res]\n",
    "'''\n",
    "# try to save it, just to see what it looks like\n",
    "if SAVELSH or True:\n",
    "    tmp = pd.DataFrame.from_dict(candidates_grouped, orient = 'index')\n",
    "    print(tmp.columns)\n",
    "    tmp.to_csv(output_dir +  '/LSH_results_grouped_no_pairs_{}_th{}.csv'.format(qualifier,LSH_th))\n",
    "\n",
    "if SAVELSH or True:\n",
    "    pd.DataFrame.from_dict(candidates_paired, orient = 'index', columns = [\n",
    "                                                                    'NOS 1','NOS 2']).to_csv(output_dir + \n",
    "                                                '/LSH_results_paired_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVELSH and False:\n",
    "    pd.DataFrame.from_dict(candidates, orient = 'index').to_csv(output_dir + '/LSH_results_{}_th{}.csv'.format(\n",
    "        qualifier,LSH_th))\n",
    "    with open(output_dir + '/Candidates_nos_{}_th{}.pickle'.format(qualifier,LSH_th),'wb') as f:\n",
    "        pickle.dump(candidates,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many duplicates per suite / per originating organisation\n",
    "# get the value count\n",
    "for col in ['One_suite', 'Developed By']:\n",
    "    duplicated_suites = df_nos.loc[list(candidates.keys())][col].value_counts()\n",
    "    all_suites = df_nos[col].value_counts()\n",
    "    if col == 'One_suite':\n",
    "        N = 70\n",
    "        fig =plt.figure(figsize = (11,18))\n",
    "        plt.ylabel('Suite',fontsize = 18)\n",
    "    else:\n",
    "        N = 32\n",
    "        fig = plt.figure(figsize = (7,12))\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Counts', fontsize = 18)\n",
    "    with sns.plotting_context('talk'):\n",
    "        duplicated_suites[:N][::-1].plot('barh', color = nesta_colours[3])\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH or True:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_counts_for_duplicates.png'.format(col)))\n",
    "\n",
    "    # now divide by the sum (so we get fractions)\n",
    "    #duplicated_suites = duplicated_suites.map(lambda x: x/duplicated_suites.sum())\n",
    "    #all_suites = all_suites.map(lambda x: x/all_suites.sum())\n",
    "    # get the ratio of proportions with respect to the full distribution\n",
    "    suites_ratio = {}\n",
    "    for row in all_suites.index[:N]:\n",
    "        try:\n",
    "            suites_ratio[row] = duplicated_suites.loc[row]/all_suites.loc[row]\n",
    "        except:\n",
    "            suites_ratio[row] = 0\n",
    "    suites_ratio = pd.DataFrame.from_dict(suites_ratio, orient = 'index', columns = ['ratio'])\n",
    "    ## order by decreasing ratios \n",
    "    #suites_ratio= suites_ratio.sort_values(by='ratio', ascending = False)\n",
    "    # plot the ratio\n",
    "    if col == 'One_suite':\n",
    "        fig=plt.figure(figsize = (12,18))\n",
    "        ix = range(N)\n",
    "        plt.ylabel('Suite',fontsize = 18)\n",
    "    else:\n",
    "        fig =plt.figure(figsize = (7,12))\n",
    "        ix = range(N)\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Proportion of NOS', fontsize = 18)\n",
    "    plt.xlim([0,1])\n",
    "    with sns.plotting_context('talk'):\n",
    "        suites_ratio['ratio'].iloc[ix][::-1].plot('barh', color = nesta_colours[3])\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH or True:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_ratios_for_duplicates_{}.png'.format(col,LSH_th)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute level of overlap for each group identified via the LSH algorithm\n",
    "- Use the average level of pairwise overlap between NOS in each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    res = float(intersection / union)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to assign keys and NOS to a dictionary (?)\n",
    "#print(candidates_grouped['Group 0'])\n",
    "content_dict = {}\n",
    "for ix,elem in enumerate(content):\n",
    "    content_dict[elem[0]] = elem[1]\n",
    "    \n",
    "print(content_dict[lsh.query(elem[1])[0]].jaccard(content_dict[lsh.query(elem[1])[0]]))\n",
    "print(lsh.query(elem[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average level of overlap within groups and compare to the average number of common words\n",
    "#candidates_grouped_new = copy.deepcopy(candidates_grouped)\n",
    "#A = []\n",
    "#B = []\n",
    "all_j_values = []\n",
    "all_w_values = []\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_paired.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_paired[g]\n",
    "        j_values = []\n",
    "        g_suites = []\n",
    "        g_cat = []\n",
    "        w_values = []\n",
    "        \n",
    "        for t,ix1 in enumerate(G):\n",
    "            # create bag of words that retain duplicated words\n",
    "            if len(G)<50:\n",
    "                t1 = []\n",
    "                for tt in df_nos.loc[ix1[2]]['clean_full_text'].split():\n",
    "                    if tt not in t1:\n",
    "                        t1.append(tt)\n",
    "                    else:\n",
    "                        t1.append(tt+'1')\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values.append(content_dict[ix1[2]].jaccard(content_dict[ix2[2]]))\n",
    "                if len(G)<50:\n",
    "                    # create bag of words that retain duplicates\n",
    "                    t2 = []\n",
    "                    for tt in df_nos.loc[ix2[2]]['clean_full_text'].split():\n",
    "                        if tt not in t2:\n",
    "                            t2.append(tt)\n",
    "                        else:\n",
    "                            t2.append(tt + '1')\n",
    "                    w_values.append(len(set(t1).intersection(set(t2)))/ (len(set(t1))+len(set(t2))) *2)\n",
    "            g_suites.append(df_nos.loc[ix1[2]]['One_suite'])\n",
    "            g_cat.append(df_nos.loc[ix1[2]]['NOSCategory'])\n",
    "        \n",
    "        if len(G)<50:\n",
    "            # keep jaccard similarities\n",
    "            all_j_values += j_values\n",
    "            # keep proportion of common words\n",
    "            all_w_values += w_values\n",
    "        # append average jaccard similarity and number of unique suites in the group divided by groups size\n",
    "        tmp = [t == 'generic' for t in g_cat]\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "# print number of common words\n",
    "#print(np.mean(all_w_values[all_j_values>.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average level of overlap within groups bigger than two\n",
    "candidates_grouped_new = copy.deepcopy(candidates_grouped)\n",
    "#A = []\n",
    "#B = []\n",
    "all_j_values = []\n",
    "all_w_values = []\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_grouped.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_grouped[g]\n",
    "        j_values = []\n",
    "        g_suites = []\n",
    "        g_cat = []\n",
    "        w_values = []\n",
    "        \n",
    "        for t,ix1 in enumerate(G):\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values.append(content_dict[ix1[2]].jaccard(content_dict[ix2[2]]))\n",
    "        candidates_grouped_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "\n",
    "#candidates_grouped_new.pop('Finished with groups')\n",
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns = {0:'Avg group similarity'})\n",
    "# rename the columns\n",
    "rename_dict= {}\n",
    "for ii in range(len(tmp.columns)):\n",
    "    rename_dict[ii] = 'NOS {}'.format(ii)\n",
    "tmp = tmp.rename(columns = rename_dict)\n",
    "# sort by descending values of the average similarity\n",
    "tmp = tmp.sort_values(by = 'Avg group similarity', ascending = False)\n",
    "# rename the rows after the sorting\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "rename_dict = {}\n",
    "for ii in range(len(tmp)):\n",
    "    rename_dict[ii] = 'Group {}'.format(ii)\n",
    "tmp = tmp.rename(index = rename_dict)\n",
    "\n",
    "if SAVELSH or True:\n",
    "#   tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns= {0:'Avg group similarity'})\n",
    "    tmp.to_csv(output_dir + '/LSH_results_grouped_no_pairs_with_score_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average level of overlap within pairs\n",
    "candidates_paired_new = copy.deepcopy(candidates_paired)\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_paired.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_paired[g]\n",
    "        j_values = []\n",
    "        for t,ix1 in enumerate(G):\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values = [content_dict[ix1[2]].jaccard(content_dict[ix2[2]])]\n",
    "                if ig%30>-1:\n",
    "                    if ix1[2]=='mpqmg31.pdf':\n",
    "                        print(j_values,content_dict[ix1[2]] )\n",
    "                    #print(ix1[2],ix2[2])\n",
    "        candidates_paired_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "\n",
    "#candidates_grouped_new.pop('Finished with groups')\n",
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(candidates_paired_new, orient = 'index').rename(columns = {0:'Avg group similarity'})\n",
    "# rename the columns\n",
    "rename_dict= {}\n",
    "for ii in range(len(tmp.columns)):\n",
    "    rename_dict[ii] = 'NOS {}'.format(ii)\n",
    "tmp = tmp.rename(columns = rename_dict)\n",
    "# sort by descending values of the average similarity\n",
    "tmp = tmp.sort_values(by = 'Avg group similarity', ascending = False)\n",
    "# rename the rows after the sorting\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "rename_dict = {}\n",
    "for ii in range(len(tmp)):\n",
    "    rename_dict[ii] = 'Pair {}'.format(ii)\n",
    "tmp = tmp.rename(index = rename_dict)\n",
    "if SAVELSH or True:\n",
    "#   tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns= {0:'Avg group similarity'})\n",
    "    tmp.to_csv(output_dir + '/LSH_results_pairs_with_score_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('90%: ',np.array(all_w_values)[np.array(all_j_values)>0.9].mean())\n",
    "print('80%: ',np.array(all_w_values)[(np.array(all_j_values)>0.8) & (np.array(all_j_values)<0.9)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nos['clean_full_text'].map(lambda x: len(x.split())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad hoc requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_nos = df_nos.loc[list(candidates.keys())]\n",
    "duplicated_cos = duplicated_nos[duplicated_nos['Developed By']=='construction skills']\n",
    "duplicated_nos = duplicated_nos[duplicated_nos['Developed By']=='semta']\n",
    "tmp = list(duplicated_nos.index)\n",
    "print([t for t in tmp if 'l' in t[-7:]])\n",
    "print(duplicated_nos.columns)\n",
    "print(duplicated_nos['Version_number'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# to be old and new updated nos, it would have to be the following conditions:\n",
    "1. group of two NOS\n",
    "2. two different versions (1 and higher)\n",
    "3. same or very similar suite (that is, the same aside from numbers)\n",
    "'''\n",
    "semta_versions = []\n",
    "mixed_version = []\n",
    "semta_suites = []\n",
    "semta_titles = []\n",
    "for g in candidates_grouped_new:\n",
    "    group = candidates_grouped_new[g]\n",
    "    if group[1][0][:3] == 'sem':\n",
    "        #print(len(group))\n",
    "        semta_versions.append([])\n",
    "        semta_suites.append([])\n",
    "        semta_titles.append([])\n",
    "        if len(group)==3:\n",
    "            # only take groups of pairs\n",
    "            for ix in range(1,len(group)):\n",
    "                #print(duplicated_nos.loc[group[ix][2]]['Version_number'])\n",
    "                try:\n",
    "                    semta_versions[-1].append(duplicated_nos.loc[group[ix][2]]['Version_number'])\n",
    "                    semta_suites[-1].append(duplicated_nos.loc[group[ix][2]]['One_suite'])\n",
    "                    semta_titles[-1].append(group[ix][1])\n",
    "                except:\n",
    "                    1\n",
    "            \n",
    "            if (1.0 in semta_versions[-1]) & ((2.0 in semta_versions[-1]) or (3.0 in semta_versions[-1])):\n",
    "                # check the suites\n",
    "                suite1 = ''.join([t for t in semta_suites[-1][0] if not t.isdigit()])\n",
    "                suite2 = ''.join([t for t in semta_suites[-1][1] if not t.isdigit()])\n",
    "                title1 = semta_titles[-1][0]\n",
    "                title2 = semta_titles[-1][1]\n",
    "                out = process.extract(title1, [title2])\n",
    "                out2 = process.extract(suite1, [suite2])\n",
    "                # assume two titles are similar if the fuzzy matching is higher than 90\n",
    "                if out[0][1]>89:\n",
    "                    # if we also require the two suite names to have a match higher than 90\n",
    "                    if out2[0][1]>89:\n",
    "                        mixed_version.append(group)\n",
    "                        print(semta_suites[-1][0],semta_suites[-1][1])\n",
    "                        \n",
    "#print(mixed_version)\n",
    "len(mixed_version), len(semta_versions), len(duplicated_nos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction skills should be responsible for the very big group\n",
    "print(len(duplicated_cos))\n",
    "for g in candidates_grouped_new:\n",
    "    group = candidates_grouped_new[g]\n",
    "    if group[1][0][:3] == 'cos':\n",
    "        if len(group)>200:\n",
    "            tmp = [group[ix][0][:3] for ix in range(1,len(group))]\n",
    "            print(sum([t=='cos' for t in tmp]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
