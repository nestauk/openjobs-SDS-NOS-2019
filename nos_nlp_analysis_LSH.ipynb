{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions and snippets of code that are useful for analysing text. Most of the techniques used are unsupervised. Functions are defined up front and then used in sections below.\n",
    "\n",
    "This notebook is to apply:\n",
    "- Tokenizers (based on n-grams and 'as_is')\n",
    "- LSH\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse html\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "#HTML Parser Methods\n",
    "#Initializing lists\n",
    "    lsData = list()\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.lsData.append(data)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return ''.join(self.lsData)\n",
    "\n",
    "           \n",
    "def strip_tags(some_html):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes html tags.\n",
    "    Returns a string.\n",
    "    \"\"\"\n",
    "    s = MyHTMLParser()\n",
    "    s.lsData = list()\n",
    "    s.feed(some_html)\n",
    "    data = s.get_data()\n",
    "    s.reset\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/NR/NOS_project/NOS_results/nlp_analysis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/NR/NOS_project/NOS_results/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/glove.twitter.27B'\n",
    "\n",
    "# to make the glove model file compatible with gensim\n",
    "#for dim in ['25','50','100','200']:\n",
    "##    glove_file = os.path.join(glove_dir,'glove.twitter.27B.{}d.txt'.format(dim))\n",
    "#    tmp_file = os.path.join(glove_dir, 'word2vec.glove.twitter.27B.{}d.txt'.format(dim) )\n",
    "#    _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "LOADGLOVE = False\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.glove.twitter.27B.100d.txt'))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "#r2 = requests.get(\"https://www.instituteforapprenticeships.org/api/fullstandards/\")\n",
    "#df_api= pd.DataFrame(r2.json())\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nos[['NOS Title', 'URN', 'Clean SOC Code', 'One_suite']].rename(columns = {\n",
    "#    'NOS Title': 'NOS Title', 'URN':'URN', 'Clean SOC Code': 'SOC Code', 'One_suite': 'Suite'\n",
    "#}).to_csv(os.path.join(output_dir,'NOS_and_SOC_codes.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHashLSHEnsemble, MinHash, MinHashLSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(text, char_ngram=5):\n",
    "    '''\n",
    "    This function splits strings into continuous sets of characters of length n. In the current example n = 5.\n",
    "    '''\n",
    "    if len(text) == 5:\n",
    "        res = set([text, text])\n",
    "    else:\n",
    "        res = set(text[head:head + char_ngram] \\\n",
    "               for head in range(0, len(text) - char_ngram))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "shingled_desc = [shingles(desc) for desc in df_nos['clean_full_text']]\n",
    "print_elapsed(t0, 'splitting the text into groups of characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create hash signatures for shingles\n",
    "t0 = time.time()\n",
    "hash_objects = []\n",
    "for i in range(len(shingled_desc)):\n",
    "    m = MinHash(num_perm=200)\n",
    "    hash_objects.append(m)\n",
    "print_elapsed(t0, 'creating hash signatures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVELSH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    for d in desc:\n",
    "        hash_objects[ix].update(d.encode('utf8'))\n",
    "print_elapsed(t0, 'encoding hash objects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "standard_labels = list(df_nos.index.values) #df_nos['URN'].values)\n",
    "title_labels = list(df_nos['NOS Title'].values)\n",
    "urn_labels = list(df_nos['URN'].values)\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    content.append((standard_labels[ix], hash_objects[ix]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSH and Jaccard similarity threshold\n",
    "LSH_th = 0.8\n",
    "lsh = MinHashLSH(threshold=LSH_th, num_perm=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,elem in enumerate(content):\n",
    "    #lsh.insert('{}'.format(ix), elem[1]) #elem[0], elem[1])\n",
    "    lsh.insert(elem[0], elem[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For each standard search all signatures and identify potential clashes (e.g. other standards \n",
    "# with Jaccard similarity of shingle sets greater or equal to the threshold). \n",
    "# Note: some of the candidates might be false positives.\n",
    "candidates = {}\n",
    "singletons = []\n",
    "candidates2 = []\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "    if len(result) >1:\n",
    "        full_result = []\n",
    "        for res in result:\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        \n",
    "        candidates[standard_labels[ix]] = full_result\n",
    "        candidates2.append(result)\n",
    "        if ix<40:\n",
    "            print(urn_labels[ix], ': ', full_result)\n",
    "            print('***************')\n",
    "    else:\n",
    "        singletons.append(standard_labels[ix])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "Alternative function to get matches and organise them in groups. (From Jyl's code)\n",
    "\n",
    "def union_find(data):\n",
    "    ''''''Create Disjoint Data Structure from list of lists''''''\n",
    "    parents = {}\n",
    "    def find(i):\n",
    "        j = parents.get(i, i)\n",
    "        if j == i:\n",
    "            return i\n",
    "        k = find(j)\n",
    "        if k != j:\n",
    "            parents[i] = k\n",
    "        return k\n",
    "    for l in filter(None, data):\n",
    "        parents.update(dict.fromkeys(map(find, l), find(l[0])))\n",
    "    merged = {}\n",
    "    for k, v in parents.items():\n",
    "        merged.setdefault(find(v), []).append(k)\n",
    "    return list(merged.values())\n",
    "\n",
    "\n",
    "candidates = []\n",
    "for ix, skill_set in enumerate(skill_sets):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "#    if len(result) >1:\n",
    "    candidates.append(result)\n",
    "\n",
    "disjoint_candidates = union_find(candidates)\n",
    "\n",
    "'''\n",
    "print('to try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of NOS that found at least a match = total NOS - number of singletons\n",
    "print('Nb. of NOS that were not matched with anything: {}'.format(len(singletons)))\n",
    "print('Nb. of NOS that matched: {}'.format(len(title_labels) - len(singletons)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''# If we make the assumptions that groups are closed and don't form chains, then:\n",
    "# nb of groups of N = nb of matches of length N / N\n",
    "all_lengths = np.array([len(t) for t in candidates2])\n",
    "unique_lengths = list(set(all_lengths))\n",
    "len_counts = []\n",
    "for ln in unique_lengths:\n",
    "    len_counts.append((ln,np.sum(all_lengths == ln)))\n",
    "print(np.sum([t[1] for t in len_counts]))\n",
    "print(len(candidates2),len(candidates))\n",
    "print(len_counts)\n",
    "'''\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmatched = len(title_labels) - len(singletons)\n",
    "Adj_matrix = np.zeros((Nmatched,Nmatched))\n",
    "np.sum(Adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think that the first thing should be to create an adjacency matrix\n",
    "t0 = time.time()\n",
    "Nmatched = len(title_labels) - len(singletons)\n",
    "Adj_matrix = np.zeros((Nmatched,Nmatched))\n",
    "# create dictionary of indices\n",
    "indices = {}\n",
    "indices_reverse = {}\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    indices[candidate] = ix\n",
    "    indices_reverse[ix] = candidate\n",
    "# now cycle again through the matched NOS and populate the adjacency matrix\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    idx1 = ix\n",
    "    for k in candidates[candidate]:\n",
    "        # now this is a list of tuples, where the first element is the urn label\n",
    "        idx2 = indices[k[2]]\n",
    "        Adj_matrix[idx1,idx2] = 1\n",
    "\n",
    "print_elapsed(t0,'creating the adjacency matrix')\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(Adj_matrix[:50,:50])\n",
    "print('The highest degree in the adjacency matrix is: ', np.max(np.sum(Adj_matrix,axis=1)))\n",
    "print('The number of matched couples are: ', np.sum(np.sum(Adj_matrix, axis = 1)==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the NOS that were matched as similar\n",
    "t0 = time.time()\n",
    "matched_groups = []\n",
    "matched_indices = []\n",
    "for ix in range(Adj_matrix.shape[0]):\n",
    "    idx_used = []\n",
    "    # find the adjacent nodes\n",
    "    where_list = list(np.where(Adj_matrix[ix])[0])\n",
    "    where_list_cumul = []\n",
    "    # don't go into the rabbit hole of nodes with very high degree - \n",
    "    # also, don't use indices already matched\n",
    "    if len(where_list)<60000 and (ix not in matched_indices):\n",
    "        for ix2 in where_list:\n",
    "            # if the neighborhood has connections to indices that we haven't included yet, \n",
    "            # add them to the list to be analysed later\n",
    "            where_list_cumul += list(np.where(Adj_matrix[ix2])[0])\n",
    "            idx_used.append(ix2)\n",
    "            # grow the neighbourhood by adding the new connections\n",
    "            new_list = [t for t in where_list_cumul if t not in where_list]\n",
    "            # don't go into the rabbit hole of nodes with very high degree\n",
    "            if len(new_list)>60000:\n",
    "                break # this one tells it to break the inner for cycle - \n",
    "                # it goes to the next \"if (set(idx_used) == set(where_list))...\"\n",
    "            if len(new_list):\n",
    "                # if the length is zero it means there are no new connected nodes\n",
    "                where_list+=new_list\n",
    "        # if it has never gone into a rabbit hole then add the group just found\n",
    "        # if and only if the neighbourhood is self-contained, \n",
    "        # that is if the nodes for which we have collected the neighbours are the same\n",
    "        # that appeared in the combined neighbourhoods\n",
    "        if (set(idx_used) == set(where_list)) and (len(new_list)<6):\n",
    "            if len(new_list)>6:\n",
    "                print('got here after breaking for index', idx_used)\n",
    "            matched_groups.append(tuple(idx_used))\n",
    "            matched_indices += idx_used #[t for t in idx_used if t not in matched_indices]\n",
    "print_elapsed(t0, 'grouping the similar NOS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_lengths = [len(t) for t in matched_groups]\n",
    "group_lengths = Counter(group_lengths)\n",
    "group_lengths.most_common()\n",
    "print('Number of groups with different lengths')\n",
    "print(sorted(group_lengths.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in matched_groups:\n",
    "    if len(group)==40:\n",
    "        for t in group:\n",
    "            print(indices_reverse[t])\n",
    "            #print(candidates[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in matched_couples:\n",
    "#    if not (t in matched_groups):\n",
    "#        print(t)\n",
    "# show some of the groups\n",
    "groups_length = [len(t) for t in matched_groups]\n",
    "print('Number of groups of size up to 3: ', np.sum([t<4 for t in groups_length]))\n",
    "nbprint = 0\n",
    "for t in matched_groups:\n",
    "    if len(t)>4 and len(t)<7:\n",
    "        print(t)\n",
    "        for it in t:\n",
    "            print(np.where(Adj_matrix[it])[0])\n",
    "        nbprint+=1\n",
    "    if nbprint>2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all the groups are not overlapping: if it prints something is bad\n",
    "print('If something gets printed next, then the groups indentified have overlaps')\n",
    "matched_groups2 = []\n",
    "for it,t in enumerate(matched_groups):\n",
    "    if it>40000:\n",
    "        break\n",
    "    flag = 0\n",
    "    for t2 in matched_groups:\n",
    "        if t != t2:\n",
    "            if set(t).intersection(set(t2)):\n",
    "                print(t,t2)\n",
    "                flag += 1\n",
    "                #if (set(t)-set(t2)) and (set(t2) - set(t)):\n",
    "                #    print(t,t2)\n",
    "    if flag == 0:\n",
    "        matched_groups2.append(t)\n",
    "    else:\n",
    "        print(flag)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "N = Adj_matrix.shape[0]\n",
    "Nmatched= len(matched_indices)\n",
    "print('number of grouped NOS', N, 'number of matched NOS', Nmatched)\n",
    "# finally, check that the grouped and the non-grouped indices have a separate adjacency matrix\n",
    "Adj_matrix2 = copy.deepcopy(Adj_matrix)\n",
    "leftout_indices = list(set(range(N)) - set(matched_indices))\n",
    "neworder_indices = np.array(matched_indices + leftout_indices)\n",
    "# rearrange rows and columns of the adjacency matrix\n",
    "Adj_matrix2 = Adj_matrix2[:, neworder_indices][neworder_indices]\n",
    "print(''.join(['For the two parts of the Adjacency matrix to be separate, there has to',\n",
    "               ' be no edges between the two.']))\n",
    "print('The number of edges is: ')\n",
    "print(np.sum(Adj_matrix2[:Nmatched,Nmatched:]))\n",
    "# check the matrix is symmetric\n",
    "print('if 1 the matrix is symmetric: ', np.mean(Adj_matrix2 == Adj_matrix2.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leftout_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now redo the dictionary to save, with placing the self-contained groups first \n",
    "# and then the unpaired ones\n",
    "from collections import OrderedDict\n",
    "candidates_grouped = OrderedDict()\n",
    "candidates_paired = OrderedDict()\n",
    "\n",
    "pair_counter = 0\n",
    "group_counter = 0\n",
    "for ix, group in enumerate(matched_groups):\n",
    "    full_result = []\n",
    "    if len(group)==2:\n",
    "        for ig in group:\n",
    "            res = indices_reverse[ig]\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        candidates_paired['Pair {}'.format(pair_counter)] = full_result\n",
    "        pair_counter+=1\n",
    "for ix, group in enumerate(matched_groups):\n",
    "    full_result = []\n",
    "    if len(group)>2:\n",
    "        for ig in group:\n",
    "            res = indices_reverse[ig]\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        candidates_grouped['Group {}'.format(group_counter)] = full_result\n",
    "        group_counter+=1\n",
    "    \n",
    "#candidates_grouped['Finished with groups'] = '-'\n",
    "\n",
    "'''# now add the ones that weren't matched\n",
    "for ix in leftout_indices:\n",
    "    res = indices_reverse[ig]\n",
    "    candidates_grouped[res] = candidates[res]\n",
    "'''\n",
    "# try to save it, just to see what it looks like\n",
    "## OBSOLETE ONES\n",
    "#if SAVELSH and False:\n",
    "#    tmp = pd.DataFrame.from_dict(candidates_grouped, orient = 'index')\n",
    "#    print(tmp.columns)\n",
    "#    tmp.to_csv(output_dir +  '/LSH_results_grouped_no_pairs_{}_th{}.csv'.format(\n",
    "#                                                    qualifier,LSH_th))\n",
    "\n",
    "#if SAVELSH and False:\n",
    "#    pd.DataFrame.from_dict(candidates_paired, orient = 'index', columns = [\n",
    "#                                                    'NOS 1','NOS 2']).to_csv(output_dir + \n",
    "#                                  '/LSH_results_paired_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVELSH and False:\n",
    "    pd.DataFrame.from_dict(candidates, orient = 'index').to_csv(output_dir + '/LSH_results_{}_th{}.csv'.format(\n",
    "        qualifier,LSH_th))\n",
    "    with open(output_dir + '/Candidates_nos_{}_th{}.pickle'.format(qualifier,LSH_th),'wb') as f:\n",
    "        pickle.dump(candidates,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original near duplicates only for the big construction group\n",
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many duplicates per suite / per originating organisation\n",
    "# get the value count\n",
    "def add_text_to_hist_new(values, xvalues = None, addval = None, orient = 'vertical'):\n",
    "    if addval is None:\n",
    "        addval = .5 + np.floor(2*np.log(max(values)))\n",
    "    addx = -.2 if orient=='horizontal' else 0\n",
    "    for ix,i in enumerate(values):\n",
    "        if i>-1:\n",
    "            if not xvalues:\n",
    "                x = ix - .2\n",
    "            else:\n",
    "                x = xvalues[ix] +.02\n",
    "            if orient == 'vertical':\n",
    "                plt.text(i+addval, x, '{}'.format(i), fontsize = 14)\n",
    "            else:\n",
    "                plt.text(x, ix+addval,'{}'.format(i), fontsize = 14)\n",
    "                \n",
    "for col in ['One_suite', 'Developed By']:\n",
    "    duplicated_suites = df_nos.loc[list(candidates.keys())][col].value_counts()\n",
    "    all_suites = df_nos[col].value_counts()\n",
    "    if col == 'One_suite':\n",
    "        N = 70\n",
    "        fig =plt.figure(figsize = (11,18))\n",
    "        plt.ylabel('Suite',fontsize = 18)\n",
    "    else:\n",
    "        N = 32\n",
    "        fig = plt.figure(figsize = (7,12))\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Counts', fontsize = 18)\n",
    "    with sns.plotting_context('talk'):\n",
    "        duplicated_suites[:N][::-1].plot('barh', color = nesta_colours[3])\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_counts_for_duplicates.png'.format(col)))\n",
    "\n",
    "    # now divide by the sum (so we get fractions)\n",
    "    #duplicated_suites = duplicated_suites.map(lambda x: x/duplicated_suites.sum())\n",
    "    #all_suites = all_suites.map(lambda x: x/all_suites.sum())\n",
    "    # get the ratio of proportions with respect to the full distribution\n",
    "    suites_ratio = {}\n",
    "    abs_values = {}\n",
    "    for row in all_suites.index[:N]:\n",
    "        try:\n",
    "            suites_ratio[row] = duplicated_suites.loc[row]/all_suites.loc[row]\n",
    "            abs_values[row] = duplicated_suites.loc[row]\n",
    "        except:\n",
    "            suites_ratio[row] = 0\n",
    "            abs_values[row] = 0\n",
    "    suites_ratio = pd.DataFrame.from_dict(suites_ratio, orient = 'index', columns = ['ratio'])\n",
    "    ## order by decreasing ratios \n",
    "    #suites_ratio= suites_ratio.sort_values(by='ratio', ascending = False)\n",
    "    # plot the ratio\n",
    "    if col == 'One_suite':\n",
    "        fig=plt.figure(figsize = (12,18))\n",
    "        ix = range(N)\n",
    "        plt.ylabel('Suite',fontsize = 18)\n",
    "    else:\n",
    "        fig =plt.figure(figsize = (7,12))\n",
    "        ix = range(N)\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Proportion of NOS', fontsize = 18)\n",
    "    plt.xlim([0,1])\n",
    "    with sns.plotting_context('talk'):\n",
    "        suites_ratio['ratio'].iloc[ix][::-1].plot('barh', color = nesta_colours[3])\n",
    "    add_text_to_hist_new(list(abs_values.values())[::-1], xvalues = list(suites_ratio['ratio'].iloc[ix][::-1]), \n",
    "                         addval = -0.3, orient = 'horizontal')\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_ratios_for_duplicates_{}.png'.format(col,LSH_th)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute level of overlap for each group identified via the LSH algorithm\n",
    "- Use the average level of pairwise overlap between NOS in each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    res = float(intersection / union)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to assign keys and NOS to a dictionary - \n",
    "# so far they're in a list, so I can't easily find the MinHash corresponding to a given NOS\n",
    "\n",
    "content_dict = {}\n",
    "for ix,elem in enumerate(content):\n",
    "    content_dict[elem[0]] = elem[1]\n",
    "    \n",
    "print(content_dict[lsh.query(elem[1])[0]].jaccard(content_dict[lsh.query(elem[1])[0]]))\n",
    "print(lsh.query(elem[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to show the precision of the LSH algorithm. # Just get the pairs of near duplicates \n",
    "# and compute the Jaccard similarity using the inbuil function\n",
    "# Then check how many of these pairs actually do have Jaccard > LSH_th\n",
    "pair_similarities = []\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "    if len(result)==2:\n",
    "        pair_similarities.append(content_dict[result[0]].jaccard(content_dict[result[1]]))\n",
    "plt.hist(pair_similarities)\n",
    "a = (np.array(pair_similarities)<LSH_th).sum()\n",
    "b = len(pair_similarities)\n",
    "print(a, b, 1-a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average level of overlap within groups and compare to the average \n",
    "# number of common words\n",
    "# candidates_grouped_new = copy.deepcopy(candidates_grouped)\n",
    "#A = []\n",
    "#B = []\n",
    "all_j_values = []\n",
    "all_w_values = []\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_paired.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_paired[g]\n",
    "        j_values = []\n",
    "        g_suites = []\n",
    "        g_cat = []\n",
    "        w_values = []\n",
    "        \n",
    "        for t,ix1 in enumerate(G):\n",
    "            # create bag of words that retain duplicated words\n",
    "            if len(G)<50:\n",
    "                t1 = []\n",
    "                for tt in df_nos.loc[ix1[2]]['clean_full_text'].split():\n",
    "                    if tt not in t1:\n",
    "                        t1.append(tt)\n",
    "                    else:\n",
    "                        t1.append(tt+'1')\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values.append(content_dict[ix1[2]].jaccard(content_dict[ix2[2]]))\n",
    "                if len(G)<50:\n",
    "                    # create bag of words that retain duplicates\n",
    "                    t2 = []\n",
    "                    for tt in df_nos.loc[ix2[2]]['clean_full_text'].split():\n",
    "                        if tt not in t2:\n",
    "                            t2.append(tt)\n",
    "                        else:\n",
    "                            t2.append(tt + '1')\n",
    "                    w_values.append(len(set(t1).intersection(set(t2)))/ (len(set(t1)\n",
    "                                                                        )+len(set(t2))) *2)\n",
    "            g_suites.append(df_nos.loc[ix1[2]]['One_suite'])\n",
    "            g_cat.append(df_nos.loc[ix1[2]]['NOSCategory'])\n",
    "        \n",
    "        if len(G)<50:\n",
    "            # keep jaccard similarities\n",
    "            all_j_values += j_values\n",
    "            # keep proportion of common words\n",
    "            all_w_values += w_values\n",
    "        # append average jaccard similarity and number of unique suites in the group \n",
    "        # divided by groups size\n",
    "        tmp = [t == 'generic' for t in g_cat]\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "# print number of common words\n",
    "#print(np.mean(np.array(all_w_values)[np.array(all_j_values)>0.9]))\n",
    "#np.mean(all_w_values[np.array(all_j_values)>.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average level of overlap within groups bigger than two\n",
    "candidates_grouped_new = copy.deepcopy(candidates_grouped)\n",
    "candidates_grouped_string = {}\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_grouped.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_grouped[g]\n",
    "        j_values = []\n",
    "        g_suites = []\n",
    "        g_cat = []\n",
    "        w_values = []\n",
    "        nos_strings = []\n",
    "        for t,ix1 in enumerate(G):\n",
    "            urn = ix1[0]\n",
    "            title = ix1[1]\n",
    "            nos_file = ix1[2]\n",
    "            validity = df_nos.loc[nos_file]['Validity']\n",
    "            status = df_nos.loc[nos_file]['Status']\n",
    "            nos_strings.append('[{}] \\\"{}\\\" (\\\"{}\\\", {}, {})'.format(urn, title.capitalize(), nos_file, \n",
    "                                                                 validity.capitalize(),status.capitalize()))\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values.append(content_dict[ix1[2]].jaccard(content_dict[ix2[2]]))\n",
    "        candidates_grouped_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "        candidates_grouped_string[g] = [np.around(np.mean(j_values),3)] + nos_strings\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('First part done in {:.4f}s'.format(time.time()-t0))\n",
    "\n",
    "#candidates_grouped_new.pop('Finished with groups')\n",
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(candidates_grouped_string, orient = 'index').rename(columns = {0:'Avg group similarity'})\n",
    "# rename the columns\n",
    "rename_dict= {}\n",
    "for ii in range(len(tmp.columns)):\n",
    "    rename_dict[ii] = 'NOS {}'.format(ii)\n",
    "tmp = tmp.rename(columns = rename_dict)\n",
    "# sort by descending values of the average similarity\n",
    "tmp = tmp.sort_values(by = 'Avg group similarity', ascending = False)\n",
    "# rename the rows after the sorting\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "rename_dict = {}\n",
    "for ii in range(len(tmp)):\n",
    "    rename_dict[ii] = 'Group {}'.format(ii)\n",
    "tmp = tmp.rename(index = rename_dict)\n",
    "\n",
    "# OFFICIAL ONE\n",
    "if SAVELSH and False:\n",
    "#   tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns= {0:'Avg group similarity'})\n",
    "    tmp.to_csv(output_dir + '/LSH_results_grouped_no_pairs_with_score_{}_th{}.csv'.format(qualifier,LSH_th))\n",
    "print('All done in {:.4f}s'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split large construction group into communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_group =candidates_grouped_new['Group 0']\n",
    "long_group = long_group[1:]\n",
    "\n",
    "\n",
    "'''# recompute pairwise Jaccard similarities\n",
    "j_values = np.zeros((len(long_group),len(long_group)))\n",
    "#g_suites = []\n",
    "#g_cat = []\n",
    "#w_values = []\n",
    "#nos_strings = []\n",
    "for t,ix1 in enumerate(long_group):\n",
    "    urn = ix1[0]\n",
    "    title = ix1[1]\n",
    "    nos_file = ix1[2]\n",
    "    #validity = df_nos.loc[nos_file]['Validity']\n",
    "    #status = df_nos.loc[nos_file]['Status']\n",
    "    #nos_strings.append('[{}] \\\"{}\\\" (\\\"{}\\\", {}, {})'.format(urn, title.capitalize(), nos_file, \n",
    "    #                                                     validity.capitalize(),status.capitalize()))\n",
    "    for _t,ix2 in enumerate(long_group[t+1:]):\n",
    "        t2 = t+_t+1\n",
    "        j_values[t,t2]  =content_dict[ix1[2]].jaccard(content_dict[ix2[2]])\n",
    "j_values = j_values + j_values.T + np.identity(len(long_group))'''\n",
    "print(long_group[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original near-duplicates for the very big construction group\n",
    "long_group_df = {}\n",
    "for doc in long_group:\n",
    "    doc_pdf = doc[2]\n",
    "    near_duplicate_docs = candidates[doc_pdf]\n",
    "    only_duplicate_docs = []\n",
    "    for dupl_doc in near_duplicate_docs:\n",
    "        if doc_pdf != dupl_doc[2]:\n",
    "            only_duplicate_docs.append(dupl_doc)\n",
    "    dupl_key = f'[{doc[0]}] \"{doc[1]}\" (\"{doc[2]}\")'\n",
    "    long_group_df[dupl_key] = []\n",
    "    for dupl_doc in only_duplicate_docs:\n",
    "        long_group_df[dupl_key].append(f'[{dupl_doc[0]}] \"{dupl_doc[1]}\" (\"{dupl_doc[2]}\")')\n",
    "    #break\n",
    "\n",
    "long_group_df = pd.DataFrame.from_dict(long_group_df, orient = 'index')\n",
    "long_group_df = long_group_df.rename(columns = {t: f'candidate duplicate {t}' for t in long_group_df.columns})\n",
    "\n",
    "if SAVELSH or True:\n",
    "    long_group_df.to_csv('/Users/stefgarasto/Google Drive/Documents/scripts/NOS/results/nlp_analysis/'+\n",
    "                        'construction_group_individual_duplicates.csv', encoding = 'utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community detection algorithm\n",
    "t0 = time.time()\n",
    "partition_lenghts = []\n",
    "for th in [.7,.75,.8,.85]:\n",
    "    graph_full = nx.from_numpy_matrix(j_values*(j_values>th))\n",
    "    #nx.draw(graph_full)\n",
    "    C = list(community.greedy_modularity_communities(graph_full))\n",
    "    print(f'{time.time()-t0:.2f}')\n",
    "    print('Is it a partition? ', community.is_partition(graph_full,C))\n",
    "    partition_lenghts.append([len(c) for c in C])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example group\n",
    "c= C[0]\n",
    "for t in c:\n",
    "    print(long_group[t+1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average level of overlap within pairs\n",
    "candidates_paired_new = copy.deepcopy(candidates_paired)\n",
    "candidates_paired_string = {}\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_paired.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_paired[g]\n",
    "        j_values = []\n",
    "        nos_strings = []\n",
    "        for t,ix1 in enumerate(G):\n",
    "            urn = ix1[0]\n",
    "            title = ix1[1]\n",
    "            nos_file = ix1[2]\n",
    "            validity = df_nos.loc[nos_file]['Validity']\n",
    "            status = df_nos.loc[nos_file]['Status']\n",
    "            nos_strings.append('[{}] \\\"{}\\\" (\\\"{}\\\", {}, {})'.format(urn, title.capitalize(), nos_file, \n",
    "                                                                 validity.capitalize(), status.capitalize()))\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values = [content_dict[ix1[2]].jaccard(content_dict[ix2[2]])]\n",
    "                if ig%30>-1:\n",
    "                    if ix1[2]=='mpqmg31.pdf':\n",
    "                        print(j_values,content_dict[ix1[2]] )\n",
    "                    #print(ix1[2],ix2[2])\n",
    "        candidates_paired_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "        candidates_paired_string[g] = [np.around(np.mean(j_values),3)] + nos_strings\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "\n",
    "#candidates_grouped_new.pop('Finished with groups')\n",
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(candidates_paired_string, orient = 'index').rename(columns = {0:'Avg group similarity'})\n",
    "# rename the columns\n",
    "rename_dict= {}\n",
    "for ii in range(len(tmp.columns)):\n",
    "    rename_dict[ii] = 'NOS {}'.format(ii)\n",
    "tmp = tmp.rename(columns = rename_dict)\n",
    "# sort by descending values of the average similarity\n",
    "tmp = tmp.sort_values(by = 'Avg group similarity', ascending = False)\n",
    "# rename the rows after the sorting\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "rename_dict = {}\n",
    "for ii in range(len(tmp)):\n",
    "    rename_dict[ii] = 'Pair {}'.format(ii)\n",
    "tmp = tmp.rename(index = rename_dict)\n",
    "if SAVELSH:\n",
    "#   tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns= {0:'Avg group similarity'})\n",
    "    tmp.to_csv(output_dir + '/LSH_results_pairs_with_score_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('90%: ', np.mean(np.array(all_w_values)[np.array(all_j_values)>0.9]))\n",
    "print('80%: ',np.array(all_w_values)[(np.array(all_j_values)>0.8) & (np.array(all_j_values)<0.9)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nos['clean_full_text'].map(lambda x: len(x.split())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_nos = df_nos.loc[list(candidates.keys())]\n",
    "duplicated_cos = duplicated_nos[duplicated_nos['Developed By']=='construction skills']\n",
    "duplicated_nos = duplicated_nos[duplicated_nos['Developed By']=='semta']\n",
    "tmp = list(duplicated_nos.index)\n",
    "print([t for t in tmp if 'l' in t[-7:]])\n",
    "print(duplicated_nos.columns)\n",
    "print(duplicated_nos['Version_number'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# to be old and new updated nos, it would have to be the following conditions:\n",
    "1. group of two NOS\n",
    "2. two different versions (1 and higher)\n",
    "3. same or very similar suite (that is, the same aside from numbers)\n",
    "'''\n",
    "semta_versions = []\n",
    "mixed_version = []\n",
    "semta_suites = []\n",
    "semta_titles = []\n",
    "for g in candidates_paired_new:\n",
    "    group = candidates_paired_new[g]\n",
    "    if group[1][0][:3]:# == 'sem':\n",
    "        #print(len(group))\n",
    "        semta_versions.append([])\n",
    "        semta_suites.append([])\n",
    "        semta_titles.append([])\n",
    "        if len(group)==3:\n",
    "            # only take groups of pairs\n",
    "            for ix in range(1,len(group)):\n",
    "                #print(duplicated_nos.loc[group[ix][2]]['Version_number'])\n",
    "                try:\n",
    "                    semta_versions[-1].append(duplicated_nos.loc[group[ix][2]][\n",
    "                        'Version_number'])\n",
    "                    semta_suites[-1].append(duplicated_nos.loc[group[ix][2]]['One_suite'])\n",
    "                    semta_titles[-1].append(group[ix][1])\n",
    "                except:\n",
    "                    1\n",
    "            if (1.0 in semta_versions[-1]) & ((2.0 in semta_versions[-1]) or (\n",
    "                                                    3.0 in semta_versions[-1])):\n",
    "                # check the suites\n",
    "                suite1 = ''.join([t for t in semta_suites[-1][0] if not t.isdigit()])\n",
    "                suite2 = ''.join([t for t in semta_suites[-1][1] if not t.isdigit()])\n",
    "                title1 = semta_titles[-1][0]\n",
    "                title2 = semta_titles[-1][1]\n",
    "                out = process.extract(title1, [title2])\n",
    "                out2 = process.extract(suite1, [suite2])\n",
    "                # assume two titles are similar if the fuzzy matching is higher than 90\n",
    "                if out[0][1]>79:\n",
    "                    # if we also require the two suite names to have a match higher than 90\n",
    "                    if out2[0][1]>79:\n",
    "                        mixed_version.append(group)\n",
    "                        print(semta_suites[-1][0],semta_suites[-1][1])\n",
    "                        \n",
    "print(semta_versions)\n",
    "len(mixed_version), len(semta_versions), len(duplicated_nos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction skills should be responsible for the very big group\n",
    "print(len(duplicated_cos))\n",
    "for g in candidates_grouped_new:\n",
    "    group = candidates_grouped_new[g]\n",
    "    if group[1][0][:3] == 'cos':\n",
    "        if len(group)>200:\n",
    "            tmp = [group[ix][0][:3] for ix in range(1,len(group))]\n",
    "            print(sum([t=='cos' for t in tmp]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from xml.dom.minidom import parseString\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_soc(x):\n",
    "    if isinstance(x,list):\n",
    "        try:\n",
    "            return x[0]\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def get_one_occupation(x):\n",
    "    if isinstance(x,list):\n",
    "        y= ''.join(x)\n",
    "    else:\n",
    "        y= x\n",
    "    if isinstance(y,str):\n",
    "        return y.split(';')\n",
    "    else:\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load duplicated NOS\n",
    "if False:\n",
    "    lshduplicate_file = ''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/',\n",
    "         'LSH_results_grouped_with_score_postjoining_final_no_dropped_th0.8.csv'])\n",
    "    lshduplicate_nos = pd.read_csv(lshduplicate_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #%% process the LSH duplicates to assign group ID to NOS\n",
    "    # NOTE: these are all the groups, not just engineering NOS\n",
    "    def split_nos_in_groups(x):\n",
    "        if isinstance(x,str):\n",
    "            x = [t.strip() for t in x.replace(')','').replace('(','').replace('\\'','').split(',')]\n",
    "            return x[-1]\n",
    "        else:\n",
    "            return x\n",
    "    tmp0 = lshduplicate_nos.applymap(split_nos_in_groups)\n",
    "    df_nos_lsh = tmp0[['Unnamed: 0','Avg group similarity','1']]\n",
    "    t0 = time.time()\n",
    "    for i in range(2, len(lshduplicate_nos.columns)-2):\n",
    "        tmp = tmp0[['Unnamed: 0','Avg group similarity','{}'.format(i)]].rename(columns = {'{}'.format(i):'1'})\n",
    "        tmp = tmp[tmp['1'].notna()]\n",
    "        df_nos_lsh = pd.concat([df_nos_lsh, tmp])\n",
    "    print_elapsed(t0, 'assigning LSH groups to NOS')\n",
    "\n",
    "    #%% Join LSH groups and transferable NOS\n",
    "    df_nos2 = df_nos.join(df_nos_lsh.rename(columns = {'Unnamed: 0': 'lsh_group', \n",
    "                                                      'Avg group similarity': 'lsh_simil',\n",
    "                                                      '1':'index'}).set_index('index'), how = 'left')\n",
    "    df_nos2['lsh_group'].mask(df_nos2['lsh_group'].isnull(), 'na', inplace= True)\n",
    "    # remove LSH groups with a low overall similarity\n",
    "    #th_lsh = 0.75\n",
    "    #df_nos2['lsh_group'].mask(df_nos2['lsh_simil']<th_lsh, 'na', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the largest group\n",
    "if False:\n",
    "    df_group = df_nos2.groupby('lsh_group')[['Title','URN']].agg(len)\n",
    "    largest_group_name = df_group.sort_values('Title').index[-2]\n",
    "    # minus 2 because we don't want the 'na's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the largest group and the relevant columns, then order it according to G\n",
    "if False:\n",
    "    metadata_cols = ['NOS Title', 'URN',\n",
    "           'Originating_organisation','Date_approved_year',\n",
    "           'Clean Ind Review Year', 'Version_number', 'Developed By', 'Validity',\n",
    "           'Keywords', 'Clean SOC Code', 'NOS Document Status', 'NOSCategory', 'Occupations',\n",
    "           'One_suite']\n",
    "    largest_group = df_nos2[df_nos2['lsh_group']==largest_group_name][metadata_cols]\n",
    "    # what's missing is to collect the Jaccard similarity among each pair of NOS in this largest group\n",
    "    # get the old name\n",
    "    for g in candidates_grouped:\n",
    "        group = candidates_grouped[g]\n",
    "        if len(group)>=len(largest_group):\n",
    "            print(g,len(group),len(largest_group))\n",
    "            break\n",
    "    G = [standard for standard in group if standard[2] in largest_group.index]\n",
    "\n",
    "    largest_group = largest_group.join(pd.DataFrame(np.arange(898), index = [standard[2] for standard in G],\n",
    "                                                    columns = ['order']))\n",
    "    largest_group = largest_group.sort_values(by = 'order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_viz = '/Users/stefgarasto/Google Drive/Documents/results/NOS/data_viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all similarities\n",
    "if False:\n",
    "    j_values = []\n",
    "    g_suites = []\n",
    "    g_cat = []\n",
    "    w_values = []\n",
    "    j_values_matrix = np.zeros((len(G),len(G)))\n",
    "    t0 = time.time()\n",
    "    for t1,ix1 in enumerate(G):\n",
    "        j_values_matrix[t1,t1] = .5\n",
    "        for t2,ix2 in enumerate(G):\n",
    "            if t2<=t1:\n",
    "                continue\n",
    "            tmp_simil = content_dict[ix1[2]].jaccard(content_dict[ix2[2]])\n",
    "            j_values.append(tmp_simil)\n",
    "            j_values_matrix[t1,t2] = tmp_simil\n",
    "        #candidates_grouped_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "    print_elapsed(t0, 'computing similarities')\n",
    "    # the matrix so far is TRIU - add the lower triangular bit to make it full\n",
    "    j_values_matrix = j_values_matrix + j_values_matrix.T\n",
    "    SAVEDATALSH = False\n",
    "    if SAVEDATALSH:\n",
    "        with open(os.path.join(output_dir_viz, \n",
    "                'data_from_largest_lsh_group_similarities.pickle'), 'wb') as f:\n",
    "            pickle.dump((largest_group, j_values_matrix), f)\n",
    "    #print(time.time() - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute a maximum spanning tree and use Gephi to get a good node layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    t0 = time.time()\n",
    "    # create graph\n",
    "    graph_full = nx.from_numpy_matrix(j_values_matrix)\n",
    "    # assign labels\n",
    "    new_labels = dict(zip(range(len(G)),largest_group.index))\n",
    "    graph_full = nx.relabel_nodes(graph_full, new_labels)\n",
    "    print_elapsed(t0,'creating and relabelling graph')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create maximum spanning tree\n",
    "if False:\n",
    "    t0 = time.time()\n",
    "    graph_tree = nx.maximum_spanning_tree(graph_full)\n",
    "    print_elapsed(t0,'getting max spanning tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save as gexf\n",
    "if False:\n",
    "    nx.write_gexf(graph_tree, os.path.join(output_dir_viz, \n",
    "                'max_tree_largest_group.gexf'))\n",
    "\n",
    "    #pos = nx.random_layout(graph_full)\n",
    "    #nx.draw_networkx_edge_labels(graph_full, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proceed_after_gephi = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only AFTER changing the layout in Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not proceed_after_gephi:\n",
    "    stop\n",
    "    \n",
    "#node = tree.getElementsByTagName('node')[0].getElementsByTagName('attvalue')\n",
    "#print(node[0].attributes.items())\n",
    "#print(nodes.getElementsByTagName('attvalue')[0].attributes.items())#getElementsByTagName('attvalue'))\n",
    "#print(nodes.getElementsByTagName('attvalue')[2].attributes.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos = nx.planar_layout(graph_tree)\n",
    "#nx.draw(graph_tree, pos)\n",
    "# read the new gexf layout\n",
    "#graph_tree2 = nx.read_gexf(os.path.join(output_dir_viz,'largest_group_tree_layout_with_force_atlas2.gexf'))\n",
    "if False:\n",
    "    pos_dict ={}\n",
    "    # pase the gmxf file and save the position\n",
    "\n",
    "    with open(os.path.join(output_dir_viz,'largest_group_tree_layout_with_force_atlas2_2.gexf'),\n",
    "              'r') as f:\n",
    "        tree = parseString(f.read())\n",
    "\n",
    "    nodes = tree.getElementsByTagName('node')\n",
    "    for node in nodes:\n",
    "        k =node.getAttribute('id')\n",
    "        pos_dict[k] = {}\n",
    "        # to get the degree\n",
    "        pos_tree= node.getElementsByTagName('attvalue')\n",
    "        for value in pos_tree:\n",
    "            pos_values = value.attributes.items()\n",
    "            pos_dict[k][pos_values[0][1]] = pos_values[1][1]\n",
    "\n",
    "        # to get the position\n",
    "        pos_tree = node.getElementsByTagName('viz:position')\n",
    "        pos_values = pos_tree[0].attributes.items()\n",
    "        for att in pos_values:\n",
    "            pos_dict[k][att[0]] = float(att[1])\n",
    "\n",
    "        # to get the size\n",
    "        pos_tree = node.getElementsByTagName('viz:size')\n",
    "        pos_dict[k]['size']= pos_tree[0].attributes.items()[0][1]\n",
    "\n",
    "        # to get the colour (synonym of community I think)\n",
    "        pos_tree = node.getElementsByTagName('viz:color')\n",
    "        pos_values = pos_tree[0].attributes.items()\n",
    "        pos_dict[k]['color']= 'rbg({},{},{})'.format(pos_values[0][1],pos_values[1][1],pos_values[2][1])\n",
    "\n",
    "    # turn the dict into a dataframe\n",
    "    pos_df = pd.DataFrame.from_dict(pos_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pos_df.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with the original dataframe\n",
    "if False:\n",
    "    largest_group_with_pos = largest_group.join(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    largest_group_with_pos.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract edges information\n",
    "if False:\n",
    "    with open(os.path.join(output_dir_viz,'largest_group_tree_layout_with_force_atlas2_2.gexf'),\n",
    "              'r') as f:\n",
    "        tree = parseString(f.read())\n",
    "    edges_weights = pd.read_csv(''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/',\n",
    "                                         'data_viz/largest_group_tree_layout_with_force_atlas_weights2_3.csv']))\n",
    "    edges = tree.getElementsByTagName('edge')\n",
    "    edges_dict = {}\n",
    "    #{\"source\":\"n6\",\"target\":\"n51\",\"id\":\"212\",\"attributes\":{},\"color\":\"rgb(36,129,178)\",\"size\":0.06302209943532944}\n",
    "    for edge in edges:\n",
    "        edge_values = edge.attributes.items()\n",
    "        k = edge_values[0][1] #k=id\n",
    "        edges_dict[k] = {}\n",
    "        for val in edge_values[1:]:\n",
    "            if val[0] == 'weight':\n",
    "                edges_dict[k]['size'] = val[1]\n",
    "            else:\n",
    "                edges_dict[k]['old_'+val[0]] = val[1]\n",
    "        # to get the colour (synonym of community I think)\n",
    "        pos_tree = edge.getElementsByTagName('viz:color')\n",
    "        pos_values = pos_tree[0].attributes.items()\n",
    "        edges_dict[k]['color'] = 'rbg({},{},{})'.format(pos_values[0][1],pos_values[1][1],pos_values[2][1])\n",
    "        # turn source/target into nID\n",
    "        # get size = weight from pandas csv\n",
    "        if 'size' not in edges_dict[k]:\n",
    "            tmp = edges_weights[edges_weights['Id'] == int(k)]['Weight'].values[0]\n",
    "            edges_dict[k]['size'] = tmp\n",
    "    print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template to copy\n",
    "if False:\n",
    "    file_name= ''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/data_viz/',\n",
    "                       'data_supply_chain_suite.json'])\n",
    "    with open(file_name,'r') as f:\n",
    "        supply_chain_json = json.load(f)\n",
    "    print(supply_chain_json['nodes'][0].keys())#['attributes'])\n",
    "    print(supply_chain_json['nodes'][0]['attributes'])\n",
    "    for ix in range(2):\n",
    "        print(supply_chain_json['nodes'][0]['size'])\n",
    "    print(supply_chain_json['nodes'][0]['color'])\n",
    "    A = [t['color'] for t in supply_chain_json['nodes']]\n",
    "    print('colors for nodes' ,collections.Counter(A))\n",
    "    print()\n",
    "    A = [t['color'] for t in supply_chain_json['edges']]\n",
    "    print('colors for edges' ,collections.Counter(A))\n",
    "    print()\n",
    "    A = [t['size'] for t in supply_chain_json['edges']]\n",
    "    print('nb of sizes for edges' , len(collections.Counter(A)))\n",
    "    print(supply_chain_json['edges'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_list(x):\n",
    "    #all_keywords = []\n",
    "    #x = df_line['Keywords']\n",
    "    if isinstance(x, list):\n",
    "        # I think that ik can be a collection of words separated by \";\" or \",\"\n",
    "        ik_elems0 = ' '.join([elem for elem in x if not elem.isdigit()])\n",
    "        ik_elems0 = ik_elems0.replace('-', ' ').replace(':','').replace(',',';')\n",
    "        ik_elems0 = ik_elems0.replace('(','').replace(')','')\n",
    "        ik_elems0 = ik_elems0.split(';')\n",
    "        # remove extra spaces and make lowercase\n",
    "        ik_elems0 = [elem.strip().lower() for elem in ik_elems0]\n",
    "        ik_elems0 = [elem for elem in ik_elems0 if len(elem)]\n",
    "        ik_elems = []\n",
    "        for ik_elem in ik_elems0:\n",
    "            ik_elem0 = ' '.join([elem for elem in ik_elem.split() if\n",
    "                  (not elem.isdigit()) & (elem not in stopwords) & len(elem)])\n",
    "            if len(ik_elem0):\n",
    "                ik_elems.append(ik_elem0)\n",
    "                    #print(ik_elems)\n",
    "        return [elem.strip() for elem in ik_elems if len(elem)]\n",
    "    elif isinstance(x,str):\n",
    "        #ik_elems = re.findall(r\"[\\w']+\", df.loc[ix].replace('-',''))\n",
    "        ik_elems = x.replace('-', ' ').replace(',',';')\n",
    "        ik_elems = ik_elems.replace('(','').replace(')','').split(';')\n",
    "        if len(ik_elems)==1:\n",
    "            # lacking proper separators - have to use spaces\n",
    "            ik_elems = ik_elems[0].split()\n",
    "        # remove extra spaces\n",
    "        ik_elems = [elem.strip().lower().replace('\\n','') for elem in ik_elems]\n",
    "        # remove digits\n",
    "        ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "        ik_elems = [elem for elem in ik_elems if len(elem)>1]\n",
    "        return [elem.strip() for elem in ik_elems if len(elem)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a dictionary\n",
    "if False:\n",
    "    attributes_dict= {'year_developed': 'Date_approved_year', \n",
    "                      'year': 'Date_approved_year', #,'keywords': 'Keywords' #'Clean Ind Review Year', \n",
    "                      'organisation': 'Developed By', 'title': 'NOS Title'}\n",
    "    output_dict = {'nodes':[], 'edges': []}\n",
    "    nodes_id_dict = {}\n",
    "    for ix,node in enumerate(largest_group_with_pos.index):\n",
    "        tmp_dict = {}\n",
    "        row = largest_group_with_pos.loc[node]\n",
    "        tmp_dict['label'] = row['URN'] #node\n",
    "        for label in ['x','y']:\n",
    "            tmp_dict[label] = row[label]\n",
    "        tmp_dict['id'] = 'n{}'.format(ix)\n",
    "        tmp_dict['degree'] = row['degree']\n",
    "        tmp_dict['color'] = row['color']#'rgb(36,129,178)'\n",
    "        tmp_dict['size']= 10.0\n",
    "        tmp_dict['attributes'] = {}\n",
    "        tmp_dict['attributes']['id'] = '{}'.format(ix)\n",
    "        for attribute in attributes_dict.keys():\n",
    "            tmp_dict['attributes'][attribute] = row[attributes_dict[attribute]]\n",
    "            if isinstance(tmp_dict['attributes'][attribute], float):\n",
    "                if np.isnan(tmp_dict['attributes'][attribute]):\n",
    "                    tmp_dict['attributes'][attribute] = ''\n",
    "        # extract and add keywords\n",
    "        tmp_dict['keywords'] = get_keywords_list(row['Keywords'])\n",
    "        if not tmp_dict['keywords']:\n",
    "            print(tmp_dict['keywords'],type(tmp_dict['keywords']))\n",
    "        #try:\n",
    "        #    tmp_dict['attributes']['year'] = str(int(tmp_dict['attributes']['year']))\n",
    "        #except:\n",
    "        #    tmp_dict['attributes']['year'] = ''\n",
    "        tmp_dict['attributes']['year_developed'] = tmp_dict['attributes']['year']\n",
    "        #try:\n",
    "        #    tmp_dict['attributes']['year_developed'] = str(int(tmp_dict['attributes'\n",
    "        #                                                           ]['year_developed']))\n",
    "        #except:\n",
    "        #    pass\n",
    "        tmp_dict['attributes']['file_name'] = node\n",
    "        output_dict['nodes'].append(tmp_dict)\n",
    "        # save the relationship label to id created here\n",
    "        nodes_id_dict[node] = tmp_dict['id']\n",
    "\n",
    "    # now add the edges\n",
    "    for edge_id in edges_dict.keys():\n",
    "        edge = copy.deepcopy(edges_dict[edge_id])\n",
    "        edge['id'] = edge_id\n",
    "        # transform the source/target from title to id\n",
    "        edge['source'] = nodes_id_dict[edge['old_source']]\n",
    "        edge['target'] = nodes_id_dict[edge['old_target']]\n",
    "        edge['attributes'] = {}\n",
    "        edge.pop('old_source')\n",
    "        edge.pop('old_target')\n",
    "        output_dict['edges'].append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(output_dict['nodes'][:5])\n",
    "    print()\n",
    "    print(output_dict['edges'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save the json\n",
    "SAVEJSON= False\n",
    "if SAVEJSON:\n",
    "    with open(os.path.join(output_dir_viz, 'max_tree_largest_group.json'), 'w') as fp:\n",
    "        json.dump(output_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#largest_group_with_pos['Clean Ind Review Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create clickable list of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs:\n",
    "1. Test the transfer Mac to Windows with the new strategy.\n",
    "\n",
    "    a. try removing asterisks from file names and using jsons instead of .txts    \n",
    "    b. try removing asterisks from file names while still using \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of duplicates\n",
    "duplicates_names = []\n",
    "duplicates_info = {}\n",
    "for pair_id in candidates_paired:\n",
    "    pair = candidates_paired[pair_id]\n",
    "    # get names\n",
    "    for standard in pair:\n",
    "        #print(standard)\n",
    "        duplicates_names.append(standard[2])\n",
    "        duplicates_info[standard[2]] = {}\n",
    "        duplicates_info[standard[2]]['title'] = standard[1]\n",
    "        duplicates_info[standard[2]]['URN'] = standard[0]\n",
    "        duplicates_info[standard[2]]['suite'] = df_nos.loc[standard[2]]['One_suite']\n",
    "        duplicates_info[standard[2]]['pair'] = 1\n",
    "        duplicates_info[standard[2]]['pair_id'] = pair_id\n",
    "        duplicates_info[standard[2]]['category'] = df_nos.loc[standard[2]]['NOSCategory']\n",
    "        duplicates_info[standard[2]]['validity'] = df_nos.loc[standard[2]]['Validity']\n",
    "        duplicates_info[standard[2]]['status'] = df_nos.loc[standard[2]]['Status']\n",
    "print('Done with pairs')\n",
    "for pair_id in candidates_grouped:\n",
    "    pair = candidates_grouped[pair_id]\n",
    "    # get names\n",
    "    for standard in pair:\n",
    "        #print(standard)\n",
    "        duplicates_names.append(standard[2])\n",
    "        duplicates_info[standard[2]] = {}\n",
    "        duplicates_info[standard[2]]['title'] = df_nos.loc[standard[2]]['NOS Title'] #standard[1]\n",
    "        duplicates_info[standard[2]]['URN'] = standard[0]\n",
    "        duplicates_info[standard[2]]['suite'] = df_nos.loc[standard[2]]['One_suite']\n",
    "        duplicates_info[standard[2]]['pair'] = 0\n",
    "        duplicates_info[standard[2]]['pair_id'] = pair_id\n",
    "        duplicates_info[standard[2]]['category'] = df_nos.loc[standard[2]]['NOSCategory']\n",
    "        duplicates_info[standard[2]]['validity'] = df_nos.loc[standard[2]]['Validity']\n",
    "        duplicates_info[standard[2]]['status'] = df_nos.loc[standard[2]]['Status']\n",
    "print('Done')\n",
    "\n",
    "# eliminate any nan:\n",
    "for k in duplicates_info:\n",
    "    for inner_k in duplicates_info[k]:\n",
    "        if isinstance(duplicates_info[k][inner_k],float):\n",
    "            if np.isnan(duplicates_info[k][inner_k]):\n",
    "                duplicates_info[k][inner_k] = 'NA'\n",
    "\n",
    "print('Done with removing nans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df = pd.DataFrame.from_dict(duplicates_info, orient = 'index')\n",
    "# group them by suite\n",
    "duplicates_groupby = duplicates_df.groupby('suite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_viz2 = '/Users/stefgarasto/Local-Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE THE SINGLE FILES AND THE LINKS SECTION OF THE MAIN HTML\n",
    "\n",
    "# for each suite sort them alphabetically\n",
    "txt_file = os.path.join(output_dir_viz2,'near_duplicates_list/duplicates_list_for_html_{}.txt'.format(\n",
    "                                                                                file_type))\n",
    "file_type = 'txt'\n",
    "file_type_dirs = {'txt': 'duplicates_txts', 'json': 'duplicates_jsons'}\n",
    "s_dir = file_type_dirs[file_type]\n",
    "sub_dict= {}\n",
    "with open(txt_file,'w') as f:\n",
    "    for name,group in duplicates_groupby:\n",
    "        #print(group)\n",
    "        f.write('<p> <h3> Suite: \\\"{}\\\" </h3> </p>'.format(name.capitalize()))\n",
    "        f.write('\\n')\n",
    "        group = group.sort_values('suite')\n",
    "        for s_name,standard in group.iterrows():\n",
    "            # Links section in the main html\n",
    "            \n",
    "            # version that write a file for each duplicate\n",
    "            #s_name_json = os.path.splitext(s_name.replace(' ','+'))[0]\n",
    "            #f.write('<p> [<a href=\"{}/{}.{}\" target=\"_blank\">\\\"{}\\\"</a>] \\\"{}\\\". </p>'.format(\n",
    "            #    s_dir,s_name_json,file_type,standard['URN'],standard['title'].capitalize()))\n",
    "            \n",
    "            # version that writes a file for each group of duplicates\n",
    "            s_name_json = duplicates_df.loc[s_name]['pair_id']\n",
    "            #s_done_json = []\n",
    "            f.write('<p> [<a href=\"{}/{}.{}\" target=\"_blank\">\\\"{}\\\"</a>] \\\"{}\\\". </p>'.format(\n",
    "                s_dir,s_name_json,file_type,standard['URN'],standard['title'].capitalize()))\n",
    "            f.write('\\n')\n",
    "            # get all duplicates and build the json/txt file\n",
    "            sub_file_name = os.path.join(output_dir_viz2,'near_duplicates_list/{}/{}.{}'.format(\n",
    "                                        s_dir,s_name_json,file_type))\n",
    "            \n",
    "            if standard['pair']:\n",
    "                others= candidates_paired[standard['pair_id']]\n",
    "            else:\n",
    "                others = candidates_grouped[standard['pair_id']]\n",
    "            # write the individual files\n",
    "            with open(sub_file_name,'w') as subf:\n",
    "                if file_type == 'txt':\n",
    "                    subf.write('Potential near duplicates ({} in total). '.format(len(others)))\n",
    "                    subf.write('\\n')\n",
    "                    subf.write('\\n')\n",
    "                elif file_type == 'json':\n",
    "                    main_json_key = 'Potential near duplicates ({} in total)'.format(len(others))\n",
    "                    sub_dict= {main_json_key: {}}\n",
    "                else:\n",
    "                    raise(ValueError)\n",
    "                for ix,other in enumerate(others):\n",
    "                    if file_type == 'txt':\n",
    "                        try:\n",
    "                            subf.write('{}. [\\\"{}\\\"] \\\"{}\\\". (\\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\")'.format(\n",
    "                                ix+1,other[0],other[1].capitalize(), \n",
    "                                duplicates_info[other[2]]['suite'].capitalize() ,other[2], \n",
    "                                duplicates_info[other[2]]['validity'].capitalize(), \n",
    "                                duplicates_info[other[2]]['status'].capitalize(),\n",
    "                                duplicates_info[other[2]]['category'].capitalize()))\n",
    "                        except:\n",
    "                            subf.write('{}. [\\\"{}\\\"] \\\"{}\\\". (\\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\")'.format(\n",
    "                                ix+1,other[0],other[1].capitalize(), name.capitalize() ,other[2], \n",
    "                                duplicates_info[other[2]]['validity'].capitalize(), \n",
    "                                duplicates_info[other[2]]['status'].capitalize(),\n",
    "                                duplicates_info[other[2]]['category'].capitalize()))\n",
    "                        subf.write('\\n')\n",
    "                    elif file_type == 'json':\n",
    "                        sub_dict[main_json_key]['Standard {}'.format(ix+1)] = {}\n",
    "                        for json_key, json_value in zip(duplicates_info[other[2]].keys(),\n",
    "                                                        duplicates_info[other[2]].values()):\n",
    "                            if 'pair' in json_key:\n",
    "                                continue\n",
    "                            if json_key != 'URN':\n",
    "                                json_value = str(json_value).capitalize()\n",
    "                                json_key = json_key.capitalize()\n",
    "                            sub_dict[main_json_key]['Standard {}'.format(ix+1)][json_key\n",
    "                                                                   ] = json_value\n",
    "                        sub_dict[main_json_key]['Standard {}'.format(ix+1)]['File name'] = other[2]\n",
    "                    else:\n",
    "                        raise(ValueError)\n",
    "                json.dump(sub_dict,subf,separators=(',',': '), indent = 4)\n",
    "\n",
    "print('Done')\n",
    "#with open('logs/output.json', 'w') as outfile:\n",
    "#     json.dump(json_data, outfile, indent=2, separators=(',', ': ')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[i*255 for i in t] for t in nesta_colours]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring transferable NOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineering = ['aeronautical engineering suite 3', 'marine engineering suite 3',\n",
    "       'explosive substances and articles', 'rail engineering',\n",
    "       'automotive engineering suite 3',\n",
    "       'mechanical manufacturing engineering suite 3 2008',\n",
    "       'marine engineering suite 2',\n",
    "       'polymer processing and related operations',\n",
    "       'engineering maintenance suite 3 2008',\n",
    "       'project control, estimating, planning and cost engineering',\n",
    "       'engineering and manufacture suite 4',\n",
    "       'engineering technical support suite 3 2009', 'down stream gas',\n",
    "       'engineering maintenance in food manufacture',\n",
    "       'land based engineering operations',\n",
    "       'performing engineering operations suite 2',\n",
    "       'engineering maintenance and installation suite 2 2008',\n",
    "       'fabrication and welding engineering suite 3',\n",
    "       'multi utility network construction',\n",
    "       'mechanical manufacturing engineering suite 2 2008',\n",
    "       'bus and coach engineering and maintenance',\n",
    "       'aeronautical engineering suite 2', 'gas network construction',\n",
    "       'building services engineering technology and project management',\n",
    "       'fabrication and welding suite 2',\n",
    "       'engineering toolmaking suite 3 2005',\n",
    "       'electrical and electronic engineering suite 3',\n",
    "       'engineering toolmaking suite 3',\n",
    "       'performing engineering operations suite 1',\n",
    "       'mechanical engineering services',\n",
    "       'electrical and electronic servicing',\n",
    "       'engineering woodworking, pattern and model making suite 3 2002',\n",
    "       'engineering technical support suite 2 2007',\n",
    "       'oil fired technical services', 'integrated systems engineering',\n",
    "       'composite engineering suite 2', 'electricity network control engineer',\n",
    "       'rail engineering signalling suite 3',\n",
    "       'maintaining plant and systems electrical',\n",
    "       'maintaining plant and systems instrument and controls',\n",
    "       'rail engineering traction and rolling stock suite 3',\n",
    "       'rail engineering telecoms suite 3', 'marine engineering',\n",
    "       'maintaining plant and systems mechanical',\n",
    "       'mechanical fire protection', 'grips and crane technicians',\n",
    "       'engineering toolmaking level 3', 'air tightness testing',\n",
    "       'plant operations extractives', 'utilities control centre operations',\n",
    "       'rail engineering telecoms suite 2',\n",
    "       'rail engineering signalling suite 2',\n",
    "       'installing plant and systems small bore tubing occupations',\n",
    "       'monitoring engineering construction activities',\n",
    "       'heating and ventilating', 'refrigeration and air conditioning',\n",
    "       'treatment processing and control in the water industry',\n",
    "       'installing plant and systems mechanical',\n",
    "       'supporting activities in engineering construction',\n",
    "       'welding plate and pipework',\n",
    "       'solar thermal photovoltaic panel installation and surveying',\n",
    "       'rail engineering overhead line equipment construction suite 2',\n",
    "       'plumbing and domestic heating',\n",
    "       'rail engineering overhead line equipment construction suite 3',\n",
    "       'installing plant and systems pipefitting', 'confined spaces',\n",
    "       'rail engineering traction and rolling stock suite 2',\n",
    "       'leakage detection and control', 'bulk explosive truck operations',\n",
    "       'electrical and electronic engineering suite 3 2004',\n",
    "       'installation, testing and commissioning of electrical systems and equipment plant',\n",
    "       'mechanical ride operation',\n",
    "       'electrotechnical services instrumentation',\n",
    "       'fabricating steel structures plating',\n",
    "       'metal decking and stud welding occupations',\n",
    "       'performing engineering operations suite 1 2006',\n",
    "       'offshore wind and marine installation and commissioning',\n",
    "       'electrotechnical services approved electrician status',\n",
    "       'engineering surveying operations',\n",
    "       'performing engineering operations suite 2 2006',\n",
    "       'land based operations', 'welding pipework',\n",
    "       'composite engineering suite 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos['NOSCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A = df_nos[df_nos['NOS Title'].map(lambda x : 'research' in x)]\n",
    "A = df_nos.loc[list(candidates.keys())]\n",
    "A = A[A['NOS Title'].map(lambda x : 'assist in' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['NOSCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[A['NOSCategory']=='job specific'][['NOS Title','One_suite','tagged_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in candidates['semret215.pdf']:\n",
    "    print(k[1])\n",
    "    print(df_nos['One_suite'].loc[k[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in candidates_paired:\n",
    "    all_suites = []\n",
    "    in_engineering = []\n",
    "    for p in candidates_paired[k]:\n",
    "        all_suites.append(df_nos['One_suite'].loc[p[2]])\n",
    "        in_engineering.append(all_suites[-1] in engineering)\n",
    "    if len(set(all_suites))>1:\n",
    "        out = process.extract(all_suites[0],all_suites[1:])\n",
    "        if (out[0][1]<80) and sum(in_engineering):\n",
    "            print(k, candidates_paired[k])#[k][0][1], p[1])\n",
    "            counter+=1\n",
    "    #if k == 'Pair 104':\n",
    "    #    print(candidates_paired[k])\n",
    "    #if counter>25:\n",
    "    #    break\n",
    "print(counter)\n",
    "print('*'*70)\n",
    "counter2 = 0\n",
    "for k in candidates_paired:\n",
    "    all_suites = []\n",
    "    in_engineering = []\n",
    "    for p in candidates_paired[k]:\n",
    "        all_suites.append(df_nos['One_suite'].loc[p[2]])\n",
    "        in_engineering.append(all_suites[-1] in engineering)\n",
    "    if len(set(all_suites))>1:\n",
    "        out = process.extract(all_suites[0],all_suites[1:])\n",
    "        if (out[0][1]>80) and sum(in_engineering):\n",
    "            print(k, candidates_paired[k])#[k][0][1], p[1])\n",
    "            counter2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.loc['cfabes004.pdf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #pos_from_viz = [t for t in node.toxml().split('<viz:') if t[:8]=='position'][0] \n",
    "    ##node.toxml().split('<viz:position')[2]\n",
    "    #pos_from_viz = pos_from_viz.split()\n",
    "    #print(pos_from_viz)\n",
    "    #tmp = re.findall('[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?',pos_from_viz[1])\n",
    "    #if '-' in pos_from_viz[1]:\n",
    "    #    pos_dict[k]['x'] = -float(tmp[0][0])\n",
    "    #else:\n",
    "    #    pos_dict[k]['x'] = float(tmp[0][0])\n",
    "    #tmp = re.findall('[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?',pos_from_viz[2])\n",
    "    #if '-' in pos_from_viz[2]:\n",
    "    #    pos_dict[k]['y'] = -float(tmp[0][0])\n",
    "    #else:\n",
    "    #    pos_dict[k]['y'] = float(tmp[0][0])\n",
    "    #break\n",
    "#print(node.toxml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
