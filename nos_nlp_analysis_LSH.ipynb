{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefgarasto/miniconda3/envs/nlp/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.7215686274509804, 0.09803921568627451], [1, 0, 0.2549019607843137], [0, 0, 0], [1, 0.35294117647058826, 0], [0.6078431372549019, 0, 0.7647058823529411], [0.6470588235294118, 0.5803921568627451, 0.5098039215686274], [0.6274509803921569, 0.5686274509803921, 0.1568627450980392], [0.7686274509803922, 0.6901960784313725, 0], [0.9647058823529412, 0.49411764705882355, 0], [0.7843137254901961, 0.1568627450980392, 0.5725490196078431], [0.23529411764705882, 0.07058823529411765, 0.3215686274509804]] [[0, 1, 2, 3, 4, 5], [0, 6, 7], [1, 3, 8], [4, 9, 10], [8, 5], [1, 11]]\n"
     ]
    }
   ],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seaborn-dark', 'seaborn-darkgrid', 'seaborn-ticks', 'fivethirtyeight', 'seaborn-whitegrid', 'classic', '_classic_test', 'fast', 'seaborn-talk', 'seaborn-dark-palette', 'seaborn-bright', 'seaborn-pastel', 'grayscale', 'seaborn-notebook', 'ggplot', 'seaborn-colorblind', 'seaborn-muted', 'seaborn', 'Solarize_Light2', 'seaborn-paper', 'bmh', 'tableau-colorblind10', 'seaborn-white', 'dark_background', 'seaborn-poster', 'seaborn-deep']\n"
     ]
    }
   ],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions and snippets of code that are useful for analysing text. Most of the techniques used are unsupervised. Functions are defined up front and then used in sections below.\n",
    "\n",
    "This notebook is to apply:\n",
    "- Tokenizers (based on n-grams and 'as_is')\n",
    "- LSH\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse html\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "#HTML Parser Methods\n",
    "#Initializing lists\n",
    "    lsData = list()\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.lsData.append(data)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return ''.join(self.lsData)\n",
    "\n",
    "           \n",
    "def strip_tags(some_html):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes html tags.\n",
    "    Returns a string.\n",
    "    \"\"\"\n",
    "    s = MyHTMLParser()\n",
    "    s.lsData = list()\n",
    "    s.feed(some_html)\n",
    "    data = s.get_data()\n",
    "    s.reset\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/glove.twitter.27B'\n",
    "\n",
    "# to make the glove model file compatible with gensim\n",
    "#for dim in ['25','50','100','200']:\n",
    "##    glove_file = os.path.join(glove_dir,'glove.twitter.27B.{}d.txt'.format(dim))\n",
    "#    tmp_file = os.path.join(glove_dir, 'word2vec.glove.twitter.27B.{}d.txt'.format(dim) )\n",
    "#    _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "LOADGLOVE = False\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.glove.twitter.27B.100d.txt'))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "#r2 = requests.get(\"https://www.instituteforapprenticeships.org/api/fullstandards/\")\n",
    "#df_api= pd.DataFrame(r2.json())\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>NOS Title</th>\n",
       "      <th>URN</th>\n",
       "      <th>Original URN</th>\n",
       "      <th>Overview</th>\n",
       "      <th>Knowledge_and_understanding</th>\n",
       "      <th>Performance_criteria</th>\n",
       "      <th>Scope_range</th>\n",
       "      <th>Glossary</th>\n",
       "      <th>Behaviours</th>\n",
       "      <th>...</th>\n",
       "      <th>All_suites</th>\n",
       "      <th>Relevant_occupations</th>\n",
       "      <th>notes</th>\n",
       "      <th>empty</th>\n",
       "      <th>extra_meta_info</th>\n",
       "      <th>Created</th>\n",
       "      <th>Path</th>\n",
       "      <th>pruned</th>\n",
       "      <th>clean_full_text</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sfshib10.pdf</th>\n",
       "      <td>maintain the performance of the human identity...</td>\n",
       "      <td>maintain the performance of the human identity...</td>\n",
       "      <td>sfshib10</td>\n",
       "      <td>[HIB10]</td>\n",
       "      <td>[This unit defines the national standard of oc...</td>\n",
       "      <td>[Provide proactive maintenance and support for...</td>\n",
       "      <td>[Provide proactive maintenance and support for...</td>\n",
       "      <td>[Provide proactive maintenance and support for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[human identity and biometrics]</td>\n",
       "      <td>[Elementary Occupations; Elementary Security O...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-31 18:05:38</td>\n",
       "      <td>publishednos-old</td>\n",
       "      <td>[performance, identity, solution, maintenance,...</td>\n",
       "      <td>maintain the performance of the human identity...</td>\n",
       "      <td>[(maintain, VB), (the, DT), (performance, NN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impqi214sv1l.pdf</th>\n",
       "      <td>contribute to the development of standard oper...</td>\n",
       "      <td>contribute to the development of standard oper...</td>\n",
       "      <td>impqi214sv1</td>\n",
       "      <td>impqi</td>\n",
       "      <td>[This standard is about the skills needed for ...</td>\n",
       "      <td>[Contribute to the development of Standard Ope...</td>\n",
       "      <td>[Contribute to the development of Standard Ope...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[achieving food manufacturing excellence]</td>\n",
       "      <td>[Manufacturing technologies; Food Preparation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[v1, Contribute to the development of Standard...</td>\n",
       "      <td>2014-03-31 17:39:08</td>\n",
       "      <td>publishednos-old</td>\n",
       "      <td>[contribute, development, procedure, sop, food...</td>\n",
       "      <td>contribute to the development of standard oper...</td>\n",
       "      <td>[(contribute, NN), (to, TO), (the, DT), (devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semme3057.pdf</th>\n",
       "      <td>loading and proving cnc fabrication machine to...</td>\n",
       "      <td>loading and proving cnc fabrication machine to...</td>\n",
       "      <td>semme3057</td>\n",
       "      <td>[]</td>\n",
       "      <td>[This standard identifies the competences you ...</td>\n",
       "      <td>[K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11,...</td>\n",
       "      <td>[P1, P2, P3, P4, P5, P6, P7, P8, work safely a...</td>\n",
       "      <td>[1. Apply all of the following checks and prac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[marine engineering suite 3]</td>\n",
       "      <td>[Marine Engineering Trades]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-03-31 17:51:56</td>\n",
       "      <td>publishednos-old</td>\n",
       "      <td>[loading, fabrication, machine, tool, program,...</td>\n",
       "      <td>loading and proving cnc fabrication machine to...</td>\n",
       "      <td>[(loading, NN), (and, CC), (proving, VBG), (cn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Title  \\\n",
       "sfshib10.pdf      maintain the performance of the human identity...   \n",
       "impqi214sv1l.pdf  contribute to the development of standard oper...   \n",
       "semme3057.pdf     loading and proving cnc fabrication machine to...   \n",
       "\n",
       "                                                          NOS Title  \\\n",
       "sfshib10.pdf      maintain the performance of the human identity...   \n",
       "impqi214sv1l.pdf  contribute to the development of standard oper...   \n",
       "semme3057.pdf     loading and proving cnc fabrication machine to...   \n",
       "\n",
       "                          URN Original URN  \\\n",
       "sfshib10.pdf         sfshib10      [HIB10]   \n",
       "impqi214sv1l.pdf  impqi214sv1        impqi   \n",
       "semme3057.pdf       semme3057           []   \n",
       "\n",
       "                                                           Overview  \\\n",
       "sfshib10.pdf      [This unit defines the national standard of oc...   \n",
       "impqi214sv1l.pdf  [This standard is about the skills needed for ...   \n",
       "semme3057.pdf     [This standard identifies the competences you ...   \n",
       "\n",
       "                                        Knowledge_and_understanding  \\\n",
       "sfshib10.pdf      [Provide proactive maintenance and support for...   \n",
       "impqi214sv1l.pdf  [Contribute to the development of Standard Ope...   \n",
       "semme3057.pdf     [K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11,...   \n",
       "\n",
       "                                               Performance_criteria  \\\n",
       "sfshib10.pdf      [Provide proactive maintenance and support for...   \n",
       "impqi214sv1l.pdf  [Contribute to the development of Standard Ope...   \n",
       "semme3057.pdf     [P1, P2, P3, P4, P5, P6, P7, P8, work safely a...   \n",
       "\n",
       "                                                        Scope_range Glossary  \\\n",
       "sfshib10.pdf      [Provide proactive maintenance and support for...      NaN   \n",
       "impqi214sv1l.pdf                                                NaN      NaN   \n",
       "semme3057.pdf     [1. Apply all of the following checks and prac...      NaN   \n",
       "\n",
       "                 Behaviours  ...                                 All_suites  \\\n",
       "sfshib10.pdf            NaN  ...            [human identity and biometrics]   \n",
       "impqi214sv1l.pdf        NaN  ...  [achieving food manufacturing excellence]   \n",
       "semme3057.pdf           NaN  ...               [marine engineering suite 3]   \n",
       "\n",
       "                                               Relevant_occupations notes  \\\n",
       "sfshib10.pdf      [Elementary Occupations; Elementary Security O...   NaN   \n",
       "impqi214sv1l.pdf  [Manufacturing technologies; Food Preparation ...   NaN   \n",
       "semme3057.pdf                           [Marine Engineering Trades]   NaN   \n",
       "\n",
       "                  empty                                    extra_meta_info  \\\n",
       "sfshib10.pdf      False                                                NaN   \n",
       "impqi214sv1l.pdf  False  [v1, Contribute to the development of Standard...   \n",
       "semme3057.pdf     False                                                NaN   \n",
       "\n",
       "                             Created              Path  \\\n",
       "sfshib10.pdf     2014-03-31 18:05:38  publishednos-old   \n",
       "impqi214sv1l.pdf 2014-03-31 17:39:08  publishednos-old   \n",
       "semme3057.pdf    2014-03-31 17:51:56  publishednos-old   \n",
       "\n",
       "                                                             pruned  \\\n",
       "sfshib10.pdf      [performance, identity, solution, maintenance,...   \n",
       "impqi214sv1l.pdf  [contribute, development, procedure, sop, food...   \n",
       "semme3057.pdf     [loading, fabrication, machine, tool, program,...   \n",
       "\n",
       "                                                    clean_full_text  \\\n",
       "sfshib10.pdf      maintain the performance of the human identity...   \n",
       "impqi214sv1l.pdf  contribute to the development of standard oper...   \n",
       "semme3057.pdf     loading and proving cnc fabrication machine to...   \n",
       "\n",
       "                                                      tagged_tokens  \n",
       "sfshib10.pdf      [(maintain, VB), (the, DT), (performance, NN),...  \n",
       "impqi214sv1l.pdf  [(contribute, NN), (to, TO), (the, DT), (devel...  \n",
       "semme3057.pdf     [(loading, NN), (and, CC), (proving, VBG), (cn...  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nos[['NOS Title', 'URN', 'Clean SOC Code', 'One_suite']].rename(columns = {\n",
    "#    'NOS Title': 'NOS Title', 'URN':'URN', 'Clean SOC Code': 'SOC Code', 'One_suite': 'Suite'\n",
    "#}).to_csv(os.path.join(output_dir,'NOS_and_SOC_codes.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHashLSHEnsemble, MinHash, MinHashLSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(text, char_ngram=5):\n",
    "    '''\n",
    "    This function splits strings into continuous sets of characters of length n. In the current example n = 5.\n",
    "    '''\n",
    "    if len(text) == 5:\n",
    "        res = set([text, text])\n",
    "    else:\n",
    "        res = set(text[head:head + char_ngram] \\\n",
    "               for head in range(0, len(text) - char_ngram))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with splitting the text into groups of characters. Elapsed time: 34.911565\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "shingled_desc = [shingles(desc) for desc in df_nos['clean_full_text']]\n",
    "print_elapsed(t0, 'splitting the text into groups of characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with creating hash signatures. Elapsed time: 17.214881\n"
     ]
    }
   ],
   "source": [
    "#Create hash signatures for shingles\n",
    "t0 = time.time()\n",
    "hash_objects = []\n",
    "for i in range(len(shingled_desc)):\n",
    "    m = MinHash(num_perm=200)\n",
    "    hash_objects.append(m)\n",
    "print_elapsed(t0, 'creating hash signatures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVELSH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with encoding hash objects. Elapsed time: 744.036693\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    for d in desc:\n",
    "        hash_objects[ix].update(d.encode('utf8'))\n",
    "print_elapsed(t0, 'encoding hash objects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "standard_labels = list(df_nos.index.values) #df_nos['URN'].values)\n",
    "title_labels = list(df_nos['NOS Title'].values)\n",
    "urn_labels = list(df_nos['URN'].values)\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    content.append((standard_labels[ix], hash_objects[ix]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSH and Jaccard similarity threshold\n",
    "LSH_th = 0.8\n",
    "lsh = MinHashLSH(threshold=LSH_th, num_perm=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,elem in enumerate(content):\n",
    "    #lsh.insert('{}'.format(ix), elem[1]) #elem[0], elem[1])\n",
    "    lsh.insert(elem[0], elem[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fspaml13 :  [('fspaml13', 'advise and guide staff in your organisation on anti-money laundering and countering terrorist financing  ', 'advise-and-guide-staff-in-your-organisation-on-anti-money-laundering-and-countering-terrorist-financing-fspaml13.pdf'), ('fspaml13', 'advise and guide staff in your organisation on anti-money laundering and countering terrorist financing', 'fspaml13.pdf')]\n",
      "***************\n",
      "cosvr383 :  [('cosvr383', 'apply specialist pavement surfacing materials manually', 'cosvr383.pdf'), ('cosvr383', 'apply specialist pavement surfacing materials manually', 'apply-specialist-pavement-surfacing-materials-manually-cosvr383.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516l.pdf'), ('cosvr401', 'set out secondary dimensional work control', 'cosvr401.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516.pdf')]\n",
      "***************\n",
      "coglata3-04 :  [('coglata3-04', 'assess and communicate scientific or technical information to authorised personnel', 'assess-and-communicate-scientific-or-technical-information-to-authorised-personnel-coglata3-04.pdf'), ('coglata3-04', 'assess and communicate scientific or technical information to authorised personnel', 'coglata3-04.pdf')]\n",
      "***************\n",
      "fspaml5 :  [('fspaml5', 'assess and mitigate the anti-money laundering and countering terrorist financing compliance risks relevant to your  organisation ', 'assess-and-mitigate-the-anti-money-laundering-and-countering-terrorist-financing-compliance-risks-relevant-fspaml5.pdf'), ('fspaml5', 'assess and mitigate the anti-money laundering and countering terrorist financing compliance risks relevant to your organisation', 'fspaml5.pdf')]\n",
      "***************\n",
      "cosvr541 :  [('cosvr357', 'apply non-reactive spray coatings legacy', 'cosvr357l.pdf'), ('cosvr227', 'install internal stone flooring', 'cosvr227.pdf'), ('cosvr521', 'carry out chemical grouting and soil injection', 'cosvr521l.pdf'), ('cosvr98', 'repair roof sheeting and cladding systems', 'cosvr98l.pdf'), ('cosvr493', 'prepare resources for fabric and membrane cladding legacy', 'cosvr493l.pdf'), ('cosvr114', 'repair and maintain liquid waterproofing systems - legacy', 'cosvr114l.pdf'), ('cosvr486', 'prepare to install well point dewatering systems', 'cosvr486l.pdf'), ('cosvr225', 'place and compact concrete', 'cosvr225l.pdf'), ('cosvr358', 'install fire resisting timber door-sets; legacy', 'cosvr358l.pdf'), ('cosvr496', 'carry out underwater lifting operations', 'cosvr496.pdf'), ('cosvr355', 'erect fire resisting walls and linings legacy', 'cosvr355l.pdf'), ('cosvr70', 'apply projection plaster and maintain equipment', 'cosvr70l.pdf'), ('cosvr41', 'set out to form masonry structures', 'set-out-to-form-masonry-structures-cosvr41.pdf'), ('cosvr492', 'carry out structural waterproofing', 'cosvr492l.pdf'), ('cosvr11 ', 'erect structural carcassing components', 'cosvr11.pdf'), ('cosvr111', 'repair and maintain single ply roofing - legacy', 'cosvr111l.pdf'), ('cosvr541', 'assess area for repair and resurfacing', 'assess-area-for-repair-and-resurfacing-cosvr541.pdf'), ('cosvr506', 'maintain and repair industrial storage systems legacy', 'cosvr506l.pdf'), ('cosvr136', 'install and repair refractory brickwork/blockwork', 'cosvr136l.pdf'), ('cosvr488', 'prepare to install deep wells or ejectors as dewatering systems', 'cosvr488l.pdf'), ('cosvr72', 'position and secure fibrous plaster components', 'cosvr72l.pdf'), ('cosvr746', 'draw and carve letters and numbers by hand', 'cosvr746.pdf'), ('cosvr383', 'apply specialist road surfacing manually', 'cosvr383l.pdf'), ('cosvr351', 'apply intumescent coatings legacy', 'cosvr351l.pdf'), ('cosvr381', 'implement, monitor and remove mobile traffic', 'cosvr381.pdf'), ('cosvr152', 'clean façade surfaces', 'clean-façade-surfaces-cosvr152.pdf'), ('cosvr542', 'prepare area for repair and resurfacing', 'prepare-area-for-repair-and-resurfacing-cosvr542.pdf'), ('cosvr135', 'prepare backgrounds for refractory installations', 'cosvr135l.pdf'), ('cosvr637', 'produce sawn stone products', 'cosvr637.pdf'), ('cosvr10', 'install second fixing components', 'cosvr10.pdf'), ('cosvr22 ', 'install shopfronts and finishings', 'cosvr22.pdf'), ('cosvr140', 'install ceramic fibre', 'cosvr140l.pdf'), ('cosvr328', 'repair and maintain metal roofing and weathering', 'cosvr328l.pdf'), ('cosvr94', 'prepare resources for roof sheeting and cladding', 'cosvr94l.pdf'), ('cosvr55', 'apply sealant to chimney structures', 'cosvr55.pdf'), ('cosvr495', 'carry out construction diving operations', 'cosvr495.pdf'), ('cosvr402', 'slinging and signalling the movement of loads - legacy', 'cosvr402l.pdf'), ('cosvr66', 'produce internal solid plastering finishes', 'cosvr66l.pdf'), ('cosvr352', 'install fire resisting ductwork systems legacy', 'cosvr352l.pdf'), ('cosvr655', 'apply insulation and finishes to complicated surfaces', 'cosvr655.pdf'), ('cosvr507', 'inspect industrial storage systems legacy', 'cosvr507l.pdf'), ('cosvr83', 'repair complex fibrous plaster components', 'cosvr83l.pdf'), ('cosvr207', 'produce complex architectural stone enrichments', 'cosvr207.pdf'), ('cosvr56', 'clean flues and appliances', 'cosvr56.pdf'), ('cosvr525', 'repair, replace and renew gates, posts and fencing', 'cosvr525.pdf'), ('cosvr474', 'produce bored wood and wood-based products', 'cosvr474.pdf'), ('cosvr20 ', 'install shopfitting frames and finishings', 'cosvr20.pdf'), ('cosvr485', 'repair masonry by replacement methods', 'cosvr485.pdf'), ('cosvr71', 'install mechanically fixed plasterboard', 'cosvr71l.pdf'), ('cosvr610', 'prepare for resin and grout injection operations', 'cosvr610l.pdf'), ('cosvr120', 'carry out site measurements and evaluations', 'cosvr120.pdf'), ('cosvr491', 'prepare backgrounds for structural waterproofing', 'cosvr491l.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516l.pdf'), ('cosvr324', 'fit sheet metal insulation protection', 'cosvr324.pdf'), ('cosvr477', 'machinery and equipment maintenance', 'cosvr477.pdf'), ('cosvr53', 'erect metal chimneys', 'cosvr53.pdf'), ('cosvr354', 'install cavity barriers legacy', 'cosvr354l.pdf'), ('cosvr173', 'provide temporary support to deep underpinning', 'cosvr173l.pdf'), ('cosvr245', 'apply sealants mechanically', 'cosvr245.pdf'), ('cosvr511', 'construct, maintain and repair complex timber and proprietary formwork', 'cosvr511.pdf'), ('cosvr368', 'lay kerbs and channels', 'cosvr368l.pdf'), ('cosvr445', 'apply preservation treatment', 'cosvr445l.pdf'), ('cosvr322', 'apply insulation and finishes to cylindrical and flat surfaces', 'cosvr322.pdf'), ('cosvr519', 'install post tensioning system as sub-structural repair', 'cosvr519l.pdf'), ('cosvr522', 'prepare to install preformed piles', 'cosvr522l.pdf'), ('cosvr65', 'apply finishing plaster to background surfaces', 'cosvr65l.pdf'), ('cosvr517', 'stress and test ground anchors', 'cosvr517.pdf'), ('cosvr543', 'repair and resurface damaged area', 'repair-and-resurface-damaged-area-cosvr543.pdf'), ('cosvr484', 'repair masonry by reinforcement methods', 'cosvr484.pdf'), ('cosvr494', 'install fabric and membrane cladding legacy', 'cosvr494l.pdf'), ('cosvr89', 'prepare backgrounds for mastic asphalt', 'cosvr89.pdf'), ('cosvr573', 'attach and prepare suspended loads for movement - legacy', 'cosvr573l.pdf'), ('cosvr470', 'produce profiled wood and wood-based products', 'cosvr470.pdf'), ('cosvr566', 'stabilise steep cutting slopes', 'cosvr566l.pdf'), ('cosvr168', 'excavate for underpinning', 'cosvr168l.pdf'), ('cosvr563', 'install mass gravity retaining structures', 'cosvr563l.pdf'), ('cosvr82', 'install complex fibrous plaster components', 'cosvr82l.pdf'), ('cosvr90', 'apply mastic asphalt', 'cosvr90.pdf'), ('cosvr54', 'line chimney structures and flues', 'cosvr54.pdf'), ('cosvr243', 'apply sealants to structural fabric', 'cosvr243.pdf'), ('cosvr47', 'maintain slate and tile roofing', 'cosvr47.pdf'), ('cosvr21 ', 'install shopfitting fitments', 'cosvr21.pdf'), ('cosvr469', 'produce planed wood and wood-based products', 'cosvr469.pdf'), ('cosvr526', 'repair or replace glazing to windows and doors', 'cosvr526.pdf'), ('cosvr524', 'clear site and hand over on completion', 'cosvr524.pdf'), ('cosvr99', 'install roof sheeting and cladding systems to curved and complex formations', 'cosvr99l.pdf'), ('cosvr596', 'produce complex external plaster finishes and surfaces', 'cosvr596.pdf'), ('cosvr73', 'repair fibrous plaster components', 'cosvr73l.pdf'), ('cosvr195', 'produce standard architectural stone enrichments', 'cosvr195.pdf'), ('cosvr68', 'install direct bond dry linings', 'cosvr68l.pdf'), ('cosvr52', 'line chimney structures with cast material', 'cosvr52.pdf'), ('cosvr509', 'fabricate and maintain timber and proprietary formwork systems (legacy)', 'cosvr509l.pdf'), ('cosvr43', 'lay domestic drainage', 'cosvr43.pdf'), ('cosvr348', 'apply metal leaf to surfaces', 'cosvr348.pdf'), ('cosvr520', 'prepare for chemical grouting and soil injection', 'cosvr520l.pdf')]\n",
      "***************\n",
      "eusldc1 :  [('euswfrbe1', 'ensure your actions reduce risks to health and safety', 'euswfrbe1.pdf'), ('eusldc1', 'assess the configuration of metered areas', 'assess-the-configuration-of-metered-areas-eusldc1.pdf')]\n",
      "***************\n",
      "coglata3-15 :  [('coglata3-15', 'assess your own scientific or technical knowledge and skills for workplace activities', 'coglata3-15.pdf'), ('coglata3-15', 'assess your own scientific or technical knowledge and skills for workplace activities', 'assess-your-own-scientific-or-technical-knowledge-and-skills-for-workplace-activities-coglata3-15.pdf')]\n",
      "***************\n",
      "fspaml2 :  [('fspaml2', 'assist in developing the compliance culture of your organisation', 'fspaml2.pdf'), ('fspaml2', 'assist in developing the compliance culture of your organisation ', 'assist-in-developing-the-compliance-culture-of-your-organisation--fspaml2.pdf')]\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "#For each standard search all signatures and identify potential clashes (e.g. other standards \n",
    "# with Jaccard similarity of shingle sets greater or equal to the threshold). \n",
    "# Note: some of the candidates might be false positives.\n",
    "candidates = {}\n",
    "singletons = []\n",
    "candidates2 = []\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "    if len(result) >1:\n",
    "        full_result = []\n",
    "        for res in result:\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        \n",
    "        candidates[standard_labels[ix]] = full_result\n",
    "        candidates2.append(result)\n",
    "        if ix<40:\n",
    "            print(urn_labels[ix], ': ', full_result)\n",
    "            print('***************')\n",
    "    else:\n",
    "        singletons.append(standard_labels[ix])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to try\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "Alternative function to get matches and organise them in groups. (From Jyl's code)\n",
    "\n",
    "def union_find(data):\n",
    "    ''''''Create Disjoint Data Structure from list of lists''''''\n",
    "    parents = {}\n",
    "    def find(i):\n",
    "        j = parents.get(i, i)\n",
    "        if j == i:\n",
    "            return i\n",
    "        k = find(j)\n",
    "        if k != j:\n",
    "            parents[i] = k\n",
    "        return k\n",
    "    for l in filter(None, data):\n",
    "        parents.update(dict.fromkeys(map(find, l), find(l[0])))\n",
    "    merged = {}\n",
    "    for k, v in parents.items():\n",
    "        merged.setdefault(find(v), []).append(k)\n",
    "    return list(merged.values())\n",
    "\n",
    "\n",
    "candidates = []\n",
    "for ix, skill_set in enumerate(skill_sets):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "#    if len(result) >1:\n",
    "    candidates.append(result)\n",
    "\n",
    "disjoint_candidates = union_find(candidates)\n",
    "\n",
    "'''\n",
    "print('to try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. of NOS that were not matched with anything: 15289\n",
      "Nb. of NOS that matched: 7328\n"
     ]
    }
   ],
   "source": [
    "# Amount of NOS that found at least a match = total NOS - number of singletons\n",
    "print('Nb. of NOS that were not matched with anything: {}'.format(len(singletons)))\n",
    "print('Nb. of NOS that matched: {}'.format(len(title_labels) - len(singletons)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''# If we make the assumptions that groups are closed and don't form chains, then:\n",
    "# nb of groups of N = nb of matches of length N / N\n",
    "all_lengths = np.array([len(t) for t in candidates2])\n",
    "unique_lengths = list(set(all_lengths))\n",
    "len_counts = []\n",
    "for ln in unique_lengths:\n",
    "    len_counts.append((ln,np.sum(all_lengths == ln)))\n",
    "print(np.sum([t[1] for t in len_counts]))\n",
    "print(len(candidates2),len(candidates))\n",
    "print(len_counts)\n",
    "'''\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nmatched = len(title_labels) - len(singletons)\n",
    "Adj_matrix = np.zeros((Nmatched,Nmatched))\n",
    "np.sum(Adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with creating the adjacency matrix. Elapsed time: 0.186375\n",
      "The highest degree in the adjacency matrix is:  282.0\n",
      "The number of matched couples are:  3611\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I think that the first thing should be to create an adjacency matrix\n",
    "t0 = time.time()\n",
    "Nmatched = len(title_labels) - len(singletons)\n",
    "Adj_matrix = np.zeros((Nmatched,Nmatched))\n",
    "# create dictionary of indices\n",
    "indices = {}\n",
    "indices_reverse = {}\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    indices[candidate] = ix\n",
    "    indices_reverse[ix] = candidate\n",
    "# now cycle again through the matched NOS and populate the adjacency matrix\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    idx1 = ix\n",
    "    for k in candidates[candidate]:\n",
    "        # now this is a list of tuples, where the first element is the urn label\n",
    "        idx2 = indices[k[2]]\n",
    "        Adj_matrix[idx1,idx2] = 1\n",
    "\n",
    "print_elapsed(t0,'creating the adjacency matrix')\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(Adj_matrix[:50,:50])\n",
    "print('The highest degree in the adjacency matrix is: ', np.max(np.sum(Adj_matrix,axis=1)))\n",
    "print('The number of matched couples are: ', np.sum(np.sum(Adj_matrix, axis = 1)==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with grouping the similar NOS. Elapsed time: 126.097320\n"
     ]
    }
   ],
   "source": [
    "# group the NOS that were matched as similar\n",
    "t0 = time.time()\n",
    "matched_groups = []\n",
    "matched_indices = []\n",
    "for ix in range(Adj_matrix.shape[0]):\n",
    "    idx_used = []\n",
    "    # find the adjacent nodes\n",
    "    where_list = list(np.where(Adj_matrix[ix])[0])\n",
    "    where_list_cumul = []\n",
    "    # don't go into the rabbit hole of nodes with very high degree - \n",
    "    # also, don't use indices already matched\n",
    "    if len(where_list)<60000 and (ix not in matched_indices):\n",
    "        for ix2 in where_list:\n",
    "            # if the neighborhood has connections to indices that we haven't included yet, \n",
    "            # add them to the list to be analysed later\n",
    "            where_list_cumul += list(np.where(Adj_matrix[ix2])[0])\n",
    "            idx_used.append(ix2)\n",
    "            # grow the neighbourhood by adding the new connections\n",
    "            new_list = [t for t in where_list_cumul if t not in where_list]\n",
    "            # don't go into the rabbit hole of nodes with very high degree\n",
    "            if len(new_list)>60000:\n",
    "                break # this one tells it to break the inner for cycle - \n",
    "                # it goes to the next \"if (set(idx_used) == set(where_list))...\"\n",
    "            if len(new_list):\n",
    "                # if the length is zero it means there are no new connected nodes\n",
    "                where_list+=new_list\n",
    "        # if it has never gone into a rabbit hole then add the group just found\n",
    "        # if and only if the neighbourhood is self-contained, \n",
    "        # that is if the nodes for which we have collected the neighbours are the same\n",
    "        # that appeared in the combined neighbourhoods\n",
    "        if (set(idx_used) == set(where_list)) and (len(new_list)<6):\n",
    "            if len(new_list)>6:\n",
    "                print('got here after breaking for index', idx_used)\n",
    "            matched_groups.append(tuple(idx_used))\n",
    "            matched_indices += idx_used #[t for t in idx_used if t not in matched_indices]\n",
    "print_elapsed(t0, 'grouping the similar NOS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups with different lengths\n",
      "[(2, 1533), (3, 275), (4, 129), (5, 51), (6, 37), (7, 20), (8, 13), (9, 5), (10, 12), (11, 10), (12, 6), (13, 6), (14, 9), (15, 3), (16, 3), (17, 4), (18, 1), (19, 2), (21, 1), (22, 1), (23, 1), (24, 3), (26, 1), (28, 2), (29, 1), (30, 1), (38, 1), (39, 1), (40, 1), (45, 2), (46, 1), (900, 1)]\n"
     ]
    }
   ],
   "source": [
    "group_lengths = [len(t) for t in matched_groups]\n",
    "group_lengths = Counter(group_lengths)\n",
    "group_lengths.most_common()\n",
    "print('Number of groups with different lengths')\n",
    "print(sorted(group_lengths.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semae3111.pdf\n",
      "semae3113.pdf\n",
      "semme3105.pdf\n",
      "semme3114.pdf\n",
      "semme3112.pdf\n",
      "semme3115.pdf\n",
      "semme3116.pdf\n",
      "semme3120.pdf\n",
      "semme3127.pdf\n",
      "semme3130.pdf\n",
      "semme3131.pdf\n",
      "semme3132.pdf\n",
      "semme3107.pdf\n",
      "semme3111.pdf\n",
      "semme3113.pdf\n",
      "semme3117.pdf\n",
      "semme3129.pdf\n",
      "semme3108.pdf\n",
      "semme3124.pdf\n",
      "semme3125.pdf\n",
      "semme3104.pdf\n",
      "semme3119.pdf\n",
      "semme3123.pdf\n",
      "semme3126.pdf\n",
      "semme3118.pdf\n",
      "semme3122.pdf\n",
      "semme3103.pdf\n",
      "semme2043.pdf\n",
      "semme3128.pdf\n",
      "semme3109.pdf\n",
      "semme2036.pdf\n",
      "semme2037.pdf\n",
      "semme2042.pdf\n",
      "semme2044.pdf\n",
      "semme2045.pdf\n",
      "semme2038.pdf\n",
      "semme2040.pdf\n",
      "semme2039.pdf\n",
      "semme2041.pdf\n",
      "semme2046.pdf\n"
     ]
    }
   ],
   "source": [
    "for group in matched_groups:\n",
    "    if len(group)==40:\n",
    "        for t in group:\n",
    "            print(indices_reverse[t])\n",
    "            #print(candidates[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups of size up to 3:  1808\n",
      "(13, 306, 3041, 3461, 3853)\n",
      "[  13  306 3041 3461 3853]\n",
      "[  13  306 3041 3461 3853]\n",
      "[  13  306 3041 3461 3853]\n",
      "[  13  306 3041 3461 3853]\n",
      "[  13  306 3041 3461 3853]\n",
      "(24, 2380, 2439, 3402, 3933, 258)\n",
      "[  24 2380 2439 3402 3933]\n",
      "[  24 2380]\n",
      "[  24 2439 3402 3933]\n",
      "[  24 2439 3402]\n",
      "[  24  258 2439 3933]\n",
      "[ 258 3933]\n",
      "(36, 359, 3386, 3468, 3855, 6871)\n",
      "[  36  359 3386 3468 3855 6871]\n",
      "[ 36 359]\n",
      "[  36 3386 3468 3855]\n",
      "[  36 3386 3468]\n",
      "[  36 3386 3855]\n",
      "[  36 6871]\n"
     ]
    }
   ],
   "source": [
    "#for t in matched_couples:\n",
    "#    if not (t in matched_groups):\n",
    "#        print(t)\n",
    "# show some of the groups\n",
    "groups_length = [len(t) for t in matched_groups]\n",
    "print('Number of groups of size up to 3: ', np.sum([t<4 for t in groups_length]))\n",
    "nbprint = 0\n",
    "for t in matched_groups:\n",
    "    if len(t)>4 and len(t)<7:\n",
    "        print(t)\n",
    "        for it in t:\n",
    "            print(np.where(Adj_matrix[it])[0])\n",
    "        nbprint+=1\n",
    "    if nbprint>2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If something gets printed next, then the groups indentified have overlaps\n"
     ]
    }
   ],
   "source": [
    "# check that all the groups are not overlapping: if it prints something is bad\n",
    "print('If something gets printed next, then the groups indentified have overlaps')\n",
    "matched_groups2 = []\n",
    "for it,t in enumerate(matched_groups):\n",
    "    if it>40000:\n",
    "        break\n",
    "    flag = 0\n",
    "    for t2 in matched_groups:\n",
    "        if t != t2:\n",
    "            if set(t).intersection(set(t2)):\n",
    "                print(t,t2)\n",
    "                flag += 1\n",
    "                #if (set(t)-set(t2)) and (set(t2) - set(t)):\n",
    "                #    print(t,t2)\n",
    "    if flag == 0:\n",
    "        matched_groups2.append(t)\n",
    "    else:\n",
    "        print(flag)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of grouped NOS 7328 number of matched NOS 7328\n",
      "For the two parts of the Adjacency matrix to be separate, there has to be no edges between the two.\n",
      "The number of edges is: \n",
      "0.0\n",
      "if 1 the matrix is symmetric:  1.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "N = Adj_matrix.shape[0]\n",
    "Nmatched= len(matched_indices)\n",
    "print('number of grouped NOS', N, 'number of matched NOS', Nmatched)\n",
    "# finally, check that the grouped and the non-grouped indices have a separate adjacency matrix\n",
    "Adj_matrix2 = copy.deepcopy(Adj_matrix)\n",
    "leftout_indices = list(set(range(N)) - set(matched_indices))\n",
    "neworder_indices = np.array(matched_indices + leftout_indices)\n",
    "# rearrange rows and columns of the adjacency matrix\n",
    "Adj_matrix2 = Adj_matrix2[:, neworder_indices][neworder_indices]\n",
    "print(''.join(['For the two parts of the Adjacency matrix to be separate, there has to',\n",
    "               ' be no edges between the two.']))\n",
    "print('The number of edges is: ')\n",
    "print(np.sum(Adj_matrix2[:Nmatched,Nmatched:]))\n",
    "# check the matrix is symmetric\n",
    "print('if 1 the matrix is symmetric: ', np.mean(Adj_matrix2 == Adj_matrix2.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(leftout_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# now add the ones that weren't matched\\nfor ix in leftout_indices:\\n    res = indices_reverse[ig]\\n    candidates_grouped[res] = candidates[res]\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now redo the dictionary to save, with placing the self-contained groups first \n",
    "# and then the unpaired ones\n",
    "from collections import OrderedDict\n",
    "candidates_grouped = OrderedDict()\n",
    "candidates_paired = OrderedDict()\n",
    "\n",
    "pair_counter = 0\n",
    "group_counter = 0\n",
    "for ix, group in enumerate(matched_groups):\n",
    "    full_result = []\n",
    "    if len(group)==2:\n",
    "        for ig in group:\n",
    "            res = indices_reverse[ig]\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        candidates_paired['Pair {}'.format(pair_counter)] = full_result\n",
    "        pair_counter+=1\n",
    "for ix, group in enumerate(matched_groups):\n",
    "    full_result = []\n",
    "    if len(group)>2:\n",
    "        for ig in group:\n",
    "            res = indices_reverse[ig]\n",
    "            full_result.append((urn_labels[standard_labels.index(res)\n",
    "                                          ],df_nos['NOS Title'].loc[res],res))\n",
    "        candidates_grouped['Group {}'.format(group_counter)] = full_result\n",
    "        group_counter+=1\n",
    "    \n",
    "#candidates_grouped['Finished with groups'] = '-'\n",
    "\n",
    "'''# now add the ones that weren't matched\n",
    "for ix in leftout_indices:\n",
    "    res = indices_reverse[ig]\n",
    "    candidates_grouped[res] = candidates[res]\n",
    "'''\n",
    "# try to save it, just to see what it looks like\n",
    "## OBSOLETE ONES\n",
    "#if SAVELSH and False:\n",
    "#    tmp = pd.DataFrame.from_dict(candidates_grouped, orient = 'index')\n",
    "#    print(tmp.columns)\n",
    "#    tmp.to_csv(output_dir +  '/LSH_results_grouped_no_pairs_{}_th{}.csv'.format(\n",
    "#                                                    qualifier,LSH_th))\n",
    "\n",
    "#if SAVELSH and False:\n",
    "#    pd.DataFrame.from_dict(candidates_paired, orient = 'index', columns = [\n",
    "#                                                    'NOS 1','NOS 2']).to_csv(output_dir + \n",
    "#                                  '/LSH_results_paired_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVELSH and False:\n",
    "    pd.DataFrame.from_dict(candidates, orient = 'index').to_csv(output_dir + '/LSH_results_{}_th{}.csv'.format(\n",
    "        qualifier,LSH_th))\n",
    "    with open(output_dir + '/Candidates_nos_{}_th{}.pickle'.format(qualifier,LSH_th),'wb') as f:\n",
    "        pickle.dump(candidates,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`Series.plot()` should not be called with positional arguments, only keyword arguments. The order of positional arguments will change in the future. Use `Series.plot(kind='barh')` instead of `Series.plot('barh',)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1d0a15d6e947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Counts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mduplicated_suites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'barh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnesta_colours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         x, y, kind, kwargs = self._get_call_args(\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         )\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_get_call_args\u001b[0;34m(backend_name, data, args, kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0;34mf\"`Series.plot({positional_args})`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             )\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mpos_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: `Series.plot()` should not be called with positional arguments, only keyword arguments. The order of positional arguments will change in the future. Use `Series.plot(kind='barh')` instead of `Series.plot('barh',)`."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x1296 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot how many duplicates per suite / per originating organisation\n",
    "# get the value count\n",
    "def add_text_to_hist_new(values, xvalues = None, addval = None, orient = 'vertical'):\n",
    "    if addval is None:\n",
    "        addval = .5 + np.floor(2*np.log(max(values)))\n",
    "    addx = -.2 if orient=='horizontal' else 0\n",
    "    for ix,i in enumerate(values):\n",
    "        if i>-1:\n",
    "            if not xvalues:\n",
    "                x = ix - .2\n",
    "            else:\n",
    "                x = xvalues[ix] +.02\n",
    "            if orient == 'vertical':\n",
    "                plt.text(i+addval, x, '{}'.format(i), fontsize = 14)\n",
    "            else:\n",
    "                plt.text(x, ix+addval,'{}'.format(i), fontsize = 14)\n",
    "                \n",
    "for col in ['One_suite', 'Developed By']:\n",
    "    duplicated_suites = df_nos.loc[list(candidates.keys())][col].value_counts()\n",
    "    all_suites = df_nos[col].value_counts()\n",
    "    if col == 'One_suite':\n",
    "        N = 70\n",
    "        fig =plt.figure(figsize = (11,18))\n",
    "        plt.ylabel('Suite',fontsize = 18)\n",
    "    else:\n",
    "        N = 32\n",
    "        fig = plt.figure(figsize = (7,12))\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Counts', fontsize = 18)\n",
    "    with sns.plotting_context('talk'):\n",
    "        duplicated_suites[:N][::-1].plot('barh', color = nesta_colours[3])\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_counts_for_duplicates.png'.format(col)))\n",
    "\n",
    "    # now divide by the sum (so we get fractions)\n",
    "    #duplicated_suites = duplicated_suites.map(lambda x: x/duplicated_suites.sum())\n",
    "    #all_suites = all_suites.map(lambda x: x/all_suites.sum())\n",
    "    # get the ratio of proportions with respect to the full distribution\n",
    "    suites_ratio = {}\n",
    "    abs_values = {}\n",
    "    for row in all_suites.index[:N]:\n",
    "        try:\n",
    "            suites_ratio[row] = duplicated_suites.loc[row]/all_suites.loc[row]\n",
    "            abs_values[row] = duplicated_suites.loc[row]\n",
    "        except:\n",
    "            suites_ratio[row] = 0\n",
    "            abs_values[row] = 0\n",
    "    suites_ratio = pd.DataFrame.from_dict(suites_ratio, orient = 'index', columns = ['ratio'])\n",
    "    ## order by decreasing ratios \n",
    "    #suites_ratio= suites_ratio.sort_values(by='ratio', ascending = False)\n",
    "    # plot the ratio\n",
    "    if col == 'One_suite':\n",
    "        fig=plt.figure(figsize = (12,18))\n",
    "        ix = range(N)\n",
    "        plt.ylabel('Suite',fontsize = 18)\n",
    "    else:\n",
    "        fig =plt.figure(figsize = (7,12))\n",
    "        ix = range(N)\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Proportion of NOS', fontsize = 18)\n",
    "    plt.xlim([0,1])\n",
    "    with sns.plotting_context('talk'):\n",
    "        suites_ratio['ratio'].iloc[ix][::-1].plot('barh', color = nesta_colours[3])\n",
    "    add_text_to_hist_new(list(abs_values.values())[::-1], xvalues = list(suites_ratio['ratio'].iloc[ix][::-1]), \n",
    "                         addval = -0.3, orient = 'horizontal')\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_ratios_for_duplicates_{}.png'.format(col,LSH_th)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute level of overlap for each group identified via the LSH algorithm\n",
    "- Use the average level of pairwise overlap between NOS in each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    res = float(intersection / union)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "['write-text-based-material-for-multi-platform-use-sksj13.pdf']\n"
     ]
    }
   ],
   "source": [
    "# this is just to assign keys and NOS to a dictionary - \n",
    "# so far they're in a list, so I can't easily find the MinHash corresponding to a given NOS\n",
    "\n",
    "content_dict = {}\n",
    "for ix,elem in enumerate(content):\n",
    "    content_dict[elem[0]] = elem[1]\n",
    "    \n",
    "print(content_dict[lsh.query(elem[1])[0]].jaccard(content_dict[lsh.query(elem[1])[0]]))\n",
    "print(lsh.query(elem[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909 3611 0.7482691775131542\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 921.6x633.6 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# just to show the precision of the LSH algorithm. # Just get the pairs of near duplicates \n",
    "# and compute the Jaccard similarity using the inbuil function\n",
    "# Then check how many of these pairs actually do have Jaccard > LSH_th\n",
    "pair_similarities = []\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "    if len(result)==2:\n",
    "        pair_similarities.append(content_dict[result[0]].jaccard(content_dict[result[1]]))\n",
    "plt.hist(pair_similarities)\n",
    "a = (np.array(pair_similarities)<LSH_th).sum()\n",
    "b = len(pair_similarities)\n",
    "print(a, b, 1-a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update. Time = 7.5618s\n",
      "Done in 14.7172s\n"
     ]
    }
   ],
   "source": [
    "# compute the average level of overlap within groups and compare to the average \n",
    "# number of common words\n",
    "# candidates_grouped_new = copy.deepcopy(candidates_grouped)\n",
    "#A = []\n",
    "#B = []\n",
    "all_j_values = []\n",
    "all_w_values = []\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_paired.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_paired[g]\n",
    "        j_values = []\n",
    "        g_suites = []\n",
    "        g_cat = []\n",
    "        w_values = []\n",
    "        \n",
    "        for t,ix1 in enumerate(G):\n",
    "            # create bag of words that retain duplicated words\n",
    "            if len(G)<50:\n",
    "                t1 = []\n",
    "                for tt in df_nos.loc[ix1[2]]['clean_full_text'].split():\n",
    "                    if tt not in t1:\n",
    "                        t1.append(tt)\n",
    "                    else:\n",
    "                        t1.append(tt+'1')\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values.append(content_dict[ix1[2]].jaccard(content_dict[ix2[2]]))\n",
    "                if len(G)<50:\n",
    "                    # create bag of words that retain duplicates\n",
    "                    t2 = []\n",
    "                    for tt in df_nos.loc[ix2[2]]['clean_full_text'].split():\n",
    "                        if tt not in t2:\n",
    "                            t2.append(tt)\n",
    "                        else:\n",
    "                            t2.append(tt + '1')\n",
    "                    w_values.append(len(set(t1).intersection(set(t2)))/ (len(set(t1)\n",
    "                                                                        )+len(set(t2))) *2)\n",
    "            g_suites.append(df_nos.loc[ix1[2]]['One_suite'])\n",
    "            g_cat.append(df_nos.loc[ix1[2]]['NOSCategory'])\n",
    "        \n",
    "        if len(G)<50:\n",
    "            # keep jaccard similarities\n",
    "            all_j_values += j_values\n",
    "            # keep proportion of common words\n",
    "            all_w_values += w_values\n",
    "        # append average jaccard similarity and number of unique suites in the group \n",
    "        # divided by groups size\n",
    "        tmp = [t == 'generic' for t in g_cat]\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "# print number of common words\n",
    "#print(np.mean(np.array(all_w_values)[np.array(all_j_values)>0.9]))\n",
    "#np.mean(all_w_values[np.array(all_j_values)>.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First part done in 3.6672s\n",
      "All done in 4.1916s\n"
     ]
    }
   ],
   "source": [
    "# compute the average level of overlap within groups bigger than two\n",
    "candidates_grouped_new = copy.deepcopy(candidates_grouped)\n",
    "candidates_grouped_string = {}\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_grouped.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_grouped[g]\n",
    "        j_values = []\n",
    "        g_suites = []\n",
    "        g_cat = []\n",
    "        w_values = []\n",
    "        nos_strings = []\n",
    "        for t,ix1 in enumerate(G):\n",
    "            urn = ix1[0]\n",
    "            title = ix1[1]\n",
    "            nos_file = ix1[2]\n",
    "            validity = df_nos.loc[nos_file]['Validity']\n",
    "            status = df_nos.loc[nos_file]['Status']\n",
    "            nos_strings.append('[{}] \\\"{}\\\" (\\\"{}\\\", {}, {})'.format(urn, title.capitalize(), nos_file, \n",
    "                                                                 validity.capitalize(),status.capitalize()))\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values.append(content_dict[ix1[2]].jaccard(content_dict[ix2[2]]))\n",
    "        candidates_grouped_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "        candidates_grouped_string[g] = [np.around(np.mean(j_values),3)] + nos_strings\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('First part done in {:.4f}s'.format(time.time()-t0))\n",
    "\n",
    "#candidates_grouped_new.pop('Finished with groups')\n",
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(candidates_grouped_string, orient = 'index').rename(columns = {0:'Avg group similarity'})\n",
    "# rename the columns\n",
    "rename_dict= {}\n",
    "for ii in range(len(tmp.columns)):\n",
    "    rename_dict[ii] = 'NOS {}'.format(ii)\n",
    "tmp = tmp.rename(columns = rename_dict)\n",
    "# sort by descending values of the average similarity\n",
    "tmp = tmp.sort_values(by = 'Avg group similarity', ascending = False)\n",
    "# rename the rows after the sorting\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "rename_dict = {}\n",
    "for ii in range(len(tmp)):\n",
    "    rename_dict[ii] = 'Group {}'.format(ii)\n",
    "tmp = tmp.rename(index = rename_dict)\n",
    "\n",
    "# OFFICIAL ONE\n",
    "if SAVELSH:\n",
    "#   tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns= {0:'Avg group similarity'})\n",
    "    tmp.to_csv(output_dir + '/LSH_results_grouped_no_pairs_with_score_{}_th{}.csv'.format(qualifier,LSH_th))\n",
    "print('All done in {:.4f}s'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split large construction group into communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_group =candidates_grouped_new['Group 0']\n",
    "long_group = long_group[1:]\n",
    "\n",
    "\n",
    "'''# recompute pairwise Jaccard similarities\n",
    "j_values = np.zeros((len(long_group),len(long_group)))\n",
    "#g_suites = []\n",
    "#g_cat = []\n",
    "#w_values = []\n",
    "#nos_strings = []\n",
    "for t,ix1 in enumerate(long_group):\n",
    "    urn = ix1[0]\n",
    "    title = ix1[1]\n",
    "    nos_file = ix1[2]\n",
    "    #validity = df_nos.loc[nos_file]['Validity']\n",
    "    #status = df_nos.loc[nos_file]['Status']\n",
    "    #nos_strings.append('[{}] \\\"{}\\\" (\\\"{}\\\", {}, {})'.format(urn, title.capitalize(), nos_file, \n",
    "    #                                                     validity.capitalize(),status.capitalize()))\n",
    "    for _t,ix2 in enumerate(long_group[t+1:]):\n",
    "        t2 = t+_t+1\n",
    "        j_values[t,t2]  =content_dict[ix1[2]].jaccard(content_dict[ix2[2]])\n",
    "j_values = j_values + j_values.T + np.identity(len(long_group))'''\n",
    "print(long_group[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cosvr383', 'apply specialist pavement surfacing materials manually', 'cosvr383.pdf'), ('cosvr383', 'apply specialist pavement surfacing materials manually', 'apply-specialist-pavement-surfacing-materials-manually-cosvr383.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516l.pdf'), ('cosvr401', 'set out secondary dimensional work control', 'cosvr401.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516.pdf')]\n",
      "\n",
      "[('cosvr383', 'apply specialist pavement surfacing materials manually', 'cosvr383.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516l.pdf'), ('cosvr401', 'set out secondary dimensional work control', 'cosvr401.pdf'), ('cosvr516', 'install self drilling ground anchors', 'cosvr516.pdf')]\n"
     ]
    }
   ],
   "source": [
    "for doc in long_group:\n",
    "    doc_pdf = doc[2]\n",
    "    near_duplicate_docs = candidates[doc_pdf]\n",
    "    only_duplicate_docs = []\n",
    "    for dupl_doc in near_duplicate_docs:\n",
    "        if doc_pdf != dupl_doc[2]:\n",
    "            only_duplicate_docs.append(dupl_doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community detection algorithm\n",
    "t0 = time.time()\n",
    "partition_lenghts = []\n",
    "for th in [.7,.75,.8,.85]:\n",
    "    graph_full = nx.from_numpy_matrix(j_values*(j_values>th))\n",
    "    #nx.draw(graph_full)\n",
    "    C = list(community.greedy_modularity_communities(graph_full))\n",
    "    print(f'{time.time()-t0:.2f}')\n",
    "    print('Is it a partition? ', community.is_partition(graph_full,C))\n",
    "    partition_lenghts.append([len(c) for c in C])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example group\n",
    "c= C[0]\n",
    "for t in c:\n",
    "    print(long_group[t+1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49] <datasketch.minhash.MinHash object at 0x1be2f29780>\n",
      "Update. Time = 0.9032s\n",
      "Done in 1.4128s\n"
     ]
    }
   ],
   "source": [
    "# compute the average level of overlap within pairs\n",
    "candidates_paired_new = copy.deepcopy(candidates_paired)\n",
    "candidates_paired_string = {}\n",
    "t0 = time.time()\n",
    "for ig,g in enumerate(candidates_paired.keys()):\n",
    "    if g != 'Finished with groups':\n",
    "        G = candidates_paired[g]\n",
    "        j_values = []\n",
    "        nos_strings = []\n",
    "        for t,ix1 in enumerate(G):\n",
    "            urn = ix1[0]\n",
    "            title = ix1[1]\n",
    "            nos_file = ix1[2]\n",
    "            validity = df_nos.loc[nos_file]['Validity']\n",
    "            status = df_nos.loc[nos_file]['Status']\n",
    "            nos_strings.append('[{}] \\\"{}\\\" (\\\"{}\\\", {}, {})'.format(urn, title.capitalize(), nos_file, \n",
    "                                                                 validity.capitalize(), status.capitalize()))\n",
    "            for ix2 in G[t+1:]:\n",
    "                j_values = [content_dict[ix1[2]].jaccard(content_dict[ix2[2]])]\n",
    "                if ig%30>-1:\n",
    "                    if ix1[2]=='mpqmg31.pdf':\n",
    "                        print(j_values,content_dict[ix1[2]] )\n",
    "                    #print(ix1[2],ix2[2])\n",
    "        candidates_paired_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "        candidates_paired_string[g] = [np.around(np.mean(j_values),3)] + nos_strings\n",
    "    if ig%1000 == 999:\n",
    "        print('Update. Time = {:.4f}s'.format(time.time()-t0))\n",
    "print('Done in {:.4f}s'.format(time.time()-t0))\n",
    "\n",
    "#candidates_grouped_new.pop('Finished with groups')\n",
    "# create the dataframe\n",
    "tmp = pd.DataFrame.from_dict(candidates_paired_string, orient = 'index').rename(columns = {0:'Avg group similarity'})\n",
    "# rename the columns\n",
    "rename_dict= {}\n",
    "for ii in range(len(tmp.columns)):\n",
    "    rename_dict[ii] = 'NOS {}'.format(ii)\n",
    "tmp = tmp.rename(columns = rename_dict)\n",
    "# sort by descending values of the average similarity\n",
    "tmp = tmp.sort_values(by = 'Avg group similarity', ascending = False)\n",
    "# rename the rows after the sorting\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "rename_dict = {}\n",
    "for ii in range(len(tmp)):\n",
    "    rename_dict[ii] = 'Pair {}'.format(ii)\n",
    "tmp = tmp.rename(index = rename_dict)\n",
    "if SAVELSH:\n",
    "#   tmp = pd.DataFrame.from_dict(candidates_grouped_new, orient = 'index').rename(columns= {0:'Avg group similarity'})\n",
    "    tmp.to_csv(output_dir + '/LSH_results_pairs_with_score_{}_th{}.csv'.format(qualifier,LSH_th))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90%:  0.9740900880982742\n",
      "80%:  0.9268130376522428\n"
     ]
    }
   ],
   "source": [
    "print('90%: ', np.mean(np.array(all_w_values)[np.array(all_j_values)>0.9]))\n",
    "print('80%: ',np.array(all_w_values)[(np.array(all_j_values)>0.8) & (np.array(all_j_values)<0.9)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750.2505637352434\n"
     ]
    }
   ],
   "source": [
    "print(df_nos['clean_full_text'].map(lambda x: len(x.split())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_nos = df_nos.loc[list(candidates.keys())]\n",
    "duplicated_cos = duplicated_nos[duplicated_nos['Developed By']=='construction skills']\n",
    "duplicated_nos = duplicated_nos[duplicated_nos['Developed By']=='semta']\n",
    "tmp = list(duplicated_nos.index)\n",
    "print([t for t in tmp if 'l' in t[-7:]])\n",
    "print(duplicated_nos.columns)\n",
    "print(duplicated_nos['Version_number'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# to be old and new updated nos, it would have to be the following conditions:\n",
    "1. group of two NOS\n",
    "2. two different versions (1 and higher)\n",
    "3. same or very similar suite (that is, the same aside from numbers)\n",
    "'''\n",
    "semta_versions = []\n",
    "mixed_version = []\n",
    "semta_suites = []\n",
    "semta_titles = []\n",
    "for g in candidates_paired_new:\n",
    "    group = candidates_paired_new[g]\n",
    "    if group[1][0][:3]:# == 'sem':\n",
    "        #print(len(group))\n",
    "        semta_versions.append([])\n",
    "        semta_suites.append([])\n",
    "        semta_titles.append([])\n",
    "        if len(group)==3:\n",
    "            # only take groups of pairs\n",
    "            for ix in range(1,len(group)):\n",
    "                #print(duplicated_nos.loc[group[ix][2]]['Version_number'])\n",
    "                try:\n",
    "                    semta_versions[-1].append(duplicated_nos.loc[group[ix][2]][\n",
    "                        'Version_number'])\n",
    "                    semta_suites[-1].append(duplicated_nos.loc[group[ix][2]]['One_suite'])\n",
    "                    semta_titles[-1].append(group[ix][1])\n",
    "                except:\n",
    "                    1\n",
    "            if (1.0 in semta_versions[-1]) & ((2.0 in semta_versions[-1]) or (\n",
    "                                                    3.0 in semta_versions[-1])):\n",
    "                # check the suites\n",
    "                suite1 = ''.join([t for t in semta_suites[-1][0] if not t.isdigit()])\n",
    "                suite2 = ''.join([t for t in semta_suites[-1][1] if not t.isdigit()])\n",
    "                title1 = semta_titles[-1][0]\n",
    "                title2 = semta_titles[-1][1]\n",
    "                out = process.extract(title1, [title2])\n",
    "                out2 = process.extract(suite1, [suite2])\n",
    "                # assume two titles are similar if the fuzzy matching is higher than 90\n",
    "                if out[0][1]>79:\n",
    "                    # if we also require the two suite names to have a match higher than 90\n",
    "                    if out2[0][1]>79:\n",
    "                        mixed_version.append(group)\n",
    "                        print(semta_suites[-1][0],semta_suites[-1][1])\n",
    "                        \n",
    "print(semta_versions)\n",
    "len(mixed_version), len(semta_versions), len(duplicated_nos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction skills should be responsible for the very big group\n",
    "print(len(duplicated_cos))\n",
    "for g in candidates_grouped_new:\n",
    "    group = candidates_grouped_new[g]\n",
    "    if group[1][0][:3] == 'cos':\n",
    "        if len(group)>200:\n",
    "            tmp = [group[ix][0][:3] for ix in range(1,len(group))]\n",
    "            print(sum([t=='cos' for t in tmp]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from xml.dom.minidom import parseString\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_soc(x):\n",
    "    if isinstance(x,list):\n",
    "        try:\n",
    "            return x[0]\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def get_one_occupation(x):\n",
    "    if isinstance(x,list):\n",
    "        y= ''.join(x)\n",
    "    else:\n",
    "        y= x\n",
    "    if isinstance(y,str):\n",
    "        return y.split(';')\n",
    "    else:\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load duplicated NOS\n",
    "if False:\n",
    "    lshduplicate_file = ''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/',\n",
    "         'LSH_results_grouped_with_score_postjoining_final_no_dropped_th0.8.csv'])\n",
    "    lshduplicate_nos = pd.read_csv(lshduplicate_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #%% process the LSH duplicates to assign group ID to NOS\n",
    "    # NOTE: these are all the groups, not just engineering NOS\n",
    "    def split_nos_in_groups(x):\n",
    "        if isinstance(x,str):\n",
    "            x = [t.strip() for t in x.replace(')','').replace('(','').replace('\\'','').split(',')]\n",
    "            return x[-1]\n",
    "        else:\n",
    "            return x\n",
    "    tmp0 = lshduplicate_nos.applymap(split_nos_in_groups)\n",
    "    df_nos_lsh = tmp0[['Unnamed: 0','Avg group similarity','1']]\n",
    "    t0 = time.time()\n",
    "    for i in range(2, len(lshduplicate_nos.columns)-2):\n",
    "        tmp = tmp0[['Unnamed: 0','Avg group similarity','{}'.format(i)]].rename(columns = {'{}'.format(i):'1'})\n",
    "        tmp = tmp[tmp['1'].notna()]\n",
    "        df_nos_lsh = pd.concat([df_nos_lsh, tmp])\n",
    "    print_elapsed(t0, 'assigning LSH groups to NOS')\n",
    "\n",
    "    #%% Join LSH groups and transferable NOS\n",
    "    df_nos2 = df_nos.join(df_nos_lsh.rename(columns = {'Unnamed: 0': 'lsh_group', \n",
    "                                                      'Avg group similarity': 'lsh_simil',\n",
    "                                                      '1':'index'}).set_index('index'), how = 'left')\n",
    "    df_nos2['lsh_group'].mask(df_nos2['lsh_group'].isnull(), 'na', inplace= True)\n",
    "    # remove LSH groups with a low overall similarity\n",
    "    #th_lsh = 0.75\n",
    "    #df_nos2['lsh_group'].mask(df_nos2['lsh_simil']<th_lsh, 'na', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the largest group\n",
    "if False:\n",
    "    df_group = df_nos2.groupby('lsh_group')[['Title','URN']].agg(len)\n",
    "    largest_group_name = df_group.sort_values('Title').index[-2]\n",
    "    # minus 2 because we don't want the 'na's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the largest group and the relevant columns, then order it according to G\n",
    "if False:\n",
    "    metadata_cols = ['NOS Title', 'URN',\n",
    "           'Originating_organisation','Date_approved_year',\n",
    "           'Clean Ind Review Year', 'Version_number', 'Developed By', 'Validity',\n",
    "           'Keywords', 'Clean SOC Code', 'NOS Document Status', 'NOSCategory', 'Occupations',\n",
    "           'One_suite']\n",
    "    largest_group = df_nos2[df_nos2['lsh_group']==largest_group_name][metadata_cols]\n",
    "    # what's missing is to collect the Jaccard similarity among each pair of NOS in this largest group\n",
    "    # get the old name\n",
    "    for g in candidates_grouped:\n",
    "        group = candidates_grouped[g]\n",
    "        if len(group)>=len(largest_group):\n",
    "            print(g,len(group),len(largest_group))\n",
    "            break\n",
    "    G = [standard for standard in group if standard[2] in largest_group.index]\n",
    "\n",
    "    largest_group = largest_group.join(pd.DataFrame(np.arange(898), index = [standard[2] for standard in G],\n",
    "                                                    columns = ['order']))\n",
    "    largest_group = largest_group.sort_values(by = 'order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_viz = '/Users/stefgarasto/Google Drive/Documents/results/NOS/data_viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all similarities\n",
    "if False:\n",
    "    j_values = []\n",
    "    g_suites = []\n",
    "    g_cat = []\n",
    "    w_values = []\n",
    "    j_values_matrix = np.zeros((len(G),len(G)))\n",
    "    t0 = time.time()\n",
    "    for t1,ix1 in enumerate(G):\n",
    "        j_values_matrix[t1,t1] = .5\n",
    "        for t2,ix2 in enumerate(G):\n",
    "            if t2<=t1:\n",
    "                continue\n",
    "            tmp_simil = content_dict[ix1[2]].jaccard(content_dict[ix2[2]])\n",
    "            j_values.append(tmp_simil)\n",
    "            j_values_matrix[t1,t2] = tmp_simil\n",
    "        #candidates_grouped_new[g] = [np.around(np.mean(j_values),3)] + G\n",
    "    print_elapsed(t0, 'computing similarities')\n",
    "    # the matrix so far is TRIU - add the lower triangular bit to make it full\n",
    "    j_values_matrix = j_values_matrix + j_values_matrix.T\n",
    "    SAVEDATALSH = False\n",
    "    if SAVEDATALSH:\n",
    "        with open(os.path.join(output_dir_viz, \n",
    "                'data_from_largest_lsh_group_similarities.pickle'), 'wb') as f:\n",
    "            pickle.dump((largest_group, j_values_matrix), f)\n",
    "    #print(time.time() - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute a maximum spanning tree and use Gephi to get a good node layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    t0 = time.time()\n",
    "    # create graph\n",
    "    graph_full = nx.from_numpy_matrix(j_values_matrix)\n",
    "    # assign labels\n",
    "    new_labels = dict(zip(range(len(G)),largest_group.index))\n",
    "    graph_full = nx.relabel_nodes(graph_full, new_labels)\n",
    "    print_elapsed(t0,'creating and relabelling graph')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create maximum spanning tree\n",
    "if False:\n",
    "    t0 = time.time()\n",
    "    graph_tree = nx.maximum_spanning_tree(graph_full)\n",
    "    print_elapsed(t0,'getting max spanning tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save as gexf\n",
    "if False:\n",
    "    nx.write_gexf(graph_tree, os.path.join(output_dir_viz, \n",
    "                'max_tree_largest_group.gexf'))\n",
    "\n",
    "    #pos = nx.random_layout(graph_full)\n",
    "    #nx.draw_networkx_edge_labels(graph_full, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proceed_after_gephi = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only AFTER changing the layout in Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not proceed_after_gephi:\n",
    "    stop\n",
    "    \n",
    "#node = tree.getElementsByTagName('node')[0].getElementsByTagName('attvalue')\n",
    "#print(node[0].attributes.items())\n",
    "#print(nodes.getElementsByTagName('attvalue')[0].attributes.items())#getElementsByTagName('attvalue'))\n",
    "#print(nodes.getElementsByTagName('attvalue')[2].attributes.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos = nx.planar_layout(graph_tree)\n",
    "#nx.draw(graph_tree, pos)\n",
    "# read the new gexf layout\n",
    "#graph_tree2 = nx.read_gexf(os.path.join(output_dir_viz,'largest_group_tree_layout_with_force_atlas2.gexf'))\n",
    "if False:\n",
    "    pos_dict ={}\n",
    "    # pase the gmxf file and save the position\n",
    "\n",
    "    with open(os.path.join(output_dir_viz,'largest_group_tree_layout_with_force_atlas2_2.gexf'),\n",
    "              'r') as f:\n",
    "        tree = parseString(f.read())\n",
    "\n",
    "    nodes = tree.getElementsByTagName('node')\n",
    "    for node in nodes:\n",
    "        k =node.getAttribute('id')\n",
    "        pos_dict[k] = {}\n",
    "        # to get the degree\n",
    "        pos_tree= node.getElementsByTagName('attvalue')\n",
    "        for value in pos_tree:\n",
    "            pos_values = value.attributes.items()\n",
    "            pos_dict[k][pos_values[0][1]] = pos_values[1][1]\n",
    "\n",
    "        # to get the position\n",
    "        pos_tree = node.getElementsByTagName('viz:position')\n",
    "        pos_values = pos_tree[0].attributes.items()\n",
    "        for att in pos_values:\n",
    "            pos_dict[k][att[0]] = float(att[1])\n",
    "\n",
    "        # to get the size\n",
    "        pos_tree = node.getElementsByTagName('viz:size')\n",
    "        pos_dict[k]['size']= pos_tree[0].attributes.items()[0][1]\n",
    "\n",
    "        # to get the colour (synonym of community I think)\n",
    "        pos_tree = node.getElementsByTagName('viz:color')\n",
    "        pos_values = pos_tree[0].attributes.items()\n",
    "        pos_dict[k]['color']= 'rbg({},{},{})'.format(pos_values[0][1],pos_values[1][1],pos_values[2][1])\n",
    "\n",
    "    # turn the dict into a dataframe\n",
    "    pos_df = pd.DataFrame.from_dict(pos_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pos_df.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with the original dataframe\n",
    "if False:\n",
    "    largest_group_with_pos = largest_group.join(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    largest_group_with_pos.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract edges information\n",
    "if False:\n",
    "    with open(os.path.join(output_dir_viz,'largest_group_tree_layout_with_force_atlas2_2.gexf'),\n",
    "              'r') as f:\n",
    "        tree = parseString(f.read())\n",
    "    edges_weights = pd.read_csv(''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/',\n",
    "                                         'data_viz/largest_group_tree_layout_with_force_atlas_weights2_3.csv']))\n",
    "    edges = tree.getElementsByTagName('edge')\n",
    "    edges_dict = {}\n",
    "    #{\"source\":\"n6\",\"target\":\"n51\",\"id\":\"212\",\"attributes\":{},\"color\":\"rgb(36,129,178)\",\"size\":0.06302209943532944}\n",
    "    for edge in edges:\n",
    "        edge_values = edge.attributes.items()\n",
    "        k = edge_values[0][1] #k=id\n",
    "        edges_dict[k] = {}\n",
    "        for val in edge_values[1:]:\n",
    "            if val[0] == 'weight':\n",
    "                edges_dict[k]['size'] = val[1]\n",
    "            else:\n",
    "                edges_dict[k]['old_'+val[0]] = val[1]\n",
    "        # to get the colour (synonym of community I think)\n",
    "        pos_tree = edge.getElementsByTagName('viz:color')\n",
    "        pos_values = pos_tree[0].attributes.items()\n",
    "        edges_dict[k]['color'] = 'rbg({},{},{})'.format(pos_values[0][1],pos_values[1][1],pos_values[2][1])\n",
    "        # turn source/target into nID\n",
    "        # get size = weight from pandas csv\n",
    "        if 'size' not in edges_dict[k]:\n",
    "            tmp = edges_weights[edges_weights['Id'] == int(k)]['Weight'].values[0]\n",
    "            edges_dict[k]['size'] = tmp\n",
    "    print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template to copy\n",
    "if False:\n",
    "    file_name= ''.join(['/Users/stefgarasto/Google Drive/Documents/results/NOS/data_viz/',\n",
    "                       'data_supply_chain_suite.json'])\n",
    "    with open(file_name,'r') as f:\n",
    "        supply_chain_json = json.load(f)\n",
    "    print(supply_chain_json['nodes'][0].keys())#['attributes'])\n",
    "    print(supply_chain_json['nodes'][0]['attributes'])\n",
    "    for ix in range(2):\n",
    "        print(supply_chain_json['nodes'][0]['size'])\n",
    "    print(supply_chain_json['nodes'][0]['color'])\n",
    "    A = [t['color'] for t in supply_chain_json['nodes']]\n",
    "    print('colors for nodes' ,collections.Counter(A))\n",
    "    print()\n",
    "    A = [t['color'] for t in supply_chain_json['edges']]\n",
    "    print('colors for edges' ,collections.Counter(A))\n",
    "    print()\n",
    "    A = [t['size'] for t in supply_chain_json['edges']]\n",
    "    print('nb of sizes for edges' , len(collections.Counter(A)))\n",
    "    print(supply_chain_json['edges'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_list(x):\n",
    "    #all_keywords = []\n",
    "    #x = df_line['Keywords']\n",
    "    if isinstance(x, list):\n",
    "        # I think that ik can be a collection of words separated by \";\" or \",\"\n",
    "        ik_elems0 = ' '.join([elem for elem in x if not elem.isdigit()])\n",
    "        ik_elems0 = ik_elems0.replace('-', ' ').replace(':','').replace(',',';')\n",
    "        ik_elems0 = ik_elems0.replace('(','').replace(')','')\n",
    "        ik_elems0 = ik_elems0.split(';')\n",
    "        # remove extra spaces and make lowercase\n",
    "        ik_elems0 = [elem.strip().lower() for elem in ik_elems0]\n",
    "        ik_elems0 = [elem for elem in ik_elems0 if len(elem)]\n",
    "        ik_elems = []\n",
    "        for ik_elem in ik_elems0:\n",
    "            ik_elem0 = ' '.join([elem for elem in ik_elem.split() if\n",
    "                  (not elem.isdigit()) & (elem not in stopwords) & len(elem)])\n",
    "            if len(ik_elem0):\n",
    "                ik_elems.append(ik_elem0)\n",
    "                    #print(ik_elems)\n",
    "        return [elem.strip() for elem in ik_elems if len(elem)]\n",
    "    elif isinstance(x,str):\n",
    "        #ik_elems = re.findall(r\"[\\w']+\", df.loc[ix].replace('-',''))\n",
    "        ik_elems = x.replace('-', ' ').replace(',',';')\n",
    "        ik_elems = ik_elems.replace('(','').replace(')','').split(';')\n",
    "        if len(ik_elems)==1:\n",
    "            # lacking proper separators - have to use spaces\n",
    "            ik_elems = ik_elems[0].split()\n",
    "        # remove extra spaces\n",
    "        ik_elems = [elem.strip().lower().replace('\\n','') for elem in ik_elems]\n",
    "        # remove digits\n",
    "        ik_elems = [elem for elem in ik_elems if not elem.isdigit()]\n",
    "        ik_elems = [elem for elem in ik_elems if len(elem)>1]\n",
    "        return [elem.strip() for elem in ik_elems if len(elem)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a dictionary\n",
    "if False:\n",
    "    attributes_dict= {'year_developed': 'Date_approved_year', \n",
    "                      'year': 'Date_approved_year', #,'keywords': 'Keywords' #'Clean Ind Review Year', \n",
    "                      'organisation': 'Developed By', 'title': 'NOS Title'}\n",
    "    output_dict = {'nodes':[], 'edges': []}\n",
    "    nodes_id_dict = {}\n",
    "    for ix,node in enumerate(largest_group_with_pos.index):\n",
    "        tmp_dict = {}\n",
    "        row = largest_group_with_pos.loc[node]\n",
    "        tmp_dict['label'] = row['URN'] #node\n",
    "        for label in ['x','y']:\n",
    "            tmp_dict[label] = row[label]\n",
    "        tmp_dict['id'] = 'n{}'.format(ix)\n",
    "        tmp_dict['degree'] = row['degree']\n",
    "        tmp_dict['color'] = row['color']#'rgb(36,129,178)'\n",
    "        tmp_dict['size']= 10.0\n",
    "        tmp_dict['attributes'] = {}\n",
    "        tmp_dict['attributes']['id'] = '{}'.format(ix)\n",
    "        for attribute in attributes_dict.keys():\n",
    "            tmp_dict['attributes'][attribute] = row[attributes_dict[attribute]]\n",
    "            if isinstance(tmp_dict['attributes'][attribute], float):\n",
    "                if np.isnan(tmp_dict['attributes'][attribute]):\n",
    "                    tmp_dict['attributes'][attribute] = ''\n",
    "        # extract and add keywords\n",
    "        tmp_dict['keywords'] = get_keywords_list(row['Keywords'])\n",
    "        if not tmp_dict['keywords']:\n",
    "            print(tmp_dict['keywords'],type(tmp_dict['keywords']))\n",
    "        #try:\n",
    "        #    tmp_dict['attributes']['year'] = str(int(tmp_dict['attributes']['year']))\n",
    "        #except:\n",
    "        #    tmp_dict['attributes']['year'] = ''\n",
    "        tmp_dict['attributes']['year_developed'] = tmp_dict['attributes']['year']\n",
    "        #try:\n",
    "        #    tmp_dict['attributes']['year_developed'] = str(int(tmp_dict['attributes'\n",
    "        #                                                           ]['year_developed']))\n",
    "        #except:\n",
    "        #    pass\n",
    "        tmp_dict['attributes']['file_name'] = node\n",
    "        output_dict['nodes'].append(tmp_dict)\n",
    "        # save the relationship label to id created here\n",
    "        nodes_id_dict[node] = tmp_dict['id']\n",
    "\n",
    "    # now add the edges\n",
    "    for edge_id in edges_dict.keys():\n",
    "        edge = copy.deepcopy(edges_dict[edge_id])\n",
    "        edge['id'] = edge_id\n",
    "        # transform the source/target from title to id\n",
    "        edge['source'] = nodes_id_dict[edge['old_source']]\n",
    "        edge['target'] = nodes_id_dict[edge['old_target']]\n",
    "        edge['attributes'] = {}\n",
    "        edge.pop('old_source')\n",
    "        edge.pop('old_target')\n",
    "        output_dict['edges'].append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(output_dict['nodes'][:5])\n",
    "    print()\n",
    "    print(output_dict['edges'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save the json\n",
    "SAVEJSON= False\n",
    "if SAVEJSON:\n",
    "    with open(os.path.join(output_dir_viz, 'max_tree_largest_group.json'), 'w') as fp:\n",
    "        json.dump(output_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#largest_group_with_pos['Clean Ind Review Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create clickable list of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs:\n",
    "1. Test the transfer Mac to Windows with the new strategy.\n",
    "\n",
    "    a. try removing asterisks from file names and using jsons instead of .txts    \n",
    "    b. try removing asterisks from file names while still using \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of duplicates\n",
    "duplicates_names = []\n",
    "duplicates_info = {}\n",
    "for pair_id in candidates_paired:\n",
    "    pair = candidates_paired[pair_id]\n",
    "    # get names\n",
    "    for standard in pair:\n",
    "        #print(standard)\n",
    "        duplicates_names.append(standard[2])\n",
    "        duplicates_info[standard[2]] = {}\n",
    "        duplicates_info[standard[2]]['title'] = standard[1]\n",
    "        duplicates_info[standard[2]]['URN'] = standard[0]\n",
    "        duplicates_info[standard[2]]['suite'] = df_nos.loc[standard[2]]['One_suite']\n",
    "        duplicates_info[standard[2]]['pair'] = 1\n",
    "        duplicates_info[standard[2]]['pair_id'] = pair_id\n",
    "        duplicates_info[standard[2]]['category'] = df_nos.loc[standard[2]]['NOSCategory']\n",
    "        duplicates_info[standard[2]]['validity'] = df_nos.loc[standard[2]]['Validity']\n",
    "        duplicates_info[standard[2]]['status'] = df_nos.loc[standard[2]]['Status']\n",
    "print('Done with pairs')\n",
    "for pair_id in candidates_grouped:\n",
    "    pair = candidates_grouped[pair_id]\n",
    "    # get names\n",
    "    for standard in pair:\n",
    "        #print(standard)\n",
    "        duplicates_names.append(standard[2])\n",
    "        duplicates_info[standard[2]] = {}\n",
    "        duplicates_info[standard[2]]['title'] = df_nos.loc[standard[2]]['NOS Title'] #standard[1]\n",
    "        duplicates_info[standard[2]]['URN'] = standard[0]\n",
    "        duplicates_info[standard[2]]['suite'] = df_nos.loc[standard[2]]['One_suite']\n",
    "        duplicates_info[standard[2]]['pair'] = 0\n",
    "        duplicates_info[standard[2]]['pair_id'] = pair_id\n",
    "        duplicates_info[standard[2]]['category'] = df_nos.loc[standard[2]]['NOSCategory']\n",
    "        duplicates_info[standard[2]]['validity'] = df_nos.loc[standard[2]]['Validity']\n",
    "        duplicates_info[standard[2]]['status'] = df_nos.loc[standard[2]]['Status']\n",
    "print('Done')\n",
    "\n",
    "# eliminate any nan:\n",
    "for k in duplicates_info:\n",
    "    for inner_k in duplicates_info[k]:\n",
    "        if isinstance(duplicates_info[k][inner_k],float):\n",
    "            if np.isnan(duplicates_info[k][inner_k]):\n",
    "                duplicates_info[k][inner_k] = 'NA'\n",
    "\n",
    "print('Done with removing nans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df = pd.DataFrame.from_dict(duplicates_info, orient = 'index')\n",
    "# group them by suite\n",
    "duplicates_groupby = duplicates_df.groupby('suite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_viz2 = '/Users/stefgarasto/Local-Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE THE SINGLE FILES AND THE LINKS SECTION OF THE MAIN HTML\n",
    "\n",
    "# for each suite sort them alphabetically\n",
    "txt_file = os.path.join(output_dir_viz2,'near_duplicates_list/duplicates_list_for_html_{}.txt'.format(\n",
    "                                                                                file_type))\n",
    "file_type = 'txt'\n",
    "file_type_dirs = {'txt': 'duplicates_txts', 'json': 'duplicates_jsons'}\n",
    "s_dir = file_type_dirs[file_type]\n",
    "sub_dict= {}\n",
    "with open(txt_file,'w') as f:\n",
    "    for name,group in duplicates_groupby:\n",
    "        #print(group)\n",
    "        f.write('<p> <h3> Suite: \\\"{}\\\" </h3> </p>'.format(name.capitalize()))\n",
    "        f.write('\\n')\n",
    "        group = group.sort_values('suite')\n",
    "        for s_name,standard in group.iterrows():\n",
    "            # Links section in the main html\n",
    "            \n",
    "            # version that write a file for each duplicate\n",
    "            #s_name_json = os.path.splitext(s_name.replace(' ','+'))[0]\n",
    "            #f.write('<p> [<a href=\"{}/{}.{}\" target=\"_blank\">\\\"{}\\\"</a>] \\\"{}\\\". </p>'.format(\n",
    "            #    s_dir,s_name_json,file_type,standard['URN'],standard['title'].capitalize()))\n",
    "            \n",
    "            # version that writes a file for each group of duplicates\n",
    "            s_name_json = duplicates_df.loc[s_name]['pair_id']\n",
    "            #s_done_json = []\n",
    "            f.write('<p> [<a href=\"{}/{}.{}\" target=\"_blank\">\\\"{}\\\"</a>] \\\"{}\\\". </p>'.format(\n",
    "                s_dir,s_name_json,file_type,standard['URN'],standard['title'].capitalize()))\n",
    "            f.write('\\n')\n",
    "            # get all duplicates and build the json/txt file\n",
    "            sub_file_name = os.path.join(output_dir_viz2,'near_duplicates_list/{}/{}.{}'.format(\n",
    "                                        s_dir,s_name_json,file_type))\n",
    "            \n",
    "            if standard['pair']:\n",
    "                others= candidates_paired[standard['pair_id']]\n",
    "            else:\n",
    "                others = candidates_grouped[standard['pair_id']]\n",
    "            # write the individual files\n",
    "            with open(sub_file_name,'w') as subf:\n",
    "                if file_type == 'txt':\n",
    "                    subf.write('Potential near duplicates ({} in total). '.format(len(others)))\n",
    "                    subf.write('\\n')\n",
    "                    subf.write('\\n')\n",
    "                elif file_type == 'json':\n",
    "                    main_json_key = 'Potential near duplicates ({} in total)'.format(len(others))\n",
    "                    sub_dict= {main_json_key: {}}\n",
    "                else:\n",
    "                    raise(ValueError)\n",
    "                for ix,other in enumerate(others):\n",
    "                    if file_type == 'txt':\n",
    "                        try:\n",
    "                            subf.write('{}. [\\\"{}\\\"] \\\"{}\\\". (\\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\")'.format(\n",
    "                                ix+1,other[0],other[1].capitalize(), \n",
    "                                duplicates_info[other[2]]['suite'].capitalize() ,other[2], \n",
    "                                duplicates_info[other[2]]['validity'].capitalize(), \n",
    "                                duplicates_info[other[2]]['status'].capitalize(),\n",
    "                                duplicates_info[other[2]]['category'].capitalize()))\n",
    "                        except:\n",
    "                            subf.write('{}. [\\\"{}\\\"] \\\"{}\\\". (\\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\", \\\"{}\\\")'.format(\n",
    "                                ix+1,other[0],other[1].capitalize(), name.capitalize() ,other[2], \n",
    "                                duplicates_info[other[2]]['validity'].capitalize(), \n",
    "                                duplicates_info[other[2]]['status'].capitalize(),\n",
    "                                duplicates_info[other[2]]['category'].capitalize()))\n",
    "                        subf.write('\\n')\n",
    "                    elif file_type == 'json':\n",
    "                        sub_dict[main_json_key]['Standard {}'.format(ix+1)] = {}\n",
    "                        for json_key, json_value in zip(duplicates_info[other[2]].keys(),\n",
    "                                                        duplicates_info[other[2]].values()):\n",
    "                            if 'pair' in json_key:\n",
    "                                continue\n",
    "                            if json_key != 'URN':\n",
    "                                json_value = str(json_value).capitalize()\n",
    "                                json_key = json_key.capitalize()\n",
    "                            sub_dict[main_json_key]['Standard {}'.format(ix+1)][json_key\n",
    "                                                                   ] = json_value\n",
    "                        sub_dict[main_json_key]['Standard {}'.format(ix+1)]['File name'] = other[2]\n",
    "                    else:\n",
    "                        raise(ValueError)\n",
    "                json.dump(sub_dict,subf,separators=(',',': '), indent = 4)\n",
    "\n",
    "print('Done')\n",
    "#with open('logs/output.json', 'w') as outfile:\n",
    "#     json.dump(json_data, outfile, indent=2, separators=(',', ': ')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[i*255 for i in t] for t in nesta_colours]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring transferable NOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineering = ['aeronautical engineering suite 3', 'marine engineering suite 3',\n",
    "       'explosive substances and articles', 'rail engineering',\n",
    "       'automotive engineering suite 3',\n",
    "       'mechanical manufacturing engineering suite 3 2008',\n",
    "       'marine engineering suite 2',\n",
    "       'polymer processing and related operations',\n",
    "       'engineering maintenance suite 3 2008',\n",
    "       'project control, estimating, planning and cost engineering',\n",
    "       'engineering and manufacture suite 4',\n",
    "       'engineering technical support suite 3 2009', 'down stream gas',\n",
    "       'engineering maintenance in food manufacture',\n",
    "       'land based engineering operations',\n",
    "       'performing engineering operations suite 2',\n",
    "       'engineering maintenance and installation suite 2 2008',\n",
    "       'fabrication and welding engineering suite 3',\n",
    "       'multi utility network construction',\n",
    "       'mechanical manufacturing engineering suite 2 2008',\n",
    "       'bus and coach engineering and maintenance',\n",
    "       'aeronautical engineering suite 2', 'gas network construction',\n",
    "       'building services engineering technology and project management',\n",
    "       'fabrication and welding suite 2',\n",
    "       'engineering toolmaking suite 3 2005',\n",
    "       'electrical and electronic engineering suite 3',\n",
    "       'engineering toolmaking suite 3',\n",
    "       'performing engineering operations suite 1',\n",
    "       'mechanical engineering services',\n",
    "       'electrical and electronic servicing',\n",
    "       'engineering woodworking, pattern and model making suite 3 2002',\n",
    "       'engineering technical support suite 2 2007',\n",
    "       'oil fired technical services', 'integrated systems engineering',\n",
    "       'composite engineering suite 2', 'electricity network control engineer',\n",
    "       'rail engineering signalling suite 3',\n",
    "       'maintaining plant and systems electrical',\n",
    "       'maintaining plant and systems instrument and controls',\n",
    "       'rail engineering traction and rolling stock suite 3',\n",
    "       'rail engineering telecoms suite 3', 'marine engineering',\n",
    "       'maintaining plant and systems mechanical',\n",
    "       'mechanical fire protection', 'grips and crane technicians',\n",
    "       'engineering toolmaking level 3', 'air tightness testing',\n",
    "       'plant operations extractives', 'utilities control centre operations',\n",
    "       'rail engineering telecoms suite 2',\n",
    "       'rail engineering signalling suite 2',\n",
    "       'installing plant and systems small bore tubing occupations',\n",
    "       'monitoring engineering construction activities',\n",
    "       'heating and ventilating', 'refrigeration and air conditioning',\n",
    "       'treatment processing and control in the water industry',\n",
    "       'installing plant and systems mechanical',\n",
    "       'supporting activities in engineering construction',\n",
    "       'welding plate and pipework',\n",
    "       'solar thermal photovoltaic panel installation and surveying',\n",
    "       'rail engineering overhead line equipment construction suite 2',\n",
    "       'plumbing and domestic heating',\n",
    "       'rail engineering overhead line equipment construction suite 3',\n",
    "       'installing plant and systems pipefitting', 'confined spaces',\n",
    "       'rail engineering traction and rolling stock suite 2',\n",
    "       'leakage detection and control', 'bulk explosive truck operations',\n",
    "       'electrical and electronic engineering suite 3 2004',\n",
    "       'installation, testing and commissioning of electrical systems and equipment plant',\n",
    "       'mechanical ride operation',\n",
    "       'electrotechnical services instrumentation',\n",
    "       'fabricating steel structures plating',\n",
    "       'metal decking and stud welding occupations',\n",
    "       'performing engineering operations suite 1 2006',\n",
    "       'offshore wind and marine installation and commissioning',\n",
    "       'electrotechnical services approved electrician status',\n",
    "       'engineering surveying operations',\n",
    "       'performing engineering operations suite 2 2006',\n",
    "       'land based operations', 'welding pipework',\n",
    "       'composite engineering suite 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos['NOSCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A = df_nos[df_nos['NOS Title'].map(lambda x : 'research' in x)]\n",
    "A = df_nos.loc[list(candidates.keys())]\n",
    "A = A[A['NOS Title'].map(lambda x : 'assist in' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['NOSCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[A['NOSCategory']=='job specific'][['NOS Title','One_suite','tagged_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in candidates['semret215.pdf']:\n",
    "    print(k[1])\n",
    "    print(df_nos['One_suite'].loc[k[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in candidates_paired:\n",
    "    all_suites = []\n",
    "    in_engineering = []\n",
    "    for p in candidates_paired[k]:\n",
    "        all_suites.append(df_nos['One_suite'].loc[p[2]])\n",
    "        in_engineering.append(all_suites[-1] in engineering)\n",
    "    if len(set(all_suites))>1:\n",
    "        out = process.extract(all_suites[0],all_suites[1:])\n",
    "        if (out[0][1]<80) and sum(in_engineering):\n",
    "            print(k, candidates_paired[k])#[k][0][1], p[1])\n",
    "            counter+=1\n",
    "    #if k == 'Pair 104':\n",
    "    #    print(candidates_paired[k])\n",
    "    #if counter>25:\n",
    "    #    break\n",
    "print(counter)\n",
    "print('*'*70)\n",
    "counter2 = 0\n",
    "for k in candidates_paired:\n",
    "    all_suites = []\n",
    "    in_engineering = []\n",
    "    for p in candidates_paired[k]:\n",
    "        all_suites.append(df_nos['One_suite'].loc[p[2]])\n",
    "        in_engineering.append(all_suites[-1] in engineering)\n",
    "    if len(set(all_suites))>1:\n",
    "        out = process.extract(all_suites[0],all_suites[1:])\n",
    "        if (out[0][1]>80) and sum(in_engineering):\n",
    "            print(k, candidates_paired[k])#[k][0][1], p[1])\n",
    "            counter2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.loc['cfabes004.pdf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #pos_from_viz = [t for t in node.toxml().split('<viz:') if t[:8]=='position'][0] \n",
    "    ##node.toxml().split('<viz:position')[2]\n",
    "    #pos_from_viz = pos_from_viz.split()\n",
    "    #print(pos_from_viz)\n",
    "    #tmp = re.findall('[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?',pos_from_viz[1])\n",
    "    #if '-' in pos_from_viz[1]:\n",
    "    #    pos_dict[k]['x'] = -float(tmp[0][0])\n",
    "    #else:\n",
    "    #    pos_dict[k]['x'] = float(tmp[0][0])\n",
    "    #tmp = re.findall('[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?',pos_from_viz[2])\n",
    "    #if '-' in pos_from_viz[2]:\n",
    "    #    pos_dict[k]['y'] = -float(tmp[0][0])\n",
    "    #else:\n",
    "    #    pos_dict[k]['y'] = float(tmp[0][0])\n",
    "    #break\n",
    "#print(node.toxml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
