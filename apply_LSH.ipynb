{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# LSH imports\n",
    "from datasketch import MinHashLSHEnsemble, MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the script is set up so that data is a DataFrame with the texts to be compared in the \"full_text\" column\n",
    "# data = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(text, char_ngram=5):\n",
    "    '''\n",
    "    This function splits strings into continuous sets of characters of length n. In the current example n = 5.\n",
    "    '''\n",
    "    if len(text) == 5:\n",
    "        res = set([text, text])\n",
    "    else:\n",
    "        res = set(text[head:head + char_ngram] \\\n",
    "               for head in range(0, len(text) - char_ngram))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shingles\n",
    "t0 = time.time()\n",
    "shingled_desc = [shingles(desc) for desc in data['full_text']]\n",
    "print_elapsed(t0, 'splitting the text into groups of characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding shingles\n",
    "t0 = time.time()\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    for d in desc:\n",
    "        hash_objects[ix].update(d.encode('utf8'))\n",
    "print_elapsed(t0, 'encoding hash objects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a unique label to the hash objects - the label corresponds to the DataFrame index\n",
    "content = []\n",
    "standard_labels = list(data.index.values)\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    content.append((standard_labels[ix], hash_objects[ix]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSH and Jaccard similarity threshold\n",
    "# the threshold needs to be set in advance, in can't be changed later\n",
    "LSH_th = 0.8\n",
    "lsh = MinHashLSH(threshold=LSH_th, num_perm=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the objects created (label + hash encoding) into the lsh object\n",
    "for ix,elem in enumerate(content):\n",
    "    lsh.insert(elem[0], elem[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each data point search all signatures and identify potential clashes (e.g. other data points\n",
    "# with Jaccard similarity of shingle sets greater or equal to the threshold). \n",
    "# Note: some of the candidates might be false positives (I think around at least 10% but it depends on the use case).\n",
    "candidates = {}\n",
    "singletons = []\n",
    "for ix, desc in enumerate(shingled_desc):\n",
    "    result = lsh.query(hash_objects[ix])\n",
    "    # if len(result)==1, no near-duplicate has been found\n",
    "    if len(result)>1:\n",
    "        # save results in a dictionary\n",
    "        candidates[standard_labels[ix]] = result\n",
    "        # print some random examples\n",
    "        if np.random.randn()>3:\n",
    "            print(standard_labels[ix], ': ', result)\n",
    "            print('***************')\n",
    "    else:\n",
    "        singletons.append(standard_labels[ix])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of NOS that found at least a match = total NOS - number of singletons\n",
    "print('Nb. of data points that were not matched with anything: {}'.format(len(singletons)))\n",
    "print('Nb. of data point that matched: {}'.format(len(data) - len(singletons)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part below can be ignored if not needed \n",
    "Here, the less clean part starts (most of the code above was from Jyl).\n",
    "The following sections try to group all the near-duplicates into non overlapping group.\n",
    "It's done by starting from one pair and then adding all the other data points that are connected \n",
    "It's based on the assumption that there are non-overlapping groups. That is, at some point the chain will stop\n",
    "because no new data points need to be added. Otherwise it'll just create very large groups.\n",
    "I think George has a more developed community algorithm detection that can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the adjacency matrix\n",
    "Nmatched = len(data) - len(singletons)\n",
    "Adj_matrix = np.zeros((Nmatched,Nmatched))\n",
    "\n",
    "t0 = time.time()\n",
    "# create dictionary of indices\n",
    "indices = {}\n",
    "indices_reverse = {}\n",
    "for ix, candidate in enumerate(candidates):\n",
    "    indices[candidate] = ix\n",
    "    indices_reverse[ix] = candidate\n",
    "# now cycle again through the matched data points and populate the adjacency matrix\n",
    "for idx1, candidate in enumerate(candidates):\n",
    "    for k in candidates[candidate]:\n",
    "        # now this is a list of tuples, where the first element is the urn label\n",
    "        idx2 = indices[k[2]]\n",
    "        Adj_matrix[idx1,idx2] = 1\n",
    "\n",
    "print_elapsed(t0,'creating the adjacency matrix')\n",
    "#plt.figure(figsize = (10,10))\n",
    "#plt.imshow(Adj_matrix[:50,:50])\n",
    "print('The highest degree in the adjacency matrix is: ', np.max(np.sum(Adj_matrix,axis=1)))\n",
    "print('The number of matched couples are: ', np.sum(np.sum(Adj_matrix, axis = 1)==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the NOS that were matched as similar\n",
    "t0 = time.time()\n",
    "matched_groups = []\n",
    "matched_indices = []\n",
    "for ix in range(Adj_matrix.shape[0]):\n",
    "    idx_used = []\n",
    "    # find the adjacent nodes\n",
    "    where_list = list(np.where(Adj_matrix[ix])[0])\n",
    "    where_list_cumul = []\n",
    "    # don't use indices already matched\n",
    "    if ix not in matched_indices:\n",
    "        for ix2 in where_list:\n",
    "            # if the neighborhood has connections to indices that we haven't included yet in the current list, \n",
    "            # add them to the list to be analysed later\n",
    "            where_list_cumul += list(np.where(Adj_matrix[ix2])[0])\n",
    "            idx_used.append(ix2)\n",
    "            # grow the neighbourhood by adding the new connections\n",
    "            new_list = [t for t in where_list_cumul if t not in where_list]\n",
    "            if len(new_list):\n",
    "                # if the length is zero it means there are no new connected nodes\n",
    "                where_list+=new_list\n",
    "        # add the group just found if and only if the neighbourhood is self-contained, \n",
    "        # that is if the nodes for which we have collected the neighbours are the same\n",
    "        # that appeared in the combined neighbourhoods\n",
    "        if set(idx_used) == set(where_list):\n",
    "            matched_groups.append(tuple(idx_used))\n",
    "            matched_indices += idx_used\n",
    "print_elapsed(t0, 'grouping the similar data points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
