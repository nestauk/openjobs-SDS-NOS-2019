{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "import re\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster','ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions and snippets of code that are useful for analysing text. Most of the techniques used are unsupervised. Functions are defined up front and then used in sections below.\n",
    "\n",
    "This notebook is to apply:\n",
    "- Tokenizers (based on n-grams and 'as_is')\n",
    "- Calculating distance\n",
    "- Hierarchical clustering and plotting\n",
    "- K-means clustering\n",
    "- LSH\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i) if i not in keep_asis else i for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_with_pos(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i, pos_to_wornet_dict[p]) if i not in keep_asis else i for i,p in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def lemmatise_pruned(x, pofs = 'nv'):\n",
    "    if pofs == 'nv':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['V','N']]\n",
    "    elif pofs == 'n':\n",
    "        tags = [(t,p) for t,p in x if p[:1] in ['N']]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return lemmatise_with_pos(tags)\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    #nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of functions is useful for identifying terms with highest tf-idf weights \n",
    "#in a single document or set of documents\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding \n",
    "        feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, sparse_output = False):\n",
    "    ''' Return the top n features that on average are most important \n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    if sparse_output:\n",
    "        return scipy.sparse.csr_matrix(top_tfidf_feats(tfidf_means, features, top_n))\n",
    "    else:\n",
    "        return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def all_mean_feats(Xtr, grp_ids=None, min_tfidf=0.1):\n",
    "    ''' Return the average\n",
    "        amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return tfidf_means\n",
    "\n",
    "def get_top_words_weights(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words\n",
    "\n",
    "def get_mean_tfidf(desc, vect):\n",
    "    response = vect.transform(desc)\n",
    "    tfidf_values = all_mean_feats(response, grp_ids = None)\n",
    "    return tfidf_values\n",
    "\n",
    "def get_top_words(desc, feature_names, vect, n = 25):\n",
    "    response = vect.transform(desc)\n",
    "    words = top_mean_feats(response, feature_names, grp_ids = None, top_n = n)\n",
    "    return words['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse html\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "#HTML Parser Methods\n",
    "#Initializing lists\n",
    "    lsData = list()\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        self.lsData.append(data)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return ''.join(self.lsData)\n",
    "\n",
    "           \n",
    "def strip_tags(some_html):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes html tags.\n",
    "    Returns a string.\n",
    "    \"\"\"\n",
    "    s = MyHTMLParser()\n",
    "    s.lsData = list()\n",
    "    s.feed(some_html)\n",
    "    data = s.get_data()\n",
    "    s.reset\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up / load relevant parameters\n",
    "#from set_params_thematic_groups import qualifier, qualifier0, pofs, WHICH_GLOVE, \n",
    "#from set_params_thematic_groups import glove_dir, paramsn\n",
    "\n",
    "qualifier = 'postjoining_final_no_dropped'\n",
    "qualifier0 = 'postjoining_final_no_dropped'\n",
    "pofs = 'n'\n",
    "\n",
    "WHICH_GLOVE = 'glove.6B.100d' #'glove.6B.100d' #'glove.840B.300d', \n",
    "#glove.twitter.27B.100d\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/'\n",
    "\n",
    "paramsn = {}\n",
    "paramsn['ngrams'] = 'uni'\n",
    "paramsn['pofs'] = 'nv'\n",
    "paramsn['tfidf_min'] = 3\n",
    "paramsn['tfidf_max'] = 0.4\n",
    "\n",
    "paramsn['bywhich'] = 'docs' #'docs' #'suites'\n",
    "paramsn['mode'] = 'tfidf' #'tfidf' #'meantfidf' #'combinedtfidf' #'meantfidf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n",
    "output_dir += 'nos_clusters_{}_final_no_dropped'.format(pofs)\n",
    "print(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/'\n",
    "\n",
    "LOADGLOVE = True\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.{}.txt'.format(WHICH_GLOVE)))\n",
    "    # get vocabulary\n",
    "    vector_matrix = model.vectors\n",
    "    list_of_terms = model.index2word\n",
    "    lookup_terms = [convert_from_undersc(elem) for elem in list_of_terms]\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data for approved apprenticeship standards from api\n",
    "#r2 = requests.get(\"https://www.instituteforapprenticeships.org/api/fullstandards/\")\n",
    "#df_api= pd.DataFrame(r2.json())\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier0))\n",
    "\n",
    "# load the cleaned and tokenised dataset\n",
    "df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs)))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove \"k\"s and \"p\"s from the pruned columns\n",
    "def remove_pk(x):\n",
    "    return [t for t in x if t not in ['k','p']]\n",
    "df_nos['pruned'] = df_nos['pruned'].map(remove_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another column where the texts are lemmatised properly\n",
    "t0 = time.time()\n",
    "df_nos['pruned_lemmas'] = df_nos['tagged_tokens'].map(lambda x: lemmatise_pruned(x,pofs))\n",
    "print(time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.sample(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'rb') as f:\n",
    "    stopwords0, no_idea_why_here_stopwords, more_stopwords = pickle.load(f)\n",
    "stopwords = stopwords0 + no_idea_why_here_stopwords \n",
    "stopwords += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„'])\n",
    "stopwords0 += tuple(['¤', '¨', 'μ', 'บ', 'ย', 'ᶟ', '‰', '©', 'ƒ', '°', '„',\"'m\", \"'re\", '£',\n",
    "                    '&', '1', '@'])\n",
    "stopwords0 += tuple(set(list(df_nos['Developed By'])))\n",
    "stopwords0 += tuple(['cosvr'])\n",
    "# more stopwords from some suites of interest\n",
    "stopwords0 += tuple(['pdf','dc','db','gov','auspex','december','wa','non','go','get','ask',\n",
    "                    'thing','ha','hm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define more functions on how to create the TfIdf vectoriser and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create your TFidfVectorizer model. This doesn't depend on whether it's used on suites or NOS. However,\n",
    "# it does require that the docs collection is already given as a collection of tokens (tokenizer=tokenize_asis)\n",
    "\n",
    "#Since we now have not just long strings in our documents, but lists of terms, we will use a different tokenizer\n",
    "def define_tfidf(params, stopwords):\n",
    "    if params['ngrams'] == 'bi':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,2), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    elif params['ngrams'] == 'tri':\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                ngram_range=(1,3), \n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    else:\n",
    "        # unigrams is the default\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize_asis,\n",
    "                                lowercase = False,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = params['tfidf_max'], \n",
    "                                min_df = params['tfidf_min'])\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, collect the text to transform\n",
    "def combine_nos_text(df_nos, col = 'pruned'):\n",
    "    all_joint_tokens = []\n",
    "    # group by suites and concatenate all docs in it\n",
    "    row_names = []\n",
    "    for name, group in df_nos.groupby('One_suite'):\n",
    "        row_names.append(name)\n",
    "        joint_tokens = []\n",
    "        for idoc in group[col].index:\n",
    "            joint_tokens += group[col].loc[idoc]\n",
    "        all_joint_tokens.append(joint_tokens)\n",
    "    # return a dataframe\n",
    "    return pd.DataFrame({'tokens': all_joint_tokens}, index = row_names)\n",
    "\n",
    "def get_tfidf_matrix(params, df_nos, tfidf, col = 'pruned'):\n",
    "    # Note: this can simply be used to get the tfidf transform, by setting bywhich=docs and any mode\n",
    "    t0 = time.time()\n",
    "    # first, get the dataframe of tokens\n",
    "    if params['bywhich'] == 'docs':\n",
    "        textfortoken = df_nos[col]\n",
    "        \n",
    "    elif params['bywhich'] == 'suites':\n",
    "        if params['mode'] == 'meantfidf':\n",
    "            textfortoken = df_nos[col]\n",
    "                \n",
    "        elif params['mode'] == 'combinedtfidf':\n",
    "            # note that this is the only case where the tfidf min and max are computed considering the number of \n",
    "            # suites as the number of elements in the collection.\n",
    "            # TODO: allow for the alternative case, where the transform is computed on individual NOS and then \n",
    "            # applied to the joint tokens\n",
    "            textfortoken = combine_nos_text(df_nos, col)['tokens']\n",
    "    \n",
    "    # apply tfidf transform to the tokenised text\n",
    "    tfidfm = tfidf.fit_transform(textfortoken)\n",
    "    \n",
    "    # if the average is needed, compute it and overwrite the matrix. Note that the step above is still needed to\n",
    "    # initialise the tfidf transform with the proper features and stopwords\n",
    "    if (params['bywhich'] == 'suites') and (params['mode'] =='meantfidf'):\n",
    "        row_names = df_nos['One_suite'].value_counts().index.values\n",
    "        tfidfm = scipy.sparse.lil_matrix(np.zeros((len(row_names),len(feature_names)), dtype = np.float32))\n",
    "        for name, group in df_nos.groupby('One_suite'):\n",
    "            tmp = get_mean_tfidf(group['pruned'], tfidf)\n",
    "            tfidfm[igroup] = tmp\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    print_elapsed(t0, 'computing the feature vector')\n",
    "    return tfidfm, feature_names, tfidf, textfortoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the file with the list of super-suites and match the suites listed inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_suites_files=  '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/NOS_Suite_Priority.xlsx'\n",
    "super_suites_names = ['Engineering','Management','FinancialServices','Construction']\n",
    "all_super_suites = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_super_suites[which_super_suite] = pd.read_excel(super_suites_files, sheet_name = which_super_suite)\n",
    "    all_super_suites[which_super_suite]['NOS Suite name'] = all_super_suites[which_super_suite]['NOS Suite name'].map(\n",
    "        lambda x: x.replace('(','').replace('(','').replace('&','and').strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_labels = list(df_nos.groupby('One_suite').groups.keys())\n",
    "all_matches = {}\n",
    "all_match_names = {}\n",
    "#match_name = []\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_matches[which_super_suite] = []\n",
    "    for suite in all_super_suites[which_super_suite]['NOS Suite name'].values:\n",
    "        # do manually some selected suites\n",
    "        if 'insurance claims' in suite:\n",
    "            tmp = standard_labels.index('general insurance')\n",
    "            all_matches[which_super_suite].append(tmp)\n",
    "            continue\n",
    "        # for the \"management and leadership marketing 2013\" both marketing and marketing 2013 would fit,\n",
    "        # but I'm only taking the latter\n",
    "        # find a fuzzy match between \n",
    "        out = process.extract(suite, standard_labels, limit=3)\n",
    "        if len(out) and out[0][1]>89:\n",
    "            # note: most of them are above 96% similarity (only one is 90%)\n",
    "            tmp = standard_labels.index(out[0][0])\n",
    "            #print(suite, out[0])\n",
    "            if tmp not in all_matches[which_super_suite]:\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "            else:\n",
    "                if suite == 'installing domestic fascia, soffit, and bargeboards':\n",
    "                    # this suite is kind of a duplicate - I aggregated it in my suites list\n",
    "                    continue\n",
    "                tmp = standard_labels.index(out[2][0])\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "                print(out[0][0],',',out[1][0],',',out[2][0],',',suite)\n",
    "        else:\n",
    "            print(suite, ' not found')\n",
    "            print(out)\n",
    "            print('\\n')\n",
    "    print(len(all_matches[which_super_suite]),len(all_super_suites[which_super_suite]))\n",
    "    all_match_names[which_super_suite] = [standard_labels[t] for t in all_matches[which_super_suite]]\n",
    "    #print(super_suites['NOS Suite name'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships between standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import fcluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we only want to cluster one suite\n",
    "#steel_construction = df_nos[df_nos['One_suite'] == 'steelfix construction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEHC = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate cosine distance between tf-idf vectors of the documents\n",
    "\n",
    "def do_hierarch_clustering(tfidfm, DOPLOTS = True):\n",
    "    t0 = time.time()\n",
    "    N2 = 11914\n",
    "    N = 400 #400*400 = 160000 distance calls per second. For N=21500 -- > 462250000 calls --> 2900*160000 calls \n",
    "    # --> I'm guessing 2900 seconds = 48 minutes (I think it's likely to be more actually)\n",
    "    # 4000*4000 takes approximately 110 seconds. It's double for the cophenet. So, for N=22500, the three functions \n",
    "    # together will take approx 4 hours (I'll do it tonight)\n",
    "\n",
    "    try:\n",
    "        distances = distance.pdist(tfidfm.todense(), metric = 'cosine') # + np.random.randn(N,N2), metric = 'cosine')\n",
    "        sparse_flag = True\n",
    "    except:\n",
    "        distances = distance.pdist(tfidfm, metric = 'cosine')\n",
    "        sparse_flag = False\n",
    "    print_elapsed(t0, 'calculating cosine distances of tfidf vectors')\n",
    "\n",
    "    #We then build linkage matrix using the distances and specifying the method. For euclidean distances typically\n",
    "    # 'Ward' produces best results. For cosine we can only use 'average' and 'single'.\n",
    "    linkage_matrix = scipy.cluster.hierarchy.linkage(distances,\n",
    "                                                     method = 'average',\n",
    "                                                     metric = 'cosine')\n",
    "    print_elapsed(t0, 'hierarchical clustering of cosine distances')\n",
    "    #We can test how well the groupings reflect actual distances. If c > 0.75 this is considered to be sufficiently\n",
    "    #good representation\n",
    "    if sparse_flag:\n",
    "        c, coph_dists = cophenet(linkage_matrix, \n",
    "                             distance.pdist(tfidfm.todense(), metric = 'cosine'))\n",
    "    else:\n",
    "        c, coph_dists = cophenet(linkage_matrix, \n",
    "                             distance.pdist(tfidfm, metric = 'cosine'))\n",
    "\n",
    "    print_elapsed(t0, 'computing the cophenetic correlation')\n",
    "\n",
    "    if DOPLOTS:\n",
    "        fig, ax =plt.subplots(figsize = (5,5))\n",
    "        plt.imshow(scipy.spatial.distance.squareform(distances))\n",
    "        plt.title('cosine distances between suites')\n",
    "        plt.colorbar()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (5,5))\n",
    "        tmp = plt.imshow(scipy.spatial.distance.squareform(coph_dists))\n",
    "        plt.colorbar()\n",
    "    print('The cophenetic coefficient is {:.4f}'.format(c))\n",
    "    return distances, linkage_matrix, c, coph_dists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters for features extraction\n",
    "\n",
    "ngrams : uni/bi/tri\n",
    "\n",
    "tfidf thresholds: min and max percentage\n",
    "\n",
    "which parts of speech were selected before\n",
    "\n",
    "whether we are working at the level of suites or of invidual NOS, and how we aggregate NOS to form the suit level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hierarchical clustering on all NOS from one super-suite at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_supersuite(x):\n",
    "    for supersuite in all_match_names.keys():\n",
    "        if x in all_match_names[supersuite]:\n",
    "            return supersuite.lower()\n",
    "    # if no match has been found\n",
    "    return 'other'\n",
    "\n",
    "def adjustsoccode(x):\n",
    "    y = re.findall(r\"[\\d']+\", str(x))\n",
    "    if len(y):\n",
    "        return y[0][1:-1]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract2digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:2])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract3digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:3])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract1digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:1])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract4digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant dataset\n",
    "df_nos['supersuite'] = df_nos['One_suite'].apply(assign_supersuite)\n",
    "df_nos_select = df_nos[~(df_nos['supersuite']=='other')]\n",
    "print(len(df_nos_select))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some more things\n",
    "df_nos_select['SOC4str'] = df_nos_select['Clean SOC Code'].map(adjustsoccode)\n",
    "df_nos_select['SOC4'] = df_nos_select['SOC4str'].map(extract4digits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical cluster of NOS in suites clusters\n",
    "For selected clusters of suite, perform hierarchical clustering of the NOS inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subdf(SELECT_MODE, clusters2use, suites_clusters, df_nos_select):\n",
    "    if isinstance(SELECT_MODE, str):\n",
    "        tmp_dict = {'engineering': 'Engineering', 'management': 'Management',\n",
    "                    'financialservices': 'Financial services', \n",
    "                    'construction': 'Construction'}\n",
    "        # select NOS from super suite\n",
    "        cluster_name = SELECT_MODE\n",
    "        cluster_name_save = cluster_name\n",
    "        cluster_name_figs = tmp_dict[SELECT_MODE]\n",
    "        subset_nos = df_nos_select[df_nos_select['supersuite']== SELECT_MODE]\n",
    "    elif isinstance(SELECT_MODE, int):\n",
    "        cluster_name = clusters2use[SELECT_MODE][1]\n",
    "        cluster_name_save = cluster_name.replace(' ','_')\n",
    "        cluster_name_figs = cluster_name.capitalize()\n",
    "        suites2use = list(suites_clusters[suites_clusters['hierarchical'].map(\n",
    "                lambda x: x in clusters2use[SELECT_MODE][0])]['Suite_names'].values)\n",
    "        subset_nos = df_nos_select[df_nos_select['One_suite'].map(\n",
    "                lambda x: x in suites2use)]\n",
    "    print('Number of NOS selected: ', len(subset_nos))\n",
    "    #print(subset_nos.columns)\n",
    "    \n",
    "    #%\n",
    "    # only select those engineering nos with SOC codes\n",
    "    nosoc = subset_nos['SOC4'].isnull()\n",
    "    print('percentage of nos without SOC codes: ', nosoc.sum()/len(nosoc))\n",
    "    if (nosoc.sum())/len(nosoc)<0.9:\n",
    "        final_nos = subset_nos[~nosoc] #np.isnan(engineering_nos['SOC4'])]\n",
    "    else:\n",
    "        final_nos = subset_nos\n",
    "    final_groups = final_nos.groupby(by = 'One_suite')\n",
    "    larger_suites = []\n",
    "    all_lengths = final_groups.agg(len)['NOS Title'].values\n",
    "    all_lengths[::-1].sort()\n",
    "    print('Number of NOS in suites belonging to this cluster: ',all_lengths)\n",
    "    #th_supers = ['engineering': 40, 'financialservices': ]\n",
    "    for name, group in final_groups:\n",
    "        if isinstance(SELECT_MODE, int):\n",
    "            larger_suites.append(name)\n",
    "        elif len(group)> all_lengths[15]:#th_supers[SELECT_MODE]:\n",
    "            #print(name, len(group))\n",
    "            larger_suites.append(name)\n",
    "\n",
    "    return final_nos, final_groups, larger_suites, cluster_name,  \\\n",
    "                    cluster_name_save, cluster_name_figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the tfidf transform on all NOS\n",
    "'''\n",
    "\n",
    "# get the transform tfidf\n",
    "tfidf_n = define_tfidf(paramsn, stopwords0)\n",
    "\n",
    "# get the matrix again (even though if the parameters stay the same, this one is the \n",
    "# same still)\n",
    "# this is to fit the transform on the entire NOS corpus (from supersuites only?)\n",
    "_, feature_names_n, tfidf_n, _ = get_tfidf_matrix(paramsn, df_nos, #df_nos_select, \n",
    "                                                        tfidf_n, col= 'pruned_lemmas')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute averaged word embeddings for NOS in supersuites\n",
    "'''\n",
    "\n",
    "# first transform via tfidf all the NOS in one supersuite because you need the top keywords\n",
    "textfortoken = df_nos_select['pruned_lemmas']\n",
    "tfidfm = tfidf_n.transform(textfortoken)\n",
    "\n",
    "top_terms_dict = {}\n",
    "top_weights_dict = {}\n",
    "top_keywords_dict = {}\n",
    "#for name, group in ifa_df.groupby('Route'):\n",
    "igroup = 0\n",
    "n_keywords =[]\n",
    "n_repeated = []\n",
    "#top_terms = {}\n",
    "t0 = time.time()\n",
    "tfidfm_dense = tfidfm.todense()\n",
    "for ix,name in enumerate(df_nos_select.index):\n",
    "    #top_terms = get_top_words(df_nos_select.loc[name]['pruned'], feature_names, tfidf, n = 20)\n",
    "    top_ngrams = np.argsort(tfidfm_dense[ix,:])\n",
    "    top_ngrams = top_ngrams.tolist()[0][-20:]\n",
    "    top_ngrams = top_ngrams[::-1]\n",
    "    # only retain the ones with non zero features\n",
    "    top_ngrams = [elem for elem in top_ngrams if tfidfm_dense[ix,elem]>0]\n",
    "    top_weights = [tfidfm_dense[ix,elem] for elem in top_ngrams]\n",
    "    top_features = [feature_names[elem] for elem in top_ngrams]\n",
    "    top_terms_dict[name] = {}\n",
    "    top_terms_dict[name] = top_features\n",
    "    top_weights_dict[name] = {}\n",
    "    top_weights_dict[name] = top_weights\n",
    "    if ix<4:\n",
    "        print(name, top_features) #, top_keywords)\n",
    "        print('**************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to check results\n",
    "'''\n",
    "print(list(top_terms_dict.keys())[885:887])\n",
    "top_terms_weights = get_top_words_weights([df_nos_select.iloc[0]['pruned_lemmas']], \n",
    "        feature_names, tfidf, n = 20)\n",
    "print(top_terms_weights.sort_values(by = 'tfidf', ascending = False).head(n=20))\n",
    "'''\n",
    "# note that the get_top_words_weights function is probably wrong - but it doesn't matter now\n",
    "print('not now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove top terms that are not in the chosen gensim model\n",
    "new_top_terms_dict = {}\n",
    "new_top_weights_dict = {}\n",
    "for k,v in top_terms_dict.items():\n",
    "    # check if the top terms for each document are in the gensim model\n",
    "    new_top_terms, weights = prep_for_gensim(v, model, weights = top_weights_dict[k])\n",
    "    # only retains the ones in the model\n",
    "    new_top_terms_dict[k] = new_top_terms\n",
    "    new_top_weights_dict[k] = weights\n",
    "    if np.random.randn(1)>3.5:\n",
    "        print(k, new_top_terms, len(new_top_terms), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Link each NOS to a skill cluster\n",
    "'''\n",
    "\n",
    "st_v_clus = {}\n",
    "st_v_clus2 = {}\n",
    "counter = 0\n",
    "for ix,k in enumerate(new_top_terms_dict):\n",
    "    weights = 1\n",
    "    test_skills, full_test_skills  = get_mean_vec(new_top_terms_dict[k], model, \n",
    "                                                  weights = new_top_weights_dict[k])\n",
    "    tmp_clus = []\n",
    "    for iskill in range(full_test_skills.shape[0]):\n",
    "        tmp_clus.append(highest_similarity(full_test_skills[iskill], comparison_vecs, clus_names)) \n",
    "    tmp_clus = Counter(tmp_clus).most_common()\n",
    "    if tmp_clus[0][1]>1:\n",
    "        st_v_clus[k] = tmp_clus[0][0]\n",
    "    else:\n",
    "        st_v_clus[k] = highest_similarity(test_skills, comparison_vecs, clus_names)\n",
    "    st_v_clus2[k] = highest_similarity(test_skills, comparison_vecs, clus_names)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# add the best clusters to the nos dataframe\n",
    "tmp = pd.DataFrame.from_dict(st_v_clus, orient = 'index')\n",
    "tmp = tmp.rename(columns = {0: 'best_cluster_nos'})\n",
    "df_nos_select['best_cluster_nos'] = tmp['best_cluster_nos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform hierarchical clustering on each super-suite separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# super-suites:\n",
    "SELECT_MODES = ['engineering','management','construction','financialservices']\n",
    "\n",
    "SAVEHC = True\n",
    "for SELECT_MODE in SELECT_MODES[:1]:\n",
    "    df_nos_n, final_groups, larger_suites, cluster_name, cluster_name_save, \\\n",
    "                cluster_name_figs = select_subdf(SELECT_MODE, clusters2use, \n",
    "                                                 suites_clusters,df_nos_select)\n",
    "    print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "    # remove legacy nos\n",
    "    print('nb with legacy nos: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n['NOS Title'].map(lambda x: 'legacy' not in x)]\n",
    "    print('nb without legacy nos 1: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n.index.map(lambda x: not x[-5:]=='l.pdf')]\n",
    "    print('nb without legacy nos 2: ',len(df_nos_n))\n",
    "    suites_in_clus = {}\n",
    "    groups_clus = df_nos_n.groupby('One_suite')\n",
    "    for name, group in groups_clus:\n",
    "        suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "    # this is to get the restricted corpus (to transform, not for fitting)\n",
    "    textfortoken = df_nos_n['pruned_lemmas']\n",
    "    tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "\n",
    "    # get labels\n",
    "    if paramsn['bywhich'] == 'suites':\n",
    "        standard_labels_n = list(df_nos_n.groupby('One_suite').groups.keys())\n",
    "    else:\n",
    "        standard_labels_n = list(df_nos_n['NOS Title'].values)\n",
    "\n",
    "    for ix,t in enumerate(standard_labels_n):\n",
    "        if len(t)>500:\n",
    "            # manual correction because of pdf extraction\n",
    "            standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "\n",
    "    # check best features in a few NOS\n",
    "    for s_idx in range(1): #34):\n",
    "        s_idx = 0#standard_labels_n.index(\n",
    "            #'lift and move permanent way materials, components and equipment')\n",
    "        TF= tfidfm_n[s_idx,:].T.todense()\n",
    "        print(standard_labels_n[s_idx])\n",
    "        ix = np.argsort(TF, axis = 0)\n",
    "        for i in ix[-20:][::-1]: #enumerate(feature_names_n):\n",
    "            i = np.array(i)\n",
    "            print(feature_names_n[i[0][0]],np.around(TF[i[0][0]][0,0],3))\n",
    "        print()\n",
    "\n",
    "\n",
    "    # perform hierarchical clustering\n",
    "    distances_n, linkage_matrix_n, c_n, _ = do_hierarch_clustering(tfidfm_n, DOPLOTS= False)\n",
    "\n",
    "\n",
    "    # Plotting the distance between successive clusters: is there a knee?\n",
    "    z = linkage_matrix_n[::-1,2]\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    plt.plot(range(1, len(z)+1), z)\n",
    "    knee = np.diff(z, 2)\n",
    "    plt.plot(range(2, len(linkage_matrix_n)), knee)\n",
    "    plt.xlabel('partition')\n",
    "    plt.ylabel('cluster distance')\n",
    "    plt.title(cluster_name_figs)\n",
    "    goodness = []\n",
    "    for i in range(3,len(z)-2):\n",
    "        a1 = scipy.stats.linregress(range(1,i+1), z[:i])\n",
    "        a2 = scipy.stats.linregress(range(i, len(z)), z[i:])\n",
    "        goodness.append(np.around(a1[2]**2 + a2[2]**2,4))\n",
    "    plt.figure(figsize = (6,6))\n",
    "    #print(goodness)\n",
    "    plt.plot(np.arange(3,len(z)-2),goodness)\n",
    "    plt.title(cluster_name_figs)\n",
    "    ixg = np.array(goodness).argmax()+3\n",
    "    print('best t-point: ',ixg)\n",
    "\n",
    "    num_ideal = np.ceil(len(df_nos_n)/10)\n",
    "    print('The ideal number of clusters would be: ',num_ideal)\n",
    "    num_clust1 = knee.argmax() + 2\n",
    "    knee[knee.argmax()] = 0\n",
    "    num_clust2 = knee.argmax() + 2\n",
    "    \n",
    "    if num_clust1 == 2:\n",
    "        num_clust = num_clust2 #2000\n",
    "    elif num_clust2 == 2:\n",
    "        num_clust = num_clust1 #2000\n",
    "    else:\n",
    "        if SELECT_MODE == 'engineering':\n",
    "            num_clust = max([num_clust1,num_clust2]) #clusters2use[SELECT_MODE][2]\n",
    "        else:\n",
    "            num_clust = min([num_clust1,num_clust2])\n",
    "            \n",
    "    print('The two peaks are, in order: ',num_clust1, num_clust2)\n",
    "    print('The selected num clust is ',num_clust)\n",
    "    #num_clust = max([num_clust1,num_clust2])\n",
    "\n",
    "    for t in np.arange(0,1,0.05):\n",
    "        labels_n = fcluster(linkage_matrix_n, t, criterion='distance')\n",
    "        n_clust = len(set(labels_n))\n",
    "        if n_clust <= num_clust:\n",
    "            cutting_th_n = t\n",
    "            break\n",
    "    print('cutting threshold: {}'.format(cutting_th_n))       \n",
    "    \n",
    "    #Plot the dendrogram (cutting at threshold)\n",
    "    #cutting_th_n = 0.6\n",
    "    h = .05*len(df_nos_n)\n",
    "    fig, ax = plt.subplots(figsize=(28, h)) # set size\n",
    "    ax = dendrogram(linkage_matrix_n, \n",
    "                    labels = [t.capitalize() for t in standard_labels_n], \n",
    "                    orientation = 'right', \n",
    "                    leaf_font_size=6,\n",
    "                   color_threshold = cutting_th_n,\n",
    "                   truncate_mode = 'level', p =15)#,\n",
    "                   #above_threshold_color = 'k');\n",
    "\n",
    "    plt.tick_params(axis= 'y',\n",
    "                    labelsize = 24)\n",
    "    plt.title('Hierarchical clustering for {}'.format(cluster_name_figs), fontsize = 30)\n",
    "#              'Hierarchical Clustering Dendrogram of Selected NOS', fontsize = 20)\n",
    "    plt.xlabel('Distance', fontsize = 30)\n",
    "    plt.ylabel('NOS title',fontsize = 30)\n",
    "    \n",
    "    #s_patch = []\n",
    "    #for ix, which_suite in enumerate(suites_in_clus[:6]):\n",
    "    #    s_patch.append(mpatches.Patch(color= nesta_colours[ix],label= \n",
    "    #                                  which_suite.capitalize()))\n",
    "    #    if ix>6:\n",
    "    #        break\n",
    "        #plt.plot(0,0,color = nesta_colours[ix],label = which_suite.capitalize())\n",
    "    #plt.legend(handles = s_patch, bbox_to_anchor=(1.01, .81), loc=2,\n",
    "    #       ncol=1, borderaxespad=0., fontsize = 24)\n",
    "    #T = plt.yticks()\n",
    "    #for t in T[1]:\n",
    "    #    for ix, which_suite in enumerate(suites_in_clus):\n",
    "    #        if t.get_text().lower() in suites_in_clus[which_suite]:\n",
    "    #            t.set_color(nesta_colours[ix])\n",
    "    #            break\n",
    "    plt.tight_layout()\n",
    "    if SAVEHC:\n",
    "        plt.savefig(os.path.join(output_dir, \n",
    "                                 'all_nos_cut_dendrogram_in_{}_{}_{}_v2.png'.format(\n",
    "            cluster_name_save,qualifier,paramsn['ngrams'])), bbox_inches = \"tight\")   \n",
    "        \n",
    "    # now get and save the clusters\n",
    "    labels_n = fcluster(linkage_matrix_n, cutting_th_n, criterion='distance')\n",
    "    short_df_n = df_nos_n.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "\n",
    "    short_df_n['hierarchical'] = labels_n\n",
    "    short_df_n = short_df_n.set_index('index')\n",
    "    if SAVEHC:\n",
    "        short_df_n.to_csv(os.path.join(output_dir, \n",
    "                                 'all_nos_cut_labels_in_{}_{}_{}_v2.csv'.format(\n",
    "            cluster_name_save,qualifier,paramsn['ngrams'])))\n",
    "        \n",
    "    # print the result of the cut dendrogram\n",
    "    hierarchical_dict= {}\n",
    "    L = {}\n",
    "    D = {}\n",
    "    for ic in range(1,num_clust+1):\n",
    "        hierarchical_dict['{}'.format(ic)] = list(short_df_n['NOS Title'][\n",
    "            short_df_n['hierarchical']==ic].values)\n",
    "        A = distance.squareform(distances_n)[(short_df_n['hierarchical']==ic).values,:][:,\n",
    "                            (short_df_n['hierarchical']==ic).values]\n",
    "        if A.sum()>0:\n",
    "            A = np.triu(A)\n",
    "            A = A[A[:]>0]\n",
    "        else:\n",
    "            A = np.ones(1)\n",
    "        D['{}'.format(ic)] = np.around(np.mean(A),3)\n",
    "        L['{}'.format(ic)] = (short_df_n['hierarchical']==ic).sum()\n",
    "    L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "    D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "    L = L.join(D)\n",
    "    if SAVEHC or True:\n",
    "        L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "            by = 'avg dist', ascending = True).to_csv(output_dir +\n",
    "                                '/all_nos_cut_clusters_in_{}_{}_{}_v2.csv'.format(\n",
    "                                cluster_name_save,qualifier,paramsn['ngrams']))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVEHC or True:\n",
    "    L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "            by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                                '/all_nos_cut_clusters_in_{}_{}_{}_v2.csv'.format(\n",
    "                                cluster_name_save,qualifier,paramsn['ngrams']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def do_kmean(xx, ks=np.arange(2,4), N=100, N2=100):\n",
    "        stab = []\n",
    "        for k in ks:\n",
    "            t0 = time.time()\n",
    "            # do N iterations\n",
    "            stab0 = []\n",
    "            A = np.empty((xx.shape[0],N))\n",
    "            for i in range(N):\n",
    "                k_clus = KMeans(k, n_init = 1, random_state = np.random.randint(1e7))\n",
    "                A[:,i] = k_clus.fit_predict(xx)\n",
    "                for j in range(i):\n",
    "                    stab0.append(adjusted_rand_score(A[:,i],A[:,j]))\n",
    "            # get stability of clusters for this nb of clusters\n",
    "            stab.append(np.mean(stab0))\n",
    "            print_elapsed(t0,'kmeans for k={}'.format(k))\n",
    "        # what number of clusters has highest stability?\n",
    "        kmax = ks[np.array(stab).argmax()]\n",
    "        # redo one last clustering with kmax \n",
    "        # and lots of iteration to get the stable versions\n",
    "        k_clus = KMeans(kmax, n_init= N2)\n",
    "        labels = k_clus.fit_predict(xx)\n",
    "        return labels, k_clus, kmax, stab\n",
    "\n",
    "def get_distance_k(df_row, centroids):\n",
    "    L = len(df_row.values) # last column is the cluster class\n",
    "    distance = pairwise_distances(df_row.values[:L-1].reshape(1, -1), \n",
    "                                  centroids[int(df_row['k_cluster'])].reshape(1, -1))\n",
    "    distance = distance[0][0]\n",
    "    return distance\n",
    "\n",
    "def get_distance_k2(df_row, labels, centroids):\n",
    "    Nf = df_row.shape[0] # last column is the cluster class\n",
    "    N = centroids.shape[0]\n",
    "    distance = np.empty(N)\n",
    "    for ix in range(N):\n",
    "        tmp = df_row[labels == ix]\n",
    "        tmp2 = pairwise_distances(tmp, centroids[ix].reshape(1,-1))\n",
    "        distance[ix] = tmp2[0][0]\n",
    "    #distance = distance[0][0]\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_MODES = ['engineering','management','construction','financialservices']\n",
    "\n",
    "SAVEKM = False\n",
    "for SELECT_MODE in SELECT_MODES[:1]:\n",
    "    df_nos_n, final_groups, larger_suites, cluster_name, cluster_name_save, \\\n",
    "                cluster_name_figs = select_subdf(SELECT_MODE, clusters2use, \n",
    "                                                 suites_clusters,df_nos_select)\n",
    "    print('Computing clusters for {}'.format(cluster_name_figs))\n",
    "\n",
    "    # remove legacy nos\n",
    "    print('nb with legacy nos: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n['NOS Title'].map(lambda x: 'legacy' not in x)]\n",
    "    print('nb without legacy nos 1: ',len(df_nos_n))\n",
    "    df_nos_n = df_nos_n[df_nos_n.index.map(lambda x: not x[-5:]=='l.pdf')]\n",
    "    print('nb without legacy nos 2: ',len(df_nos_n))\n",
    "    suites_in_clus = {}\n",
    "    groups_clus = df_nos_n.groupby('One_suite')\n",
    "    for name, group in groups_clus:\n",
    "        suites_in_clus[name] = list(group['NOS Title'].values)\n",
    "\n",
    "    # this is to get the restricted corpus (to transform, not for fitting)\n",
    "    textfortoken = df_nos_n['pruned_lemmas']\n",
    "    tfidfm_n = tfidf_n.transform(textfortoken)\n",
    "\n",
    "    # get labels\n",
    "    if paramsn['bywhich'] == 'suites':\n",
    "        standard_labels_n = list(df_nos_n.groupby('One_suite').groups.keys())\n",
    "    else:\n",
    "        standard_labels_n = list(df_nos_n['NOS Title'].values)\n",
    "\n",
    "    for ix,t in enumerate(standard_labels_n):\n",
    "        if len(t)>500:\n",
    "            # manual correction because of pdf extraction\n",
    "            standard_labels_n[ix] = standard_labels_n[ix][:50]\n",
    "            \n",
    "    #N = 400\n",
    "    #t0 = time.time()\n",
    "    # use approx the number of super suites SDS gave you\n",
    "    #k = 40 \n",
    "    print(type(tfidfm_n.toarray()))\n",
    "    xx = tfidfm_n.toarray()#StandardScaler.fit_transform(tfidfm_n.toarray())\n",
    "    \n",
    "    #k_clusters = km.labels_.tolist()\n",
    "    #print_elapsed(t0, task = 'kmean clustering')\n",
    "    out = do_kmean(xx, np.arange(150,200,5), N=10, N2= 20)\n",
    "    #k_clusters = out[1].labels_.tolist()\n",
    "    #centroids = out[1].cluster_centers_\n",
    "    #short_df['k_distance'] = tfidfm_df.apply(get_distance_k, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df_n = df_nos_n.reset_index()[['index','NOS Title', 'One_suite','supersuite']]\n",
    "short_df_n['kmeans'] = out[0]\n",
    "short_df_n = short_df_n.set_index('index')\n",
    "if SAVEKM or True:\n",
    "    short_df_n.to_csv(os.path.join(output_dir, \n",
    "                             'all_nos_cut_labels_kmeans_in_{}_{}_{}.csv'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp =pd.read_csv(os.path.join(output_dir, \n",
    "                                 'all_nos_cut_labels_in_{}_{}_{}_v2.csv'.format(\n",
    "            cluster_name_save,qualifier,paramsn['ngrams'])))\n",
    "tmp2 = pd.read_csv(os.path.join(output_dir, \n",
    "                             'all_nos_cut_labels_kmeans_in_{}_{}_{}.csv'.format(\n",
    "        cluster_name_save,qualifier,paramsn['ngrams'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(tmp['hierarchical'].values,tmp2['kmeans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = np.empty(len(tmp2), dtype = np.int)\n",
    "for ix in range(len(tmp2)):\n",
    "    labels2[ix] = int(tmp2['kmeans'].iloc[ix])\n",
    "out_d = get_distance_k2(tfidfm_n.toarray(), labels2, \n",
    "                        out[1].cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result of the cut dendrogram\n",
    "hierarchical_dict= {}\n",
    "L = {}\n",
    "D = {}\n",
    "N = max(tmp2['kmeans'].values)\n",
    "print(N)\n",
    "for ic in range(N+1):\n",
    "    hierarchical_dict['{}'.format(ic)] = list(tmp2['NOS Title'][\n",
    "        tmp2['kmeans']==ic].values)\n",
    "    #A = distance.squareform(distances_n)[(short_df_n['hierarchical']==ic).values,:][:,\n",
    "    #                    (short_df_n['hierarchical']==ic).values]\n",
    "    #if A.sum()>0:\n",
    "    #    A = np.triu(A)\n",
    "    #    A = A[A[:]>0]\n",
    "    #else:\n",
    "    #    A = np.ones(1)\n",
    "    D['{}'.format(ic)] = out_d[ic]#np.around(np.mean(A),3)\n",
    "    L['{}'.format(ic)] = (tmp2['kmeans']==ic).sum()\n",
    "L = pd.DataFrame.from_dict(L, orient = 'index', columns = ['lenght'])\n",
    "D = pd.DataFrame.from_dict(D, orient = 'index', columns = ['avg dist'])\n",
    "L = L.join(D)\n",
    "if SAVEHC or True:\n",
    "    L.join(pd.DataFrame.from_dict(hierarchical_dict, orient = 'index')).sort_values(\n",
    "        by = 'avg dist', ascending = True).T.to_csv(output_dir +\n",
    "                            '/all_nos_cut_clusters_kmeans_in_{}_{}_{}_v2.csv'.format(\n",
    "                            cluster_name_save,qualifier,paramsn['ngrams']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the centroids (that is, the suite closest to the centroid) and print the result of the clustering\n",
    "kmeans_dict= {}\n",
    "most_central = []\n",
    "igroup = 0\n",
    "for name, group in short_df.groupby('k_means'):\n",
    "    kmeans_dict['{}'.format(name)] = group['Suite_names'].values\n",
    "    if igroup < 10:\n",
    "        print(name, group.sort_values(by = 'k_distance').head(3))\n",
    "    igroup += 1\n",
    "    most_central.append(group.sort_values(by = 'k_distance').head(1))\n",
    "if SAVEKM:\n",
    "    pd.DataFrame.from_dict(kmeans_dict, orient = 'index').to_csv(output_dir +\n",
    "                                            '/Kmeans_results_{}_{}_{}.csv'.format(qualifier,bywhich,mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
