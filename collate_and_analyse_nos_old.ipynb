{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 15 14:29:44 2019\n",
    "\n",
    "@author: stefgarasto\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spelling correction\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('/Users/stefgarasto/Google Drive/Documents/data/big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Definitions\n",
    "def unique(list1):\n",
    "\n",
    "    # intilize a null list\n",
    "    unique_list = []\n",
    "    repeated_elems = []\n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "        else:\n",
    "            repeated_elems.append(x)\n",
    "    return unique_list, repeated_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make everything lowercase\n",
    "def make_lowercase(x):\n",
    "    if isinstance(x,str):\n",
    "        return x.lower().replace('â€™','').replace('â','')\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get the file where the metadata is stored\n",
    "metadata_dir = '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/'\n",
    "metadata_file = 'PublishedNos Details.xlsx'\n",
    "metadata_file_old = 'PublishedNos-old Details.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_old = pd.read_excel(os.path.join(metadata_dir, metadata_file_old))\n",
    "metadata_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metadata_old), len(metadata_old['URN'].value_counts()))\n",
    "ix = np.where(metadata_old['URN'].value_counts()==1)[0][0]\n",
    "repeated_urns_old = metadata_old['URN'].value_counts()[0:ix].index.values\n",
    "print('Number of repeated old URNs is: ',len(repeated_urns_old))\n",
    "print('Number of NAN old URNs is: ',metadata_old['URN'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(os.path.join(metadata_dir, metadata_file))\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metadata), len(metadata['URN'].value_counts()))\n",
    "ix = np.where(metadata['URN'].value_counts()==1)[0][0]\n",
    "repeated_urns = metadata['URN'].value_counts()[0:ix].index.values\n",
    "print('Number of repeated old URNs is: ',len(repeated_urns))\n",
    "print('Number of NAN old URNs is: ',metadata['URN'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate all metadata\n",
    "metadata_old.set_index = range(len(metadata),len(metadata)+len(metadata_old))\n",
    "metadata = pd.concat((metadata,metadata_old),ignore_index = True,sort=False)\n",
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate rows with no information\n",
    "metadata = metadata[~metadata['NOS Title'].isnull()]\n",
    "# known bug\n",
    "metadata['Name'] = metadata['Name'].map(lambda x: x.strip())\n",
    "# remove duplicates (keep the last one because it has the SOC code, at least here)\n",
    "ix = np.where(metadata['Name'].value_counts().values==1)[0][0]\n",
    "repeated_meta_names = metadata['Name'].value_counts()[0:ix].index.values\n",
    "repeated_meta_counts = metadata['Name'].value_counts()[0:ix].values\n",
    "metadata.drop_duplicates(subset = 'Name', inplace = True, keep = 'last')\n",
    "metadata = metadata.reset_index(drop = 'True')\n",
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metadata), len(metadata['URN'].value_counts()))\n",
    "ix = np.where(metadata['URN'].value_counts()==1)[0][0]\n",
    "repeated_urns = metadata['URN'].value_counts()[0:ix].index.values\n",
    "print('Number of repeated URNs is: ',len(repeated_urns))\n",
    "print('Number of NAN URNs is: ',metadata['URN'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_suites_corrections(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip().replace('\\n','').replace('–','-').lower().replace('&','and').replace('-',' ').replace('/',' ')\n",
    "        x = x.replace('(','').replace(')','').replace(':','').replace('\\'','')\n",
    "        x = x.replace('＆','and')\n",
    "        x = ' '.join(x.split()).replace('trades','trade').replace('professionals','professional')\n",
    "        x = x.replace('turbines','turbine').replace('para professional','paraprofessional')\n",
    "        x = x.replace('orderivatives','or derivatives').replace('productions','production')\n",
    "        x = x.replace('securities derivatives', 'securities or derivatives').replace('persons','person')\n",
    "        x = x.replace('minerals','mineral')\n",
    "        x = x.replace('managemnet','management')\n",
    "        x = x.replace('monitorin operations','monitoring operations')\n",
    "        x = x.replace('digitial','digital')\n",
    "        x = x.replace('pre cast','precast')\n",
    "        x = x.replace('fenestration i','fenestration, i')\n",
    "        x = x.replace('managmement','management')\n",
    "        x = x.replace('auxillary','auxiliary')\n",
    "        x = x.replace('installations','installation')\n",
    "        x = x.replace('vererinary','veterinary')\n",
    "        x = x.replace('managmement','management')\n",
    "        x = x.replace('wood machining','woodmachining')\n",
    "        x = x.replace('mangement','management')\n",
    "        x = x.replace('small holders','smallholders')\n",
    "        x = x.replace('digitial','digital')\n",
    "        x = x.replace('performanceandnbsp','performance')\n",
    "        x = x.replace('administration nos','administration')\n",
    "        x = x.replace('installing domestic fascia, soffit,', 'installation of domestic fascias, soffits')\n",
    "        x = x.replace('skills for security essential employability nos','skills for security essential employability')\n",
    "        x = x.replace('fenestration and installation surveying','fenestration, installation and surveying')\n",
    "        # ('nail services', 100), ('mail services', 92)\n",
    "        #x = x.replace('medicinal','medical')\n",
    "        #x = correction(x)\n",
    "    else:\n",
    "        print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Suite','Validity','Indicative Review Date','NOS Document Status']:\n",
    "    #plt.figure()\n",
    "    #sns.countplot(metadata_old[col])\n",
    "    print(col,len(metadata[col].value_counts()))\n",
    "    \n",
    "# some suites are lists separated by \";\": extract all unique suites\n",
    "all_ground_suites = []\n",
    "for row in metadata.index:\n",
    "    if isinstance(metadata['Suite'].loc[row],str):\n",
    "        all_ground_suites += metadata['Suite'].loc[row].split(\";\")\n",
    "t0 = time.time()\n",
    "all_ground_suites = [meta_suites_corrections(t) for t in all_ground_suites]\n",
    "print_elapsed(t0,'correcting ground truth suites')\n",
    "\n",
    "all_ground_suites = list(set(all_ground_suites))\n",
    "if all_ground_suites[0]=='':\n",
    "    all_ground_suites = all_ground_suites[1:]\n",
    "print('Number of unique suites from Active IS metadata: ',len(all_ground_suites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_SUITES = False\n",
    "if CHECK_SUITES:\n",
    "    counter = 0\n",
    "    t0 = time.time()\n",
    "    for ix in range(len(all_ground_suites)):\n",
    "        out = process.extract(all_ground_suites[ix], all_ground_suites, limit=2)\n",
    "        if out[1][1] > 88 and out[1][1]<100:\n",
    "            #if str(set(out[0][0])-set(out[1][0])).isdigit():\n",
    "            print(out)\n",
    "            counter+=1\n",
    "    print_elapsed(t0,'listing almost-the-same suites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, split, clean and rejoin the suites\n",
    "def clean_meta_suites(x):\n",
    "    if isinstance(x,str):\n",
    "        x = \";\".join([meta_suites_corrections(t) for t in x.split(';')])\n",
    "    return x\n",
    "metadata['Suite'] = metadata['Suite'].map(clean_meta_suites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, make it all lowercase\n",
    "metadata = metadata.applymap(make_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_old = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} rows in the metadata with non-null SOC code'.format(metadata['SOC Code'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for NOS CAtegory, change jobspecific to jobspecific\n",
    "metadata['NOSCategory'][metadata['NOSCategory']=='jobspecific'] = 'job specific'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean data extracted from pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Load and concatenate all NOS\n",
    "\n",
    "data_dir = '../../results/NOS/extracted'\n",
    "figure_dir = '../../results/NOS/nlp_analysis'\n",
    "which_files = ['extracted_standards_New NOS 1.pickle', 'extracted_standards_New NOS 2.pickle']\n",
    "for ii in range(1,23):\n",
    "    which_files.append('extracted_standards_Old NOS {}.pickle'.format(ii))\n",
    "#which_file = 'extracted_standards_Old NOS 22.pickle'\n",
    "\n",
    "repeated_refs = []\n",
    "tot_count = 0\n",
    "# load all the extracted NOS and concatenate them\n",
    "for ii,which_file in enumerate(which_files):\n",
    "    version = 'new_' #'v2_'\n",
    "    print(version + which_file)\n",
    "    with open(os.path.join(data_dir, version + which_file),'rb') as f:\n",
    "        standard_info_partial, standard_ref_partial, failed, _ = pickle.load(f)\n",
    "    tot_count += len(standard_ref_partial)\n",
    "    # quick check that the list of keys in the standard dictionary is the\n",
    "    # same as the URNs in the standard_ref list\n",
    "    tmp = [k.replace('_v2','').replace('_v3','').replace('_v4','') for k in standard_info_partial.keys()]\n",
    "    if not unique(standard_ref_partial)[0] == unique(tmp)[0]:\n",
    "        print('Dictionary keys and standard refs do not correspond. Something is wrong.')\n",
    "    if ii == 0:\n",
    "        standard_info= standard_info_partial\n",
    "        standard_ref = standard_ref_partial\n",
    "    else:\n",
    "        # check if there is any ref that is already in the full list\n",
    "        ref_intersection= list(set(standard_ref_partial).intersection(set(standard_ref)))\n",
    "        if len(ref_intersection):\n",
    "            #keep track of the repeated ones and add _v followed by the next \n",
    "            # available number to their dict keys\n",
    "            repeated_refs = repeated_refs + ref_intersection\n",
    "            for ref in ref_intersection:\n",
    "                new_version = 2\n",
    "                while ref + '_v{}'.format(new_version) in list(standard_info_partial.keys()):\n",
    "                    new_version+=1\n",
    "                    #print(new_version)\n",
    "                standard_info_partial[ref + '_v{}'.format(new_version)\n",
    "                    ] = standard_info_partial.pop(ref)\n",
    "        standard_info.update(standard_info_partial)\n",
    "        standard_ref = standard_ref + standard_ref_partial\n",
    "\n",
    "\n",
    "standard_info_partial = None\n",
    "standard_ref_partial = None\n",
    "print(len(standard_info),tot_count)\n",
    "\n",
    "# NOTE: The total number of NOS is 22757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it's fine to turn some of the fields into string\n",
    "# Specifically, these fields should be strings: URN, Title, Developed_by, Date_approved\n",
    "# Indicative review date, Validity, Status, Originating organisation, Suite\n",
    "# OBS:I would keep the Original URN as a list, since it can contain multiple one\n",
    "# How about keywords?\n",
    "# Also, version number should be an integer\n",
    "for idct, key_dct in enumerate(standard_info):\n",
    "    dct = standard_info[key_dct]\n",
    "    needed_sections = []\n",
    "    for key in ['Developed_by','Date_approved', 'Indicative_review_date', \n",
    "                'Originating_organisation', 'Status','Suite', 'Validity','Version_number']:\n",
    "        if key in dct.keys():\n",
    "            if len(dct[key]):\n",
    "                tmp = dct[key][0]\n",
    "                for ii in range(1,len(dct[key])):\n",
    "                    tmp += dct[key][ii]\n",
    "                dct[key] = tmp\n",
    "                if key=='Version_number':\n",
    "                    # sometimes, the Version number contains spurious characters, not only digit\n",
    "                    # so I need to dig out whether there is a digit in it and take it out\n",
    "                    # yes: sometimes this can return the wrong value, because it will stop at the\n",
    "                    # first digit it encounters\n",
    "                    s = dct['Version_number']\n",
    "                    re_result = re.search('\\d{1,2}',s) #('\\d+',s)\n",
    "                    if re_result:\n",
    "                        dct['Version_number'] = int(s[re_result.span()[0]:re_result.span()[1]])\n",
    "                    else:\n",
    "                        dct['Version_number'] = -1\n",
    "            else:\n",
    "                # in this case the field was an empty list, meaning the section was empty.\n",
    "                if key=='Version_number':\n",
    "                    dct['Version_number'] = -1\n",
    "                else:\n",
    "                    dct[key] = 'empty'\n",
    "    \n",
    "\n",
    "    #int(float(dct['Version_number']))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP = False\n",
    "qualifier = 'new_no_dropped'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Turn the nested dictionary into a dataframe\n",
    "nos_data = pd.DataFrame.from_dict(standard_info, orient = 'index')\n",
    "print(nos_data.columns)\n",
    "# first, clean the data\n",
    "# find all the rows with at least one column == 'empty' or == -1 (see above as to why)\n",
    "# also, there are some bad URNs (see commented cell below), which are:\n",
    "bad_urns = ['{Unique Reference Number]']\n",
    "nos_data['empty'] = nos_data.apply(lambda x: (x=='empty') | (x==-1)).any(axis=1)\n",
    "print('Number of rows we might need to drop is: ', np.sum(nos_data['empty']))\n",
    "dropped_urns = list(nos_data.index[nos_data['empty']])\n",
    "dropped_filenames = list(nos_data['file_name'][nos_data['empty']])\n",
    "dropped_titles = list(nos_data['Title'][nos_data['empty']])\n",
    "# try not eliminating the rows\n",
    "if DROP:\n",
    "    nos_data_clean = nos_data[~nos_data['empty']]\n",
    "else:\n",
    "    nos_data_clean = nos_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data_clean = nos_data_clean.applymap(make_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# plot histograms after the cleaning, but before any other change\n",
    "# just by dropping the 630 rows above, the following histograms look much cleaner\n",
    "for col in ['Originating_organisation', 'NOS Document Status', 'Validity',\n",
    "       'Indicative Review Date', 'Date_approved', 'Developed By']:\n",
    "    plt.figure(figsize = (5,10))\n",
    "    tmp = nos_data_clean[col].value_counts()\n",
    "    N = min((len(tmp),40))\n",
    "    tmp[0:N].plot('barh')\n",
    "    plt.xlabel(col)\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_{}.svg'.format(col,qualifier)))\n",
    "# without dropping the rows, we might need to sort out the status column\n",
    "'''\n",
    "print('not needed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the year in which it was approved and the indicative review year\n",
    "# first you need to look for the month and the year\n",
    "def extract_monthyear(x):\n",
    "    reg_exp = 'jan|january|february|march|april|may|june|july|august|september|october|november|december'\n",
    "    p = re.compile(reg_exp)\n",
    "    # search for the regular expression and only get the first occurrence\n",
    "    # we will be assuming that the year is right after the month\n",
    "    if isinstance(x,str):\n",
    "        # first check if they are all digits:\n",
    "        if all([d.isdigit() for d in x]):\n",
    "            return x\n",
    "        if p.search(x.lower()):\n",
    "            start, end = p.search(x.lower()).span()\n",
    "            # get intermediate string:\n",
    "            tmp_s = x[start:end+10]\n",
    "            # find first and last digit in the intermediate string\n",
    "            digits = [ii for ii,d in enumerate(tmp_s) if d.isdigit()]\n",
    "            if len(digits):\n",
    "                year = tmp_s[digits[0]:digits[-1]+1]\n",
    "            else:\n",
    "                return 'july 2130'\n",
    "            # check if there is another string:\n",
    "            x2 = x[end:]\n",
    "            if p.search(x2.lower()):\n",
    "                start2, end2 = p.search(x2.lower()).span()\n",
    "                # get intermediate string:\n",
    "                tmp_s2 = x2[start2:end2+10]\n",
    "                # find first and last digit in the intermediate string\n",
    "                digits2 = [ii for ii,d in enumerate(tmp_s2) if d.isdigit()]\n",
    "                year2 = tmp_s2[digits2[0]:digits2[-1]+1]\n",
    "                if int(year2)>int(year):\n",
    "                    return x2[start2:end2].lower() + ' ' + year2\n",
    "                else:\n",
    "                    x[start:end].lower() + ' ' + year\n",
    "            else:\n",
    "                return x[start:end].lower() + ' ' + year\n",
    "        else:\n",
    "            return 'july 2130'\n",
    "    else:\n",
    "        return 'july 2130'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_monthyear('2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the month for the indicative review date, turn into a datetime object, extract the year\n",
    "nos_data_clean['Indicative_review_year'] = pd.to_datetime(\n",
    "    nos_data_clean['Indicative_review_date'].map(extract_monthyear),errors= 'coerce',infer_datetime_format=True).map(\n",
    "    lambda x: x.year)\n",
    "\n",
    "# do the same for the date approved field\n",
    "nos_data_clean['Date_approved_year'] = pd.to_datetime(\n",
    "    nos_data_clean['Date_approved'].map(extract_monthyear),errors= 'coerce',infer_datetime_format=True).map(\n",
    "    lambda x: x.year)\n",
    "\n",
    "# get the rows without a good year string, to drop them in future\n",
    "no_review_year_rows = list(nos_data_clean.index[nos_data_clean['Indicative_review_year']==2030.0])\n",
    "no_review_year_files = list(nos_data_clean['file_name'][nos_data_clean['Indicative_review_year']==2030.0])\n",
    "no_review_year_titles = list(nos_data_clean['Title'][nos_data_clean['Indicative_review_year']==2030.0])\n",
    "no_approved_year_rows = nos_data_clean.index[nos_data_clean['Date_approved_year']==2030.0]\n",
    "no_approved_year_files = list(nos_data_clean['file_name'][nos_data_clean['Date_approved_year']==2030.0])\n",
    "no_approved_year_titles = list(nos_data_clean['Title'][nos_data_clean['Date_approved_year']==2030.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value counts as an histogram\n",
    "# indicative review data\n",
    "plt.figure(figsize = (10,4))\n",
    "nos_data_clean['Indicative_review_year'].value_counts().plot('bar')\n",
    "#nos_data_clean['Clean Ind Review Year'].value_counts().plot('bar')\n",
    "plt.xlabel('Indicative review year')\n",
    "if SAVEFIG:\n",
    "    plt.savefig(os.path.join(figure_dir, 'Most_common_Indicative_review_year_{}.svg'.format(qualifier)))\n",
    "# Approved date\n",
    "plt.figure(figsize = (10,4))\n",
    "nos_data_clean['Date_approved_year'].value_counts().plot('bar')\n",
    "plt.xlabel('Year approved')\n",
    "if SAVEFIG:\n",
    "    plt.savefig(os.path.join(figure_dir, 'Most_common_Approved_year{}.svg'.format(qualifier)))\n",
    "\n",
    "# now joint histogram\n",
    "plt.figure()\n",
    "g = sns.jointplot(\"Date_approved_year\", \"Indicative_review_year\", data=nos_data_clean)\n",
    "if SAVEFIG:\n",
    "    plt.savefig(os.path.join(figure_dir, 'Indicative_review_year_vs_Approved_year_{}.svg'.format(qualifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: clean up rather than delete suites, also remove years from suites but keep the \"suite number\"\n",
    "# things to do to clean up suites names:\n",
    "'''\n",
    "1. remove brackets\n",
    "2. traderepair --> trade repair\n",
    "3. & to and\n",
    "4. check why sound and sound are split\n",
    "5. check wood constructions\n",
    "6. eliminate years\n",
    "7. plastering (construction), roofing occupations (construction) --> split. \n",
    "7a. Generally, split by comma too (not sure, actually)\n",
    "8. offshore s1971 elected safety representatives is the only one with a number in the middle\n",
    "8a. joking, there is also h2s & other hazardous gases; 1132 marketing and sales managers; \n",
    "9. remove - and similar (like hospitality â€“ generics)\n",
    "10. be careful with laboratory and associated technical activities suite 32010 (and 42019 and 22010)\n",
    "11. be careful of it users 6.2 too\n",
    "12. constructionindustry\n",
    "13. operationscoating\n",
    "24. remove leading and end spaces\n",
    "'''\n",
    "def clean_suites(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('(',' ')\n",
    "        x = x.replace(')',' ')\n",
    "        if ('22010' in x) or ('32010' in x) or ('42010' in x):\n",
    "            x = x[0:-4]\n",
    "        x = x.replace('&','and')\n",
    "        # remove year\n",
    "        #if 's1971' not in x:\n",
    "        #    x = re.sub('\\d{4}',' ', x)\n",
    "        #    if 'h2s' not in x:\n",
    "        #        # add a semicolon after a number so that you can split by it later\n",
    "        #        x = re.sub('(\\d)', r'\\1;', x)\n",
    "        x = x.replace('&', 'and ')\n",
    "        x = x.replace('â€“', '')\n",
    "        #x = x.replace('– ',' ')\n",
    "        x = x.replace('–',' ')\n",
    "        #x = x.replace('- ',' ')\n",
    "        x = x.replace('-',' ')\n",
    "        x = x.replace('/' ,' ')\n",
    "        x = x.replace('.',' ')\n",
    "        # eliminate multiple spaces\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        # remove leading and trailing spaces\n",
    "        x = x.strip()\n",
    "        #TODO: with few exception if I find a group of max 3 digits not at \n",
    "        #the end and followed by a space or a letter, that's a point I should split\n",
    "        if ('wood occupation' in x) and ('adv' not in x):\n",
    "            x = 'wood occupations construction'\n",
    "        x = x.replace('traderepair', 'trade repair')\n",
    "        x = x.replace('constructionindustry','construction industry')\n",
    "        x = x.replace('operationscoating','operations coating')\n",
    "        x = x.replace('ofelectrical','of electrical')\n",
    "        x = x.replace('mineralsprocessing','minerals processing')\n",
    "        x = x.replace('differentcountries','different countries')\n",
    "        x = x.replace('combinedworking','combined working')\n",
    "        x = x.replace('localgovernment', 'local government')\n",
    "        x = x.replace('eventsecurity','event security')\n",
    "        x = x.replace('cropproduction','crop production')\n",
    "        x = x.replace('customerservices','customer services')\n",
    "        x = x.replace('occupationsresidential','occupations residential')\n",
    "        x = x.replace('systemsmaintenance','systems maintenance')\n",
    "        x = x.replace('andrefurbishment','and refurbishment')\n",
    "        x = x.replace('materialsprocessing','materials processing')\n",
    "        x = x.replace('thebuilding','the building')\n",
    "        x = x.replace('managerfire','manager fire')\n",
    "        x = x.replace('formanagement','for management')\n",
    "        x = x.replace('alliedoperation','allied operation')\n",
    "        x = x.replace('safetyrequirementsfabrication','safety requirements fabrication')\n",
    "        x = x.replace('operationsdrilling', 'operations drilling')\n",
    "        x = x.replace('operationsbulk', 'operations bulk')\n",
    "        x = x.replace('andmanagement', 'and management')\n",
    "        x = x.replace('artsmanagement','arts management')\n",
    "        x = x.replace('pensions','pension')\n",
    "        x = x.replace('extrusion furniture', 'extrusion and furniture')\n",
    "        x = x.replace('ofconstruction','of construction')\n",
    "        x = x.replace('2008aeronautical', '2008 aeronautical')\n",
    "        x = x.replace('2rail', '2 rail')\n",
    "        x = x.replace('2006engineering', '2006 engineering')\n",
    "        x = x.replace('1132 marketing','marketing')\n",
    "        x = x.replace('nail services 2009 2009', 'nail services 2009')\n",
    "        x = x.replace('roadbuilding','road building')\n",
    "        x = x.replace('seniorroles','senior roles')\n",
    "        x = x.replace('511con po1561', '')\n",
    "        x = x.replace('nailservices','nail services')\n",
    "        x = x.replace('servicesnos','services nos')\n",
    "        x = x.replace('sitelogistics','site logistics')\n",
    "        x = x.replace('securityoperations','security operations')\n",
    "        x = x.replace('andtendering','and tendering')\n",
    "        x = x.strip()\n",
    "    return x\n",
    "\n",
    "# this function cleans the pdf extracted suites\n",
    "#nos_data_clean['Clean_suite'] = nos_data_clean['Suite'].map(meta_suites_corrections).map(clean_suites)\n",
    "nos_data_clean['Clean_suite'] = nos_data_clean['Suite'].map(clean_suites)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data_clean[['Clean_suite','Suite']].sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform some more manual cleanup\n",
    "# for the Validity, Status, Developed_by columns, make all lower case and drop some\n",
    "# obviously wrong columns (see histograms above)\n",
    "def lower_or_empty(x):\n",
    "    if isinstance(x,str):\n",
    "        y= x.lower()\n",
    "    else:\n",
    "        y= 'empty'\n",
    "    # also, fix some common misspellings:\n",
    "    if y in ['orginal', 'origianl','orignal','original ??????????','original3']:\n",
    "        y = 'original'\n",
    "    elif y in ['import', 'imported from ento', 'amended form imported ento unit']:\n",
    "        y = 'imported'\n",
    "    elif y == 'tailored from ento':\n",
    "        y = 'tailored'\n",
    "    elif y in ['currentl','curent']:\n",
    "        y = 'current'\n",
    "    elif y == 'may 2016':\n",
    "        y = 'empty'\n",
    "    elif y == 'building information modelling':\n",
    "        y== 'empty'\n",
    "    elif y in ['proskills uk', 'manual, electronic systems, process warning enunciatorsprocess documents and logs, checklistssafety notice boards, safety data sheets, warning signsproskills']:\n",
    "        y = 'proskills'\n",
    "    elif y in ['summitskillsversionnumber1', 'summit skills']:\n",
    "        y = 'summitskills'\n",
    "    elif y == 'manufacturersâ€™ instructions6method statements7product worksheetsengineering construction industry training board':\n",
    "        y = 'empty'\n",
    "    elif y == 'how signals can enter, route through and exit mixing and recordinghow to identify common signal routing stages of mixing and recordingthe relationship between the stereo master section and a singlechannel -describetypes of professional mixing and recording consolesthe features of different professional recording consolesthe types of console different music and sound sectors requirethe main sector manufacturers of equipment for studio recording andmixing studios/programming consolesthe main sector manufacturers of equipment forbroadcast/tv/av/radio consolesthe main sector manufacturers of equipment for live mobile recording,theatre/performance and live sound/pa equipment for venuescreative & cultural skills':\n",
    "        y = 'creative and cultural skills'\n",
    "    elif y in ['skiillsactive','skills active', \n",
    "              'skillsactiveevelop your tactical skills to achieve excellence in your sport']:\n",
    "        y = 'skillsactive'\n",
    "    elif y in ['cfa', 'cfa business skill @ work', 'cfa business @ work',\n",
    "              'these business enterprise units may be relevant when you are setting up ordeveloping a business:ys1 explore your own business motivesys2 check your ability to run your businessys3 improve your business skillsee3 make deals to take your business forwardcfa business skills @ workz0g.docx']:\n",
    "        y = 'cfa business skills @ work'\n",
    "    elif y in ['mpqcrovide leadership in the mine',\n",
    "              'subsequent legislation that supersedes it, as appropriate to the workactivitympqclast to specification',\n",
    "              'resources: tools, equipment, utilities or services, consumablesmpqc',\n",
    "              'mpqceparate and dispose of wastes and by-products from mining and relatedprocessing activities',\n",
    "              'to the workplace, work activity and the associated environmental impactsmpqconitor and maintain environmental conditions in your area of responsibility',\n",
    "              'mpqcrocess materials to specification','[mpqc']:\n",
    "        y = 'mpqc'\n",
    "    elif y in ['6.46.5semtacomputer-based recordelectronic mail',\n",
    "               'complete the relevant paperwork, to include one from the following andpass it to the appropriate people:7.1 build records7.2 computer records7.3 job cards7.4 aircraft service/flight log other specific recording methodsemta']:\n",
    "        y = 'semta'\n",
    "    elif y == 'people1st':\n",
    "        y = 'people 1st'\n",
    "    elif y in ['security threatse-skills uk',\n",
    "               'k12.5 verifying the accuracy, currency, completeness and relevance ofinformation created, collected, used and documented during datadesign activitiese-skills uk']:\n",
    "        y = 'skills uk'\n",
    "    elif y == '[habia':\n",
    "        y = 'habia'\n",
    "    # finally, change & to and\n",
    "    y = y.replace('&','and')\n",
    "    return y\n",
    "    \n",
    "\n",
    "for col in ['Status', 'Validity', 'Developed_by']:\n",
    "    nos_data_clean[col] = nos_data_clean[col].map(lower_or_empty)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just change the incredibly long one in the validity field\n",
    "tmp = [nos_data_clean['Validity'].value_counts().index][0]\n",
    "tmp = [t for t in tmp if len(t)> 100]\n",
    "\n",
    "if len(tmp):\n",
    "    tmp = tmp[0]\n",
    "    for row in nos_data_clean.index:\n",
    "        if nos_data_clean['Validity'].loc[row] == tmp:\n",
    "            nos_data_clean['Validity'].loc[row] = 'current'\n",
    "            print(row)\n",
    "            print(nos_data_clean['file_name'].loc[row])\n",
    "            print(nos_data_clean['Title'].loc[row])\n",
    "\n",
    "for col in ['Status', 'Validity']:\n",
    "    plt.figure(figsize = (5,5))\n",
    "    tmp = nos_data_clean[col].value_counts().plot('barh')\n",
    "    plt.ylabel(col)\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_{}.svg'.format(col,qualifier)))\n",
    "    \n",
    "for col in ['Developed_by']:\n",
    "    plt.figure(figsize = (5,15))\n",
    "    tmp = nos_data_clean[col].value_counts().plot('barh')\n",
    "    plt.ylabel(col)\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_{}.svg'.format(col,qualifier)))\n",
    "    #print(nos_data_clean[col].value_counts())\n",
    "# TODO: get empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Now, remove the other empty/faulty rows that were identified before.\n",
    "nos_data_clean.loc[:,'empty'] = False\n",
    "tmp = nos_data_clean.applymap(lambda x: (x=='empty') | (x==['empty'])).any(axis=1)\n",
    "nos_data_clean.loc[:,'empty'] = tmp\n",
    "empty_rows = list(nos_data_clean.index[nos_data_clean['empty']])\n",
    "# include empty review/approved date in rows to remove or not\n",
    "INCLUDE_YEARS = False\n",
    "if INCLUDE_YEARS:\n",
    "    # this removes 514 rows\n",
    "    rows_to_remove = list(set(empty_rows + not_string_suites_rows + list(no_review_year_rows) + \n",
    "                              list(no_approved_year_rows)))\n",
    "else:\n",
    "    # this remove 17 rows\n",
    "    rows_to_remove = list(set(empty_rows + not_string_suites_rows))\n",
    "print('Number of rows that I would drop is: ', len(rows_to_remove))\n",
    "for row in rows_to_remove: #this for loop is incredibly slow for some reason, just FYI\n",
    "    nos_data_clean.loc[row,'empty'] = True\n",
    "dropped_urns = dropped_urns + rows_to_remove\n",
    "dropped_filenames = dropped_filenames + list(nos_data_clean['file_name'][nos_data_clean['empty']])\n",
    "dropped_titles = list(nos_data_clean['Title'][nos_data_clean['empty']])\n",
    "# again, I'm not actually deleting rows this time\n",
    "if DROP:\n",
    "    nos_data_cleaner = nos_data_clean[~nos_data_clean['empty']]\n",
    "else:\n",
    "    nos_data_cleaner = nos_data_clean\n",
    "'''\n",
    "print('not needed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: NLP analysys. The final output dataset is called nos_data_cleaner\n",
    "nos_data_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join pdf-extracted data and the metadata from ActiveIS\n",
    "\n",
    "Note that both datasets should have already been cleaned. However, some leading and trailing spaces might still appear when we split the suites by \";\". Also, I still don't know how to split lists of suites from the pdf-extracted data that are not actually separated by \";\" (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of NOS extracted from pdfs is {}'.format(len(nos_data_clean)))\n",
    "print('Number of NOS for whom we have metadata is {}'.format(len(metadata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some filenames appear different but they actually do have a match: find that match by using the URN\n",
    "t0 = time.time()\n",
    "missings = list(set(metadata['Name'])-set(nos_data_clean['file_name']))\n",
    "# + len(set(nos_data_clean['file_name'])-set(metadata['Name']))\n",
    "counter = 0\n",
    "for ix in missings:\n",
    "    row = metadata[metadata['Name']==ix]\n",
    "    urn = row['URN'].values\n",
    "    if len(urn)==1:\n",
    "        row2 = nos_data_clean[nos_data_clean['URN']==urn[0]]\n",
    "        if len(row2):\n",
    "            counter += 1\n",
    "            ixx = row2.index[0]\n",
    "            nos_data_clean['file_name'].loc[ixx] = ix\n",
    "print(counter,len(missings),'{:.4f}'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered = counter\n",
    "metadata_missing = len(list(set(metadata['Name'])-set(nos_data_clean['file_name'])))\n",
    "intersected = set(metadata['Name']).intersection(set(nos_data_clean['file_name']))\n",
    "print('Number of NOS matched from metadata to pdf-extracted is {}'.format(len(list(intersected))))\n",
    "print('Number of metadata NOS not matched is {}'.format(metadata_missing))\n",
    "print('Number of pdf-extracted NOS not matched is {}'.format(\n",
    "    len(list(set(nos_data_clean['file_name']) - set(metadata['Name'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nos_data = nos_data_clean.set_index('file_name').join(metadata.set_index('Name'), lsuffix = '_2') \n",
    "tmp = nos_data.isnull().sum()\n",
    "print('Missing values from the joined dataframe: ')\n",
    "for col in nos_data.columns.sort_values():\n",
    "    print(col,'-',tmp[col])\n",
    "print(len(nos_data)-len(metadata)+523-252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[['Clean_suite','Suite']].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the existing metadata when it's null\n",
    "cols2copy = ['Developed By', 'NOS Title', 'Occupations', \n",
    "       'Validity', 'NOS Document Status', 'URN',\n",
    "       'Keywords', 'Original URN', 'Suite']\n",
    "backup_cols = ['Developed_by', 'Title', 'Relevant_occupations',\n",
    "       'Validity_2', 'Status', 'URN_2',\n",
    "        'Keywords_2', 'Original_URN', 'Clean_suite']\n",
    "for ix,col in enumerate(cols2copy):\n",
    "    nullrows = nos_data[col].isnull()\n",
    "    nos_data[col].loc[nullrows] = nos_data[backup_cols[ix]].loc[nullrows]\n",
    "nos_data['Suite'][nos_data['Suite'].isnull()] = 'others'\n",
    "# The year needs to be dealt with separately\n",
    "# Indicative Review Date - 1029\n",
    "# Indicative_review_date - 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find repeated file names, URNs and NOS titles\n",
    "# store repeated indices (that is, file names) - I already removed duplicates from the metadata, so A inherited all\n",
    "# possible duplicates from nos_data\n",
    "ix = np.where(nos_data.index.value_counts().values==1)[0][0]\n",
    "repeated_names = list(nos_data.index.value_counts()[0:ix].index.values)\n",
    "repeated_names_counts = list(nos_data.index.value_counts()[0:ix].values)\n",
    "repeated_names += list(repeated_meta_names)\n",
    "#repeated_names_counts += repeated_meta_names_counts\n",
    "# now URN\n",
    "col = 'URN'\n",
    "ix = np.where(nos_data[col].value_counts().values==1)[0][0]\n",
    "repeated_urns = nos_data[col].value_counts()[0:ix].index.values\n",
    "repeated_urns_counts = nos_data[col].value_counts()[0:ix].values\n",
    "\n",
    "col = 'NOS Title'\n",
    "ix = np.where(nos_data[col].value_counts().values==1)[0][0]\n",
    "repeated_titles = nos_data[col].value_counts()[0:ix].index.values\n",
    "repeated_titles_counts = nos_data[col].value_counts()[0:ix].values\n",
    "\n",
    "# now, drop rows for whom both URN and Title are exactly the same\n",
    "B = nos_data[nos_data.duplicated(subset= ['NOS Title','URN'])]\n",
    "\n",
    "# save them for another time\n",
    "with open(os.path.join(figure_dir,'repeated_rows.pickle'),'wb') as f:\n",
    "    pickle.dump((repeated_names,repeated_names_counts,repeated_urns,repeated_urns_counts,repeated_titles,\n",
    "               repeated_titles_counts,B,['repeated_names,repeated_names_counts,repeated_urns,repeated_urns_counts,'+\n",
    "               'repeated_titles,repeated_titles_counts,Titles+URNs duplicated']), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, drop the duplicates indices\n",
    "nos_data = nos_data.groupby(nos_data.index).first()\n",
    "## now, drop rows that have both URN and Title exactly the same\n",
    "#B = A.drop_duplicates(subset = ['NOS Title','URN'])\n",
    "#len(B),len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the year from the metadata information. As expected, lots of them have the year 1905\n",
    "nos_data['Indicative Review Year'] = pd.to_datetime(nos_data['Indicative Review Date'],\n",
    "                                                         errors = 'coerce').map(lambda x: x.year)\n",
    "\n",
    "# for the rows with year = 1905.0 or that are null it's better to use the extracted ones\n",
    "nos_data['Clean Ind Review Year'] = nos_data['Indicative Review Year']\n",
    "nos_data['Clean Ind Review Year'].loc[nos_data['Indicative Review Year']==1905.0] = nos_data[\n",
    "    'Indicative_review_year'].loc[nos_data['Indicative Review Year']==1905.0]\n",
    "\n",
    "# also replace the rows with a null value with the extracted year\n",
    "nullrows = nos_data['Clean Ind Review Year'].isnull()\n",
    "nos_data['Clean Ind Review Year'].loc[nullrows] = nos_data['Indicative_review_year'].loc[nullrows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the suite fields to check for possible outliers. Specifically, I want to find a length threshold to discard\n",
    "# spurious elements (some might be mistakes, some might be multiple suites combined into one)\n",
    "\n",
    "# One suite fields, can contain multiple suites split by ';'. Perform the split and\n",
    "# get the lenght of each sub-element\n",
    "\n",
    "suite_len = []\n",
    "all_suites = []\n",
    "not_string_suites_rows = []\n",
    "not_string_suites_titles = []\n",
    "not_string_suites_names = []\n",
    "col2use = 'Suite'\n",
    "for row in nos_data.index:\n",
    "    if isinstance(nos_data[col2use].loc[row],str):\n",
    "        y = nos_data[col2use].loc[row].split(';')\n",
    "        y = [t.lstrip().rstrip() for t in y]\n",
    "        suite_len += [len(t) for t in y]\n",
    "        all_suites += y\n",
    "    else:\n",
    "        not_string_suites_rows_names.append(row)\n",
    "        not_string_suites.append(nos_data['URN'].loc[row])\n",
    "        not_string_suites_titles.append(nos_data['Title'].loc[row])\n",
    "        \n",
    "\n",
    "# get the unique list of suites\n",
    "unique_suites = list(set(all_suites))\n",
    "unique_len = [len(t) for t in unique_suites]\n",
    "sort_ind = np.argsort(unique_len)\n",
    "# order them by length\n",
    "unique_len = itemgetter(*sort_ind)(unique_len)\n",
    "unique_suites = itemgetter(*sort_ind)(unique_suites)\n",
    "\n",
    "# now print all the suites that contain more than N characters: how many can we safely disregard? Is there a\n",
    "# threshold that separates good from bad ones:\n",
    "N = 90\n",
    "# collect the indices of unique suites that are longer than N characters\n",
    "tmp = [ t for t,tval in enumerate(unique_len) if tval>N]\n",
    "for t in tmp:\n",
    "    print(unique_len[t],unique_suites[t])\n",
    "    print('\\n')\n",
    "    \n",
    "# Observation: if I select threshold = 90, I shouldn't be keeping any \"bad\" suites, but I'll be discarding some good\n",
    "# ones. There are, I think, 4 good and 3 bad ones that are between 90 and 122 characters. \n",
    "# DECISION: Let's set the threshold to 125\n",
    "print(not_string_suites_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle again through the dataframe to keep the whole suites list, have one extra column with only the first suite in \n",
    "# each list (not ideal, I know, but then I can categorise all the rows), and eliminate the sub-elements that are\n",
    "# longer than Nth characters. Note that for rows with only one suite to begin with, nothing should change\n",
    "\n",
    "def get_all_and_single_suites(Nth,nos_data_clean,col):\n",
    "    not_string_suites_rows = []\n",
    "    not_string_suites_titles = []\n",
    "    not_string_suites_names = []\n",
    "    #nos_data_clean['All_suites'] = 'empty' # initialise, so that if it is not overridden it will appear as empty\n",
    "    #nos_data_clean['One_suite'] = 'empty' # initialise, so that if it is not overridden it will appear as empty\n",
    "    all_suites = []\n",
    "    one_suite = []\n",
    "    for row in nos_data_clean.index:\n",
    "        if isinstance(nos_data_clean[col].loc[row],str):\n",
    "            y = nos_data_clean[col].loc[row].split(';')\n",
    "            y = [t for t in y if len(t)<Nth]\n",
    "            if len(y):\n",
    "                y = [t.strip() for t in y]\n",
    "                y = [t for t in y if len(t)]\n",
    "                all_suites.append(y)\n",
    "                one_suite.append(y[0])\n",
    "            else:\n",
    "                tmp= ''\n",
    "                y = nos_data_clean[col].loc[row].split(' ')\n",
    "                for t in y:\n",
    "                    if t not in tmp:\n",
    "                        tmp += ' ' + t\n",
    "                    else:\n",
    "                        break\n",
    "                # now split by comma\n",
    "                tmp = tmp.split(',')[0].strip()\n",
    "                #print(tmp)\n",
    "                all_suites.append([tmp])\n",
    "                one_suite.append(tmp)\n",
    "        else:\n",
    "            not_string_suites_names.append(row)\n",
    "            not_string_suites_rows.append(nos_data_clean['URN'].loc[row])\n",
    "            not_string_suites_titles.append(nos_data_clean['Title'].loc[row])\n",
    "            all_suites.append(['empty'])\n",
    "            one_suite.append('empty')\n",
    "    return all_suites, one_suite, not_string_suites_rows, not_string_suites_titles, not_string_suites_names\n",
    "\n",
    "# apply the above function once for the metadata suites:\n",
    "Nth = 200\n",
    "col = 'Suite'\n",
    "all_suites, one_suite, _, _, _ = get_all_and_single_suites(Nth,nos_data,col)\n",
    "nos_data['All_suites'] = all_suites\n",
    "nos_data['One_suite'] = one_suite\n",
    "\n",
    "'''\n",
    "# now for the pdf extracted suites, for an eventual comparison\n",
    "Nth = 125\n",
    "col = 'Clean_suite'\n",
    "all_suites, one_suite, _, _, _ = get_all_and_single_suites(Nth,nos_data,col)\n",
    "nos_data['All_suites'] = all_suites\n",
    "nos_data['One_suite'] = one_suite\n",
    "'''\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nos_data[[col,'All_suites','One_suite']].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now flatten the lists of lists with the suites and count how many unique suites there are:\n",
    "all_suites_flatten = flatten_lol(all_suites)\n",
    "# compare with the numbers from the ActiveIS metadata\n",
    "print('Unique new suites: ',len(list(set(all_suites_flatten))),'Unique ActiveIS suites: ', len(all_ground_suites))\n",
    "print('\\n')\n",
    "tmp = set(all_suites_flatten) - set(all_ground_suites)\n",
    "print('Suites ''obtained'' after joining all the data ({}):'.format(len(tmp)))\n",
    "print(tmp)\n",
    "print('\\n')\n",
    "tmp = set(all_ground_suites) - set(all_suites_flatten)\n",
    "print('Suites ''lost'' after joining all the data ({})'.format(len(tmp)))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot some summary statistics of fields content\n",
    "SAVEFIG2 = False\n",
    "cols2plot = ['Originating_organisation',\n",
    "        'Date_approved_year', 'Developed By',\n",
    "        'Validity', 'Clean Ind Review Year', 'NOS Document Status',\n",
    "        'NOSCategory', 'One_suite', 'Version_number']\n",
    "#        'Keywords', 'Occupations', 'Skills']\n",
    "hs = [10,5,8,5,5,5,5,10,5]\n",
    "ws = [18,12,18,6,12,6,6,18,9]\n",
    "with sns.plotting_context('talk'):\n",
    "    for ix,col in enumerate(cols2plot):\n",
    "        plt.figure(figsize = (ws[ix],hs[ix]))\n",
    "        tmp = nos_data[col].value_counts()\n",
    "        N = min(40,len(tmp))\n",
    "        tmp.iloc[:N].plot('bar')\n",
    "        plt.xlabel(col)\n",
    "        plt.tight_layout()\n",
    "        if SAVEFIG2:\n",
    "            plt.savefig(os.path.join(figure_dir, 'Most_common_{}_postjoining_{}.png'.format(col,qualifier)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the SOC codes separately.\n",
    "# NOTES: some are a list, some are wrong, some are less than 4 digits (only few exceptions: twentytwo 1s and one 9)\n",
    "col = 'SOC Code'\n",
    "with sns.plotting_context('talk'):\n",
    "    plt.figure(figsize = (21,5))\n",
    "    # separate groups of digits and eliminate everything that is not a digit\n",
    "    flattened_socs= flatten_lol(nos_data[col].map(lambda x: re.findall(r\"[\\d']+\", str(x))).values)\n",
    "    tmp = pd.DataFrame(flattened_socs)[0].value_counts()\n",
    "    tmp.iloc[0:50].plot('bar')\n",
    "    plt.xlabel('SOC codes, 4 digits')\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG2:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_4digit_postjoining_{}.png'.format(col,qualifier)))\n",
    "    # now plot for different number of digits\n",
    "    \n",
    "    ws = [10,15,20]\n",
    "    for ix in range(1,4):\n",
    "        plt.figure(figsize = (ws[ix-1],5))\n",
    "        tmp = pd.DataFrame(flattened_socs)[0].map(lambda x: x[0:ix]).value_counts()\n",
    "        if ix<3:\n",
    "            g = tmp.plot('bar')\n",
    "            g.set_xticklabels(g.get_xticklabels(), rotation = 0)\n",
    "        else:\n",
    "            g = tmp.iloc[0:50].plot('bar')\n",
    "        plt.xlabel('SOC codes, {} digit'.format(ix))\n",
    "        plt.tight_layout()\n",
    "        if SAVEFIG2:\n",
    "            plt.savefig(os.path.join(figure_dir, \n",
    "                                     'Most_common_{}_{}digit_postjoining_{}.png'.format(col,ix,qualifier)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually clean the SOC codes\n",
    "nos_data['Clean SOC Code'] = nos_data[col].map(lambda x: re.findall(r\"[\\d']+\", str(x)), na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many NOS are in the most common and the least common suites\n",
    "tmp = nos_data['One_suite'].value_counts()\n",
    "print('There are {} suites when only taking the first one.'.format(len(tmp)))\n",
    "print('The least common suites are (all appear only once):')\n",
    "for t in tmp.index[-40:]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the distribution of suites and originating organisations for NOS that have SOC codes\n",
    "# this needs to be moved to collate_and_analyse_nos\n",
    "for col in ['One_suite', 'Originating_organisation']:\n",
    "    duplicated_suites = df_nos[df_nos['Clean SOC Code'].notnull()][col].value_counts()\n",
    "    all_suites = df_nos[col].value_counts()\n",
    "    if col == 'One_suite':\n",
    "        plt.figure(figsize = (15,10))\n",
    "    else:\n",
    "        plt.figure(figsize = (15,7))\n",
    "    with sns.plotting_context('talk'):\n",
    "        duplicated_suites[0:40].plot('bar')\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH or True:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_proportions_for_soc_having_nos.png'.format(col)))\n",
    "\n",
    "    # divide by the sum (so we get fractions)\n",
    "    duplicated_suites = duplicated_suites.map(lambda x: x/duplicated_suites.sum())\n",
    "    all_suites = all_suites.map(lambda x: x/all_suites.sum())\n",
    "    \n",
    "    # get the ratio of proportions with respect to the full distribution\n",
    "    suites_ratio = {}\n",
    "    for row in duplicated_suites.index:\n",
    "        suites_ratio[row] = duplicated_suites.loc[row]/all_suites.loc[row]\n",
    "    suites_ratio = pd.DataFrame.from_dict(suites_ratio, orient = 'index', columns = ['ratio'])\n",
    "    # order by decreasing ratios \n",
    "    suites_ratio= suites_ratio.sort_values(by='ratio', ascending = False)\n",
    "    # plot the ratio\n",
    "    if col == 'One_suite':\n",
    "        plt.figure(figsize = (20,10))\n",
    "        ix = list(np.arange(0,50)) #list(np.arange(0,14))\n",
    "#        ix+= list(np.arange(15,51))\n",
    "    else:\n",
    "        plt.figure(figsize = (15,8.5))\n",
    "        ix = [0,1,2,4,5,6,7,9,11,13,14,15,17,19,20,21,22,23,24,25,27,29,30,\n",
    "            31,33,34,35,36,41,43,44,45,46,51,52] #[0,1,4,6,7,9,10,12,13,14,15,16,17] + list(np.arange(19,54))\n",
    "#        ix+= list(np.arange(15,50))\n",
    "        #print([(i,t) for i,t in enumerate(suites_ratio['ratio'].index[:55])])\n",
    "    with sns.plotting_context('talk'):\n",
    "        suites_ratio['ratio'].iloc[ix].plot('bar')\n",
    "    plt.tight_layout()\n",
    "    if SAVELSH or True:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_ratios_for_soc_having_nos.png'.format(col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the full text field, and save a subset of the data (only the relevant columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add one columns that collate all the data from multiple sections: title, overview, keywords, \n",
    "# relevant occupations (?), knowledge, performance, scope, glossary, behaviours, skills, values.\n",
    "\n",
    "def get_full_text(data_rows):\n",
    "    t0 = time.time()\n",
    "    full_texts = []\n",
    "    full_texts_keywords = []\n",
    "    for ii,row in enumerate(data_rows.index):\n",
    "        data_row = data_rows.loc[row]\n",
    "        for col in ['NOS Title', 'Knowledge_and_understanding', \n",
    "                    'Performance_criteria', 'Overview','Scope_range', 'Glossary', \n",
    "                    'Behaviours', 'Skills','Values','Occupations']: #'Relevant_occupations', #'extra_meta_info'?\n",
    "            if col == 'NOS Title':\n",
    "                full_texts.append(data_row[col] + ' ')\n",
    "                full_texts_keywords.append(data_row[col] + ' ')\n",
    "            else:\n",
    "                if isinstance(data_row[col],list): #not np.isnan(data_row[col]):\n",
    "                    L = len(data_row[col])\n",
    "                    for l in range(L):\n",
    "                        full_texts[-1] += data_row[col][l] + ' '\n",
    "                        full_texts_keywords[-1] += data_row[col][l] + ' '\n",
    "        # add the keywords columns only to the full_text_keywords\n",
    "        if isinstance(data_row['Keywords'], list):\n",
    "            L = len(data_row['Keywords'])\n",
    "            for l in range(L):\n",
    "                full_texts_keywords[-1] += data_row['Keywords'][l] + ' '\n",
    "        # keep track of where you are\n",
    "        if ii%1000 == 999:\n",
    "            print('Got to row {}. Time elapsed so far is {}'.format(ii,time.time()-t0))\n",
    "    return full_texts, full_texts_keywords\n",
    "\n",
    "out, _ = get_full_text(nos_data) #.apply(get_full_text, axis = 0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example of full text\n",
    "out[0][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the full_text column to the dataset: TODO: check\n",
    "nos_data_full = nos_data.join(pd.DataFrame(out, columns = ['full_text'], index = nos_data.index))\n",
    "#nos_data_full = nos_data_full.join(pd.DataFrame(out_kw, columns = ['full_text_keywords'], \n",
    "#                                                   index = nos_data_cleaner.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the relevant columns\n",
    "relevant_cols = ['Title', 'NOS Title', 'URN', 'Original URN', \n",
    "                 'Overview', 'Knowledge_and_understanding', 'Performance_criteria',\n",
    "                 'Scope_range', 'Glossary', 'Behaviours', 'Skills', 'Values', 'full_text',\n",
    "                 'Originating_organisation', 'Date_approved', 'Date_approved_year', \n",
    "                 'Indicative Review Date', 'Clean Ind Review Year', 'Version_number',\n",
    "                 'Links_to_other_NOS', 'External_Links', 'Developed By', 'Validity', \n",
    "                 'Keywords', 'Clean SOC Code', 'NOS Document Status', 'NOSCategory',\n",
    "                 'Suite', 'SuiteMetadata', 'Occupations', 'OccupationsMetadata', 'One_suite', 'All_suites',\n",
    "                 'notes', 'empty', 'extra_meta_info', 'Created', 'Modified', 'Item Type', 'Path']\n",
    "#final_relevant_cols = relevant_cols + ['full_text']\n",
    "nos_data_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset with only a subset of the columns (title, suite and full_text)\n",
    "#nos_data_full.to_csv(\n",
    "#    data_dir + '/all_nos_input_for_nlp.csv', columns = ['Title','full_text','Suite','All_suites','One_suite'], \n",
    "#    index = True, header = True)\n",
    "#[['Title','full_text','Suite','All_suites','One_suite']]\n",
    "nos_data_full[relevant_cols].to_pickle(\n",
    "    data_dir + '/all_nos_input_for_nlp_postjoining_{}.zip'.format(qualifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
