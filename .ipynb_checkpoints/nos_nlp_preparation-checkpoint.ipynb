{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs for data cleaning:\n",
    "\n",
    "1. remove square brackets\n",
    "2. make everything lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to prepare the text to then be used for NLP analysis. Mostly it is to tokenise the full text column.\n",
    "\n",
    "This specific instance of the notebook will be applied to the analysis of NOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two functions are useful for analysing bi and tri-grams with w2v models in gensim\n",
    "\n",
    "def convert_to_undersc(skill):\n",
    "    '''\n",
    "    convert spaces in skill phrases into underscores to use with trained\n",
    "    w2v model.\n",
    "    '''\n",
    "    if len(skill.split(' ')) >1:\n",
    "        new_i = '-'.join(skill.split(' '))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)\n",
    "\n",
    "def convert_from_undersc(skill):\n",
    "    '''\n",
    "    convert underscores between terms in skill phrases back to spaces.\n",
    "    '''\n",
    "    if len(skill.split('_')) >1:\n",
    "        new_i = ' '.join(skill.split('_'))\n",
    "    else:\n",
    "        new_i = skill\n",
    "    return(new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few functions for tyding up text\n",
    "def tag_for_lemmatise(s):\n",
    "    pos_to_wornet_dict = {\n",
    "        'JJ': 'a',\n",
    "        'JJR': 'a',\n",
    "        'JJS': 'a',\n",
    "        'RB': 'r',\n",
    "        'RBR': 'r',\n",
    "        'RBS': 'r',\n",
    "        'NN': 'n',\n",
    "        'NNP': 'n',\n",
    "        'NNS': 'n',\n",
    "        'NNPS': 'n',\n",
    "        'VB': 'v',\n",
    "        'VBG': 'v',\n",
    "        'VBD': 'v',\n",
    "        'VBN': 'v',\n",
    "        'VBP': 'v',\n",
    "        'VBZ': 'v',\n",
    "    }\n",
    "    try:\n",
    "        return pos_to_wornet_dict[nltk.pos_tag([s])[0][1]]\n",
    "    except:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatise(title_terms):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Removes suffixes if the new words exists in the nltk dictionary.\n",
    "    The purpose of the function is to convert plural forms into singular.\n",
    "    Allows some nouns to remain in plural form (the to_keep_asis is manually curated).\n",
    "    Returns a list.\n",
    "    >>> lemmatise(['teachers'])\n",
    "    ['teacher']\n",
    "    >>> lemmatise(['analytics'])\n",
    "    ['analytics']\n",
    "    \"\"\"\n",
    "    keep_asis = ['sales', 'years', 'goods', 'operations', 'systems',\n",
    "                    'communications', 'events', 'loans', 'grounds',\n",
    "                    'lettings', 'claims', 'accounts', 'relations',\n",
    "                    'complaints', 'services']\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    processed_terms = [wnl.lemmatize(i.strip()) if i.strip() not in keep_asis else i.strip() for i in title_terms]\n",
    "    #processed_terms = [wnl.lemmatize(i, pos = tag_for_lemmatise(i)) \n",
    "    #            if i not in keep_asis else i for i in title_terms]\n",
    "    return processed_terms\n",
    "\n",
    "def remove_digits(s):\n",
    "    \"\"\"\n",
    "    Takes a string as input.\n",
    "    Removes digits in a string.\n",
    "    Returns a string.\n",
    "    >>> remove_digits('2 recruitment consultants')\n",
    "    ' recruitment consultants'\n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in s if not i.isdigit())\n",
    "    return result\n",
    "\n",
    "def remove_list_enumeration(s):\n",
    "    '''\n",
    "    This is a specific requirement of the NOS that comes from\n",
    "    the presence of lists enumerated by strings like K+number\n",
    "    or P+number. Therefore, after \"lowerising\" and removing \n",
    "    digits, I look for and remove strings like \"k \" and \"p \"\n",
    "    '''\n",
    "    result = re.sub('( k )+',' ',s)\n",
    "    result = re.sub('( p )+', ' ', result)\n",
    "    #s.replace(' k ',' ').replace(' p ',' ') # I'm not sure this works very well \n",
    "    # it might not be necessary if I add 'k' and 'p' to stopwords\n",
    "    return result\n",
    "\n",
    "select_punct = set('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~') #only removed \"'\"\n",
    "extra_chars = set('–-•’”“µ¾âãéˆﬁ[€™¢±ï…˜')\n",
    "all_select_chars = select_punct.union(extra_chars)\n",
    "#print(all_select_chars)\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Removes punctuation from a string if the character is in select_punct.\n",
    "    Returns a string.\n",
    "   >>> replace_punctuation('sales executives/ - london')\n",
    "   'sales executives   london'\n",
    "    \"\"\"\n",
    "    for i in set(all_select_chars): #set(select_punct):\n",
    "        if i in s:\n",
    "            s = s.replace(i, ' ')\n",
    "    return s\n",
    "\n",
    "def tidy_desc(desc):\n",
    "    clean_data = desc.replace('\\r\\n', '').replace('\\xa0', '').replace('\\uf0b7','').replace('\\uf020','')\n",
    "    nodigits = remove_digits(clean_data.lower())\n",
    "    nopunct = replace_punctuation(nodigits)\n",
    "    nopunct = remove_list_enumeration(nopunct)\n",
    "    lemm = lemmatise(nopunct.split())\n",
    "    return ' '.join(lemm)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes string as input.\n",
    "    Returns list of tokens. The function is used as an argument for\n",
    "    TfidfVectorizer.\n",
    "    >>> tokenize('some job title')\n",
    "    ['some', 'job', 'title']\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_asis(some_list):\n",
    "    \"\"\"\n",
    "    Takes list as input.\n",
    "    Returns the list with elements converted to lower case. The function is \n",
    "    used as an argument for TfidfVectorizer.\n",
    "    \n",
    "    In [57]: tokenize(['Accounting', 'Microsoft Excel'])\n",
    "    Out[57]: ['accounting', 'microsoft excel']\n",
    "    \"\"\"\n",
    "    tokens = [elem.lower() for elem in some_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n",
    "\n",
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a pre-trained glove model into gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_dir = '/Users/stefgarasto/Local-Data/wordvecs/glove.twitter.27B'\n",
    "\n",
    "# to make the glove model file compatible with gensim\n",
    "#for dim in ['25','50','100','200']:\n",
    "##    glove_file = os.path.join(glove_dir,'glove.twitter.27B.{}d.txt'.format(dim))\n",
    "#    tmp_file = os.path.join(glove_dir, 'word2vec.glove.twitter.27B.{}d.txt'.format(dim) )\n",
    "#    _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "LOADGLOVE= False\n",
    "if LOADGLOVE:\n",
    "    # load the glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format\\\n",
    "    (os.path.join(glove_dir, 'word2vec.glove.twitter.27B.100d.txt'))\n",
    "    #model = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "    #model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "    #word_vectors = model.wv\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get raw data and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the NOS text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NOS data\n",
    "\n",
    "t0 = time.time()\n",
    "qualifier = 'postjoining_final_no_dropped'\n",
    "df_nos = pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}.zip'.format(qualifier))\n",
    "## relabel the index column\n",
    "#df_nos = df_nos.rename({'Unnamed: 0': 'urn_index'}, axis = 1)\n",
    "## re-set the previous index\n",
    "#df_nos = df_nos.set_index('urn_index')\n",
    "\n",
    "#Clean up description\n",
    "t0 = time.time()\n",
    "df_nos['clean_full_text'] = df_nos['full_text'].apply(lambda x:\\\n",
    "      tidy_desc(x))\n",
    "print('Time elapsed to tidy full text: {:4f}s'.format(time.time() - t0))\n",
    "\n",
    "# print an example output\n",
    "print('\\n Example of full text:')\n",
    "print(df_nos['clean_full_text'].iloc[0])\n",
    "\n",
    "print('\\n')\n",
    "print_elapsed(t0, 'loading and cleaning the full text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After cleaning, the number of unique suites (when taking the first one only) is: ',\n",
    "      len(df_nos['One_suite'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below would be useful if we wanted to test if there were misspelled words or words that were inadvertently \n",
    "#joined together\n",
    "#t0 = time.time()\n",
    "#df_nos['clean_full_text'] = df_nos['clean_full_text'].\\\n",
    "#apply(lambda x: ' '.join([elem for elem in x.split() if elem in model]))\n",
    "#print('Time elapsed: {:4f}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters for features extraction\n",
    "\n",
    "ngrams : uni/bi/tri\n",
    "\n",
    "parts of speech: verbs and/or nouns\n",
    "\n",
    "tfidf thresholds: min and max percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = 'uni'\n",
    "pofs = 'nv'\n",
    "tfidf_min = 2\n",
    "tfidf_max = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting important terms that describe NOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get standard stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Define additonal stopwords\n",
    "newStopWords = ['g','e g', 'uk', 'org', '-', '–', 'le', 'kpis', 'anti', 'client ’',\n",
    "                'l', '’ need', 'b', 'd', '”', '“', 'customer ’', \"'s\",\n",
    "                '‘',  'v', 'h', 'ass', 'http', 'http www', 'www', 'c', 'ac',\n",
    "                'skill –', 'h s', 'nh', 'customers ’', 'process e', \n",
    "                's requirement', 's degree', \"'\", \"organisation 's\", 'level',\n",
    "                'degree', 'de', '·', 'companies ’', 'e', '•', '’',\n",
    "               'aa','aaa','aac','aaes','aatcc','ab','abc','abv','k','p']\n",
    "\n",
    "#Cobmine standard and additional stopwords\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize with tf-idf - this is needed to get some of the noise as stopwords\n",
    "textfortoken= df_nos['clean_full_text']\n",
    "t0 = time.time()\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords,\n",
    "                        #ngram_range=(1,2), #if we are interested in bi-grams in addition to single terms\n",
    "                        max_df = 0.4, #if a term is in % documents greater than this value, it will be ignored\n",
    "                        min_df = 2) #number of documents that the terms should be present in to be included\n",
    "tfidfm = tfidf.fit_transform(textfortoken)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print_elapsed(t0, task = 'First tfidf vectorization from full text as one string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and add more stopwords\n",
    "more_stopwords = list(set([feat for feat in feature_names if len(feat)<4]) - set(['go','get','fat','fog','gap',\n",
    "                                                                                   'gpu', 'saw', 'tea', 'zip',\n",
    "                                                                                  'ion','law','man','map',\n",
    "                                                                                  'mat','men','met','xml','war',\n",
    "                                                                                   'wax', 'rna', 'row', 'rye',\n",
    "                                                                                  'way','web','eps','fit','nut',\n",
    "                                                                                  'set','son','toy','urn','url',\n",
    "                                                                                   'usb','blin','gov','owl','toe',\n",
    "                                                                                  'big','day','soy','csv']))\n",
    "\n",
    "#stopwords.extend(more_stopwords)\n",
    "\n",
    "even_more_stopwords = [' ', 'f', 'j', 'n', 'p', 'q', 'r', 'u', 'w', 'x', 'z', 'µ', '¾', 'â', 'ã', 'é', 'ˆ', 'ﬁ','[',\n",
    "                      'afcs','biao','bhas','clientsâ€™', 'clientâ€™s', 'client‟s', 'councilâ€™s', 'customersâ€™', \n",
    "                       'customerâ€™s','‟', '€', '™','¢', '±', 'ï', '‚', '…','clientsâ', 'clientâ', 'councilâ', \n",
    "                       'customersâ', 'customerâ','\\uf0b7','ccsrcs','woxnyqgl','sfjhg','skaa']\n",
    "\n",
    "stopwords.extend(even_more_stopwords)\n",
    "\n",
    "# do something about these: 'clientsâ€™', 'clientâ€™s', 'client‟s', councilâ€™s', 'customersâ€™', 'customerâ€™s'\n",
    "# 'developersâ€™', 'directorâ€™s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfortoken= df_nos['clean_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several strategies we could use to convert the corpus into a document-term matrix. Some of the most common ones are using:\n",
    "- Counts\n",
    "- Term frequency - inverse document frequency metric (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize with tf-idf: it's not really needed\n",
    "'''\n",
    "t0 = time.time()\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords,\n",
    "                        ngram_range=(1,2), #if we are interested in bi-grams in addition to single terms\n",
    "                        max_df = 0.4, #if a term is in % documents greater than this value, it will be ignored\n",
    "                        min_df = 2) #number of documents that the terms should be present in to be included\n",
    "tfidfm = tfidf.fit_transform(textfortoken)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print_elapsed(t0, task = 'Second tfidf vectorization from full text as one string')\n",
    "'''\n",
    "print('Not needed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might want to only select terms from a certain part of speach (e.g. nouns). This is how we'd do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "tokens = [nltk.word_tokenize(elem) for elem in textfortoken]\n",
    "print_elapsed(t0, task = 'tokenizing all NOS full texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(set(flatten_lol(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add even mode stopwords (don't know why they were not detected before)\n",
    "no_idea_why_here_stopwords = ['atn', 'yo', '\\uf020', 'pce', 'php', 'mar', 'hsi', 'itc', 'apm', 'crf', \n",
    "                              'rcs', 'wlc', 'rmr', 'yc', 'loa', 'eap', 'cyp', 'va', 'oms', 'pse',\n",
    "                              'dso', '©', 'mvt', 'bov', 'dfm', 'ksa', 'adn', 'lbl', 'adu', 'csf', 'yh', 'psf',\n",
    "                              'mba', 'msd', 'yl', 'mhe', '¨me', 'pl', 'big', 'rft', 'mva', 'ak', 'fsd', \n",
    "                              '≤', 'vu', '−', 'gog', 'rsm', 'gdb', 'dlp', 'dcc', 'ahd', 'mmc', 'cpv', 'avf', 'gnc', \n",
    "                              'gns', 'hz', 'eh', 'mvd', 'lzc', 'aoa', 'psc', 'phv', 'bct', 'plm', 'xps', 'dma', 'mrb',\n",
    "                              'nem', 'eba', 'qar', 'qrf', 'stp', 'mfp', 'cva', 'pvd', '£', '‰¤', 'ibc', 'tnc', 'hmi',\n",
    "                              'od', 'pih', 'god', 'tss', 'fad', 'ƒ', 'wui', 'ims', 'nee', 'ps', 'cps', 'ctu', 'lbp',\n",
    "                              'lm', 'wmv', 'fp', 'poe', 'rea', 'pps', 'dgr', 'pip', 'tda', 'ied', 'lub', 'lmc', 'mor',\n",
    "                              'cuv', 'dtf', 'eps', 'esl', 'cet', \"'ve\", 'hrm', 'gmc', 'ecf', 'lsb', 'wlr', \n",
    "                              'tn', 'kam', 'icd', 'lua', 'yw', 'isl', '\\uf020on', 'nrv', 'º', 'mcd', 'atr', \n",
    "                              'bf', 'duo', 'pme', 'jct', 'aaf', 'lr', 'pkm', 'bmt', 'br', 'fep', 'djy', 'siz',\n",
    "                              'cii', '\\uf032', 'yes', 'á', 'tvd', 'mve', 'cru', 'rpa', 'lf', 'hsp', 'upc', 'nov', \n",
    "                              'hl', 'dmf', 'blu', 'dga', 'hyn', 'ppf', 'smm', 'nf', 'jaa', \n",
    "                              'erf', 'elt', 'lly', 'rhi', 'gpu', 'ebb', 'pdm', 'ftk', 'bys', 'dpa',\n",
    "                              \"'so\", 'nad', 'à', 'ppq', '−−', 'bse', 'tt', 'jag',\n",
    "                              '°c', 'd°', 'hsb', 'drm', 'tps', 'tet', 'ig', 'fsc', 'fbp', 'foh', 'ux', 'gal', \n",
    "                              \"''\", \"'go\", 'dum', 'lmi', 'rfq', 'bap', 'iit', 'amf', 'mmf', 'ort', \n",
    "                              'bb', 'sft', 'rx', 'wft', 'mnt', 'ihu', 'tpe', 'noc', 'oft', 'gxk', 'flt', \n",
    "                              'abi', 'abl', 'rmg', '●', 'bsn', 'ffe', 'kwh', 'epi', 'cim', 'wml', 'ek', 'wy',\n",
    "                              'mᶟ', 'nm', 'cgi', 'ucm', 'hmt',  \"'on\", 'dvh', 'nde', '˜', 'xy', 'epr', \n",
    "                              'ifi', 'fbc', 'vdc', 'usa', 'ncc', 'ae', 'mu', 'dcr', 'sem', 'aqs', '″', 'æ', \n",
    "                              'plk', 'mph', 'lcd', 'kh', 'sma', 'rcm', 'hbw', 'bod', 'jl', 'tec', 'lpc', \n",
    "                              'dve', 'bdi', 'scr', 'cmp', 'wil', 'tor', '°', 'sot', 'pki', 'kb', \n",
    "                              'taw', 'ldm', 'pqq', '„', 'emr', 'piv', 'mfc', 'eaa', 'esr', 'svr', '¦', 'ple', \"'m\", \n",
    "                              'ley', 'bva', 'tl', 'iin', 'jay', 'tpz', 'baa', 'whe', 'enc', 'lx', \n",
    "                              'slm', 'amc', \"'re\", 'wgs', 'tlm', 'mtf', 'gnp', 'jic', 'ยบ', 'ff', 'fym', 'cpm', \n",
    "                              'dcu', 'mo', 'mrp', 'ico', 'utp', 'smb', '路', 'jmf', 'lec', 'in‟', 'yoi', \n",
    "                              'idl', 'ccd', '\\uf02d', 'toa', '\\uf033', 'md', 'ies', 'pdi', 'iea', 'mpo', \n",
    "                              'ipm', 'vdm', \"'d\", 'ead', 'scm', 'hey', 'dpv', 'epo', 'ocd', 'nhb', 'ri', \"'no\", \n",
    "                              'eir', 'hk', 'bsm', 'trv', 'ssc', 'gif', 'rao', 'cvd', \n",
    "                              'bpm', 'rfi', 'mvc', 'alt', 'cqa', 'hte', 'sad', 'pfa', 'lom', '\\uf020in', 'osi', \n",
    "                              'eqm', \"n't\", 'gby', 'ki', 'ebv', 'pfo', 'rj', 'sh', 'ilm', 'upf', 'csp', \n",
    "                              'lsi', 'pct', 'gek', 'hb', 'qad', 'tx', \"'as\", 'fam', \n",
    "                              'ltc', 'qim', 'mcc', '—', 'tc', 'hj', 'lne', 'cl', 'drr', 'ina', 'csi', 'fdr', \n",
    "                              'mou', 'xzw', 'ho', 'wh', 'pmu', 'ntw', \"'in\", 'ibr', '‚°', 'tue', 'cmv', \n",
    "                              '\\uf0d8', 'mon', 'awd', 'rlm', 'sor', 'msi', 'hta', 'dts', 'hpv', 'dvt', 'nil', 'cvc', \n",
    "                              'rcf', 'nop', 'thy', 'ltd', 'wga', 'nro', 'may', 'iy', 'spy', 'xo', 'kva', 'ykd', \n",
    "                              'spp', 'res', 'qr', 'sl', 'kp', 'mpv', 'fps', 'lsp', 'wo', '·a', 'μm', \n",
    "                              'vas', '©e', 'xed', 'ktp', 'scn', 'sif', 'ban', 'psv', 'nii', 'pvf', 'tob', 'ot', \n",
    "                              'rte', 'tna', 'tef', 'srr', '\\uf0a7', 'bnf', 'bbv', 'ssh', 'omf', 'lkp', \n",
    "                              '×', 'fhe', 'soe', 'csc', 'yg', 'grn', 'ith', 'bma', 'crp', 'edm', 'afv', 'dfa', \n",
    "                              'bdb','sksvr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the tokens\n",
    "REMOVE = False\n",
    "if REMOVE:\n",
    "    t0 = time.time()\n",
    "    tokens2 = [[t for t in elem if t not in stopwords+no_idea_why_here_stopwords] for elem in tokens]\n",
    "    print_elapsed(t0, task = 'eliminating stop words from tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(set(flatten_lol(tokens))),len(set(flatten_lol(tokens2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [len(t) for t in set(flatten_lol(tokens))]\n",
    "plt.hist(A)\n",
    "print(min(A))\n",
    "B = [t for t in set(flatten_lol(tokens)) if len(t)<4]\n",
    "#print(B)\n",
    "#for b in B:\n",
    "#    if b in stopwords:\n",
    "#        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('k' in set(flatten_lol(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:',len(tokens), '. Average nb of tokens in all documents:', \n",
    "      np.mean([len(ii) for ii in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "if REMOVE:\n",
    "    tags = [nltk.pos_tag(elem) for elem in tokens2]\n",
    "    print('Done - removing the stopwords')\n",
    "else:\n",
    "    tags = [nltk.pos_tag(elem) for elem in tokens]\n",
    "print_elapsed(t0, task = 'tagging all elements in NOS tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['get','write'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE:\n",
    "    df_nos['tokens'] = tokens2\n",
    "else:\n",
    "    df_nos['tokens'] = tokens\n",
    "df_nos['tagged_tokens'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "if pofs == 'n':\n",
    "    select = [[word for word,pos in elem if (pos == 'NN' or pos == 'NNP')]\n",
    "     for elem in tags]\n",
    "elif pofs == 'v':\n",
    "    select = [[word for word,pos in elem if (pos[0:2] == 'VB')]\n",
    "     for elem in tags]\n",
    "elif pofs == 'nv':\n",
    "    select = [[word for word,pos in elem if (pos[0:2] in ['NN','VB'])]\n",
    "     for elem in tags]\n",
    "else:\n",
    "    print('Wrong part of speech selected')\n",
    "    raise ValueError\n",
    "# manually remove \"k\"s and \"p\"s\n",
    "\n",
    "print_elapsed(t0, task = 'retaining only nouns from NOS tokenization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After keeping only the nouns. ''Number of documents:',len(select), '. Average nb of tokens in all documents:', \n",
    "      np.mean([len(ii) for ii in select]))\n",
    "s = 'finance'\n",
    "test = [elem for elem in select if s in elem]\n",
    "print('Percentage of documents with the word {} in them: {:2.2f}%'.format(s,len(test)/len(select)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0][0:20])\n",
    "test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nos['pruned'] = select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    # save the intermediate result: this contains all tokens that are names\n",
    "    if REMOVE:\n",
    "        qualifier = qualifier + '2'\n",
    "    df_nos[['pruned', 'clean_full_text', 'tagged_tokens']].to_pickle(lookup_dir + \n",
    "                                                'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(qualifier,pofs))\n",
    "    with open(lookup_dir + 'stopwords_for_nos_{}_{}.pickle'.format(qualifier,pofs),'wb') as f:\n",
    "        pickle.dump((stopwords, no_idea_why_here_stopwords, more_stopwords), f)\n",
    "if LOAD:\n",
    "    # load it if needs be\n",
    "    df_nos = df_nos.join(pd.read_pickle(lookup_dir + 'all_nos_input_for_nlp_{}_pruned_{}.zip'.format(\n",
    "                                        qualifier,pofs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
