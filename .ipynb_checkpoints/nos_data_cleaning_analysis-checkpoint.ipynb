{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 15 14:29:44 2019\n",
    "\n",
    "@author: stefgarasto\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot style\n",
    "print(plt.style.available)\n",
    "plt.style.use(['seaborn-darkgrid','seaborn-poster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nos import nesta_colours, nesta_colours_combos\n",
    "print(nesta_colours, nesta_colours_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/nlp_analysis/'\n",
    "lookup_dir = '/Users/stefgarasto/Google Drive/Documents/results/NOS/extracted/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier = 'final_no_dropped'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten lists of lists\n",
    "def flatten_lol(t):\n",
    "    return list(itertools.chain.from_iterable(t))\n",
    "flatten_lol([[1,2],[3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed(t0_local, task = 'current task'):\n",
    "    print('Done with {}. Elapsed time: {:4f}'.format(task,time.time()-t0_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Definitions\n",
    "def unique(list1):\n",
    "\n",
    "    # intilize a null list\n",
    "    unique_list = []\n",
    "    repeated_elems = []\n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "        else:\n",
    "            repeated_elems.append(x)\n",
    "    return unique_list, repeated_elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to clean the data\n",
    "They have been developed after manual exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make everything lowercase\n",
    "def make_lowercase(x):\n",
    "    if isinstance(x,str):\n",
    "        return x.lower().replace('â€™','').replace('â','')\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_suites_corrections(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.lower()\n",
    "        x = x.strip().replace('\\n','').replace('–','-').replace('&','and').replace('-',' ').replace('/',' ')\n",
    "        x = x.replace('(','').replace(')','').replace(':','').replace('\\'','')\n",
    "        x = x.replace('＆','and ')\n",
    "        x = ' '.join(x.split('\\n'))\n",
    "        #x = ' '.join(x.split())\n",
    "        # eliminate multiple spaces\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        # remove leading and trailing spaces\n",
    "        x = x.strip()\n",
    "        x = x.replace('trades','trade').replace('professionals','professional')\n",
    "        x = x.replace('jusitce','justice')\n",
    "        x = x.replace('turbines','turbine').replace('para professional','paraprofessional')\n",
    "        x = x.replace('orderivatives','or derivatives').replace('productions','production')\n",
    "        x = x.replace('securities derivatives', 'securities or derivatives').replace('persons','person')\n",
    "        x = x.replace('orderivatives','or derivatives')\n",
    "        x = x.replace('minerals','mineral')\n",
    "        x = x.replace('managemnet','management')\n",
    "        x = x.replace('monitorin operations','monitoring operations')\n",
    "        x = x.replace('digitial','digital')\n",
    "        x = x.replace('propr','props')\n",
    "        x = x.replace('pre cast','precast')\n",
    "        x = x.replace('fenestration i','fenestration, i')\n",
    "        x = x.replace('managmement','management')\n",
    "        x = x.replace('auxillary','auxiliary')\n",
    "        x = x.replace('installations','installation')\n",
    "        x = x.replace('vererinary','veterinary')\n",
    "        x = x.replace('managmement','management')\n",
    "        x = x.replace('wood machining','woodmachining')\n",
    "        x = x.replace('mangement','management')\n",
    "        x = x.replace('small holders','smallholders')\n",
    "        x = x.replace('digitial','digital')\n",
    "        x = x.replace('performanceandnbsp','performance')\n",
    "        x = x.replace('administration nos','administration')\n",
    "        x = x.replace('asphalting', 'asphalt')\n",
    "        x = x.replace('installing domestic fascia, soffit,', 'installation of domestic fascias, soffits')\n",
    "        x=x.replace('skills for security essential employability nos','skills for security essential employability')\n",
    "        x = x.replace('fenestration and installation surveying','fenestration, installation and surveying')\n",
    "        # ('nail services', 100), ('mail services', 92)\n",
    "        #x = x.replace('medicinal','medical')\n",
    "        #x = correction(x)\n",
    "    else:\n",
    "        print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: clean up rather than delete suites, also remove years from suites but keep the \"suite number\"\n",
    "# things to do to clean up suites names:\n",
    "'''\n",
    "1. remove brackets\n",
    "2. traderepair --> trade repair\n",
    "3. & to and\n",
    "4. check why sound and sound are split\n",
    "5. check wood constructions\n",
    "6. eliminate years\n",
    "7. plastering (construction), roofing occupations (construction) --> split. \n",
    "7a. Generally, split by comma too (not sure, actually)\n",
    "8. offshore s1971 elected safety representatives is the only one with a number in the middle\n",
    "8a. joking, there is also h2s & other hazardous gases; 1132 marketing and sales managers; \n",
    "9. remove - and similar (like hospitality â€“ generics)\n",
    "10. be careful with laboratory and associated technical activities suite 32010 (and 42019 and 22010)\n",
    "11. be careful of it users 6.2 too\n",
    "12. constructionindustry\n",
    "13. operationscoating\n",
    "24. remove leading and end spaces\n",
    "'''\n",
    "def clean_suites(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('(',' ')\n",
    "        x = x.replace(')',' ')\n",
    "        if ('22010' in x) or ('32010' in x) or ('42010' in x):\n",
    "            x = x[0:-4]\n",
    "        x = x.replace('&','and')\n",
    "        # remove year\n",
    "        #if 's1971' not in x:\n",
    "        #    x = re.sub('\\d{4}',' ', x)\n",
    "        #    if 'h2s' not in x:\n",
    "        #        # add a semicolon after a number so that you can split by it later\n",
    "        #        x = re.sub('(\\d)', r'\\1;', x)\n",
    "        x = x.replace('&', 'and ')\n",
    "        x = x.replace('â€“', '')\n",
    "        #x = x.replace('– ',' ')\n",
    "        x = x.replace('–',' ')\n",
    "        #x = x.replace('- ',' ')\n",
    "        x = x.replace('-',' ')\n",
    "        x = x.replace('/' ,' ')\n",
    "        x = x.replace('.',' ')\n",
    "        x = x.replace('\\'','')\n",
    "        # remove new line symbols\n",
    "        x = ' '.join(x.split('\\n'))\n",
    "        if 'editing, grip, design for the moving image,' in x:\n",
    "            x = ';'.join(x.split(','))\n",
    "        if 'operations construction, refractory installation' in x:\n",
    "            x = ';'.join(x.split(','))\n",
    "        # eliminate multiple spaces\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        # remove leading and trailing spaces\n",
    "        x = x.strip()\n",
    "        #TODO: with few exception if I find a group of max 3 digits not at \n",
    "        #the end and followed by a space or a letter, that's a point I should split\n",
    "        if ('wood occupation' in x) and ('adv' not in x):\n",
    "            x = 'wood occupations construction'\n",
    "        x = x.replace('traderepair', 'trade repair')\n",
    "        x = x.replace('constructionindustry','construction industry')\n",
    "        x = x.replace('operationscoating','operations coating')\n",
    "        x = x.replace('ofelectrical','of electrical')\n",
    "        x = x.replace('mineralsprocessing','minerals processing')\n",
    "        x = x.replace('differentcountries','different countries')\n",
    "        x = x.replace('combinedworking','combined working')\n",
    "        x = x.replace('localgovernment', 'local government')\n",
    "        x = x.replace('eventsecurity','event security')\n",
    "        x = x.replace('cropproduction','crop production')\n",
    "        x = x.replace('customerservices','customer services')\n",
    "        x = x.replace('occupationsresidential','occupations residential')\n",
    "        x = x.replace('systemsmaintenance','systems maintenance')\n",
    "        x = x.replace('andrefurbishment','and refurbishment')\n",
    "        x = x.replace('materialsprocessing','materials processing')\n",
    "        x = x.replace('thebuilding','the building')\n",
    "        x = x.replace('managerfire','manager fire')\n",
    "        x = x.replace('formanagement','for management')\n",
    "        x = x.replace('alliedoperation','allied operation')\n",
    "        x = x.replace('safetyrequirementsfabrication','safety requirements fabrication')\n",
    "        x = x.replace('operationsdrilling', 'operations drilling')\n",
    "        x = x.replace('operationsbulk', 'operations bulk')\n",
    "        x = x.replace('andmanagement', 'and management')\n",
    "        x = x.replace('artsmanagement','arts management')\n",
    "        x = x.replace('pensions','pension')\n",
    "        x = x.replace('extrusion furniture', 'extrusion and furniture')\n",
    "        x = x.replace('ofconstruction','of construction')\n",
    "        x = x.replace('2008aeronautical', '2008 aeronautical')\n",
    "        x = x.replace('2rail', '2 rail')\n",
    "        x = x.replace('2006engineering', '2006 engineering')\n",
    "        x = x.replace('1132 marketing','marketing')\n",
    "        x = x.replace('nail services 2009 2009', 'nail services 2009')\n",
    "        x = x.replace('roadbuilding','road building')\n",
    "        x = x.replace('seniorroles','senior roles')\n",
    "        x = x.replace('511con po1561', '')\n",
    "        x = x.replace('nailservices','nail services')\n",
    "        x = x.replace('servicesnos','services nos')\n",
    "        x = x.replace('sitelogistics','site logistics')\n",
    "        x = x.replace('securityoperations','security operations')\n",
    "        x = x.replace('andtendering','and tendering')\n",
    "        x = x.replace('andnbsp','')\n",
    "        x = x.replace('airconditioning', 'air conditioning')\n",
    "        x = x.replace('therapiharddwch','therapi harddwch')\n",
    "        # eliminate multiple spaces\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        # remove leading and trailing spaces\n",
    "        x = x.strip()\n",
    "        if x in ['',' ','skills cfa', ' skills for security essential employability',' skills for security']:\n",
    "            x = 'empty'\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform some more manual cleanup\n",
    "# this is for the document Status\n",
    "def lower_or_empty_status(x):\n",
    "    if isinstance(x,str):\n",
    "        y= x.lower()\n",
    "    else:\n",
    "        y= 'empty'\n",
    "    if y in ['orginal', 'origianl','orignal','original ??????????','original3']:\n",
    "        y = 'original'\n",
    "    elif y in ['import', 'imported from ento', 'amended form imported ento unit']:\n",
    "        y = 'imported'\n",
    "    elif y == 'tailored from ento':\n",
    "        y = 'tailored'\n",
    "    elif y == 'may 2016':\n",
    "        y = 'empty'\n",
    "    elif y == 'building information modelling':\n",
    "        y== 'empty'\n",
    "    # remove those that are \"out of range\"\n",
    "    if y not in ['original', 'tailored','imported']:\n",
    "        y = 'empty'\n",
    "    y = y.replace('&','and')\n",
    "    return y\n",
    "\n",
    "def lower_or_empty_validity(x):\n",
    "    if isinstance(x,str):\n",
    "        y= x.lower()\n",
    "    else:\n",
    "        y= 'empty'\n",
    "    # also, fix some common mis-spellings:\n",
    "    if y in ['currentl','curent']:\n",
    "        y = 'current'\n",
    "    elif y == 'may 2016':\n",
    "        y = 'empty'\n",
    "    elif y == 'building information modelling':\n",
    "        y== 'empty'\n",
    "    # remove those that are \"out of range\"\n",
    "    if y not in ['current','legacy','archive']:\n",
    "        y = 'empty'\n",
    "    y = y.replace('&','and')\n",
    "    return y\n",
    "\n",
    "# for the Developed_by columns make all lower case and drop some\n",
    "# obviously wrong rows\n",
    "def lower_or_empty(x):\n",
    "    if isinstance(x,str):\n",
    "        y= x.lower()\n",
    "    else:\n",
    "        y= 'empty'\n",
    "    # also, fix some common mis-spellings:\n",
    "    if y == 'may 2016':\n",
    "        y = 'empty'\n",
    "    elif y == 'building information modelling':\n",
    "        y = 'empty'\n",
    "    elif 'manufacturers instructions6method' in y:\n",
    "        y = 'empty'\n",
    "    elif y in ['proskills uk', 'manual, electronic systems, process warning enunciatorsprocess documents and logs, checklistssafety notice boards, safety data sheets, warning signsproskills',\n",
    "              'proskillls']:\n",
    "        y = 'proskills'\n",
    "    elif y in ['summitskillsversionnumber1', 'summit skills']:\n",
    "        y = 'summitskills'\n",
    "    elif y == 'manufacturersâ€™ instructions6method statements7product worksheetsengineering construction industry training board':\n",
    "        y = 'empty'\n",
    "    elif y in ['how signals can enter, route through and exit mixing and recordinghow to identify common signal routing stages of mixing and recordingthe relationship between the stereo master section and a singlechannel -describetypes of professional mixing and recording consolesthe features of different professional recording consolesthe types of console different music and sound sectors requirethe main sector manufacturers of equipment for studio recording andmixing studios/programming consolesthe main sector manufacturers of equipment forbroadcast/tv/av/radio consolesthe main sector manufacturers of equipment for live mobile recording,theatre/performance and live sound/pa equipment for venuescreative & cultural skills',\n",
    "              'creative and cultrual skills']:\n",
    "        y = 'creative and cultural skills'\n",
    "    elif y in ['skiillsactive','skills active', \n",
    "              'skillsactiveevelop your tactical skills to achieve excellence in your sport']:\n",
    "        y = 'skillsactive'\n",
    "    elif y in ['cfa', 'cfa business skill @ work', 'cfa business @ work',\n",
    "              'these business enterprise units may be relevant when you are setting up ordeveloping a business:ys1 explore your own business motivesys2 check your ability to run your businessys3 improve your business skillsee3 make deals to take your business forwardcfa business skills @ workz0g.docx']:\n",
    "        y = 'cfa business skills @ work'\n",
    "    elif y in ['mpqcrovide leadership in the mine',\n",
    "              'subsequent legislation that supersedes it, as appropriate to the workactivitympqclast to specification',\n",
    "              'resources: tools, equipment, utilities or services, consumablesmpqc',\n",
    "              'mpqceparate and dispose of wastes and by-products from mining and relatedprocessing activities',\n",
    "              'to the workplace, work activity and the associated environmental impactsmpqconitor and maintain environmental conditions in your area of responsibility',\n",
    "              'mpqcrocess materials to specification','[mpqc']:\n",
    "        y = 'mpqc'\n",
    "    elif y in ['6.46.5semtacomputer-based recordelectronic mail',\n",
    "               'complete the relevant paperwork, to include one from the following andpass it to the appropriate people:7.1 build records7.2 computer records7.3 job cards7.4 aircraft service/flight log other specific recording methodsemta']:\n",
    "        y = 'semta'\n",
    "    elif y == 'people1st':\n",
    "        y = 'people 1st'\n",
    "    elif y in ['security threatse-skills uk', 'e-skills',\n",
    "               'k12.5 verifying the accuracy, currency, completeness and relevance ofinformation created, collected, used and documented during datadesign activitiese-skills uk',\n",
    "              'e-skills uk ssc', 'e-skills uk sector skills council', 'skills uk']:\n",
    "        y = 'e-skills uk'\n",
    "    elif y == '[habia':\n",
    "        y = 'habia'\n",
    "    elif y == 'skills cfa in partnership with sfedi':\n",
    "        y = 'skills cfa'\n",
    "    elif y == 'skill for justice':\n",
    "        y = 'skills for justice'\n",
    "    elif y == 'assetskills':\n",
    "        y = 'asset skills'\n",
    "    elif y in ['the institute of the motor industry (imi)', 'imi ltd']:\n",
    "        y = 'imi'\n",
    "    elif y == 'enery and utility skills':\n",
    "        y = 'energy and utility skills'\n",
    "    elif y == 'improve ltd':\n",
    "        y = 'improve'\n",
    "    elif y in ['skills thirds sector', 'skills – third sector']:\n",
    "        y = 'skills third sector'\n",
    "    elif y in ['lifelong and skills improvement service', 'lifelong learning and skills improvement service',\n",
    "               'learning and skills improvement skills', 'learning and skills development service']: \n",
    "        y = 'learning and skills improvement service'\n",
    "    # finally, change & to and\n",
    "    y = y.replace('&','and')\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the year in which it was approved and the indicative review year\n",
    "# first you need to look for the month and the year\n",
    "def extract_monthyear(x):\n",
    "    reg_exp = 'jan|january|february|march|april|may|june|july|august|september|october|november|december'\n",
    "    p = re.compile(reg_exp)\n",
    "    # search for the regular expression and only get the first occurrence\n",
    "    # we will be assuming that the year is right after the month\n",
    "    if isinstance(x,str):\n",
    "        # first check if they are all digits:\n",
    "        if all([d.isdigit() for d in x]):\n",
    "            return x\n",
    "        if p.search(x.lower()):\n",
    "            start, end = p.search(x.lower()).span()\n",
    "            # get intermediate string:\n",
    "            tmp_s = x[start:end+10]\n",
    "            # find first and last digit in the intermediate string\n",
    "            digits = [ii for ii,d in enumerate(tmp_s) if d.isdigit()]\n",
    "            if len(digits):\n",
    "                year = tmp_s[digits[0]:digits[-1]+1]\n",
    "            else:\n",
    "                return 'july 2130'\n",
    "            # check if there is another string:\n",
    "            x2 = x[end:]\n",
    "            if p.search(x2.lower()):\n",
    "                start2, end2 = p.search(x2.lower()).span()\n",
    "                # get intermediate string:\n",
    "                tmp_s2 = x2[start2:end2+10]\n",
    "                # find first and last digit in the intermediate string\n",
    "                digits2 = [ii for ii,d in enumerate(tmp_s2) if d.isdigit()]\n",
    "                year2 = tmp_s2[digits2[0]:digits2[-1]+1]\n",
    "                if int(year2)>int(year):\n",
    "                    return x2[start2:end2].lower() + ' ' + year2\n",
    "                else:\n",
    "                    x[start:end].lower() + ' ' + year\n",
    "            else:\n",
    "                return x[start:end].lower() + ' ' + year\n",
    "        else:\n",
    "            return 'july 2130'\n",
    "    else:\n",
    "        return 'july 2130'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load metadata and light cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get the file where the metadata is stored\n",
    "metadata_dir = '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/'\n",
    "metadata_file = 'PublishedNos Details.xlsx'\n",
    "metadata_file_old = 'PublishedNos-old Details.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load old nos\n",
    "metadata_old = pd.read_excel(os.path.join(metadata_dir, metadata_file_old))\n",
    "metadata_old.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new nos\n",
    "metadata = pd.read_excel(os.path.join(metadata_dir, metadata_file))\n",
    "metadata.head(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new nos\n",
    "metadata2 = pd.read_excel(os.path.join(metadata_dir, metadata_file))\n",
    "print(metadata2['Suite'].isnull().sum())\n",
    "metadata_old2 = pd.read_excel(os.path.join(metadata_dir, metadata_file_old))\n",
    "print(metadata_old2['Suite'].isnull().sum())\n",
    "metadata2 = None\n",
    "metadata_old2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate all metadata (old and new)\n",
    "metadata_old.set_index = range(len(metadata),len(metadata)+len(metadata_old))\n",
    "metadata = pd.concat((metadata,metadata_old),ignore_index = True,sort=False)\n",
    "metadata.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light cleaning of NOS meta-data\n",
    " - eliminate rows with no information\n",
    " - remove duplicates\n",
    " - replace \"jobspecific\" with \"job specific\"\n",
    " - make lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it all lowercase\n",
    "metadata = metadata.applymap(make_lowercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the rows with no info\n",
    "metadata[metadata['NOS Title'].isnull()].head(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. eliminate rows with no information (see printout above)\n",
    "metadata = metadata[~metadata['NOS Title'].isnull()]\n",
    "\n",
    "# 2. known bug: some NOS are repeated (same pdf name) and only one of them actually contains relevant info\n",
    "metadata['Name'] = metadata['Name'].map(lambda x: x.strip())\n",
    "# find where the duplicates are (everything with value counts > 1, so find the first non-duplicate one)\n",
    "ix = np.where(metadata['Name'].value_counts().values==1)[0][0]\n",
    "repeated_meta_names = metadata['Name'].value_counts()[0:ix].index.values\n",
    "repeated_meta_counts = metadata['Name'].value_counts()[0:ix].values\n",
    "\n",
    "print(len(repeated_meta_names))\n",
    "for t in repeated_meta_names[:3]:\n",
    "    print(metadata[metadata['Name']==t][['SOC Code','Path','NOSCategory']])\n",
    "    print('\\n')\n",
    "\n",
    "# if necessary, print the duplicated rows\n",
    "PRINTONFILE = False\n",
    "if PRINTONFILE:\n",
    "    metadata[metadata['Name'].map(lambda x: x in repeated_meta_names)].to_csv(os.path.join(output_dir,\n",
    "                                                                    'repeated_metadata_pdfnames.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate file names (keep the last one because it has the SOC code, at least here)\n",
    "metadata.drop_duplicates(subset = 'Name', inplace = True, keep = 'last')\n",
    "metadata = metadata.reset_index(drop = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"jobspecific\" with \"job specific\" and plot counts\n",
    "print(metadata['NOSCategory'].value_counts())\n",
    "metadata['NOSCategory'][metadata['NOSCategory'] == 'jobspecific'] = 'job specific'\n",
    "f = plt.figure(figsize = (6,4))\n",
    "metadata['NOSCategory'].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Counts',fontsize = 18)\n",
    "plt.xlabel('NOS category',fontsize = 18)\n",
    "plt.tight_layout()\n",
    "#if SAVEFIG:\n",
    "#    plt.savefig(os.path.join(output_dir,'nos_categories_{}.png'.format(qualifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out how many repeated/missing URNs there are\n",
    "print(len(metadata), len(metadata['URN'].value_counts()))\n",
    "ix = np.where(metadata['URN'].value_counts()==1)[0][0]\n",
    "repeated_urns = metadata['URN'].value_counts()[0:ix].index.values\n",
    "print('Number of repeated URNs is: ',len(repeated_urns))\n",
    "print('Number of NAN URNs is: ',metadata['URN'].isnull().sum())\n",
    "# TODO: print all repeated URNs from ActiveIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some suites are lists separated by \";\": extract all unique suites\n",
    "all_ground_suites = []\n",
    "for row in metadata.index:\n",
    "    if isinstance(metadata['Suite'].loc[row],str):\n",
    "        all_ground_suites += metadata['Suite'].loc[row].split(\";\")\n",
    "        if 'ma' in metadata['Suite'].loc[row].split(\";\"):\n",
    "            # print the NOS with the weird suite\n",
    "            print(row,metadata['Name'].loc[row])\n",
    "t0 = time.time()\n",
    "\n",
    "print('Number of unique suites before cleaning: {}'.format(len(list(set(all_ground_suites)))))\n",
    "\n",
    "'''\n",
    "# clean the suites and check how many there are\n",
    "all_ground_suites_clean = [meta_suites_corrections(t) for t in all_ground_suites]\n",
    "print_elapsed(t0,'correcting ground truth suites')\n",
    "\n",
    "all_ground_suites_clean = list(set(all_ground_suites_clean))\n",
    "if all_ground_suites_clean[0]=='':\n",
    "    all_ground_suites_clean = all_ground_suites_clean[1:]\n",
    "print('Number of unique suites from Active IS metadata: ',len(all_ground_suites_clean))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I kind of want to show the process behind the data cleaning.\n",
    "# Note that so far, I have not actually cleaned the suite names\n",
    "# first, get unique suites\n",
    "all_ground_suites = list(set(all_ground_suites))\n",
    "print('Number of unique suites before any cleaning: {}'.format(len(all_ground_suites)))\n",
    "all_ground_suites = [t.strip() for t in all_ground_suites if len(t)]\n",
    "all_ground_suites = list(set(all_ground_suites))\n",
    "if all_ground_suites[0] == '':\n",
    "    all_ground_suites = all_ground_suites[1:]\n",
    "\n",
    "print('Number of unique suites after only removing spaces: {}'.format(len(all_ground_suites)))\n",
    "suites_checked = []\n",
    "METASUITEMATCH = False\n",
    "if METASUITEMATCH:\n",
    "    # for each of them print those with fuzzy matching more than tot\n",
    "    # do not do them twice, it just needs to be roughly consistent\n",
    "    suites_matched = {}\n",
    "    for ix in range(len(all_ground_suites)):\n",
    "        if all_ground_suites[ix] not in suites_checked:\n",
    "            out = process.extract(all_ground_suites[ix], all_ground_suites, limit=4)\n",
    "            scores = [t[1] for t in out]\n",
    "            max_ix = np.where(np.array(scores)>88)[0][-1]\n",
    "            suites_matched[all_ground_suites[ix]] = {}\n",
    "            for ix2 in range(1,max_ix+1):\n",
    "                suites_matched[all_ground_suites[ix]]['{}'.format(ix2)] = out[ix2][0]\n",
    "                suites_checked.append(out[ix2][0])\n",
    "    pd.DataFrame.from_dict(suites_matched, orient= 'index').to_csv(os.path.join(output_dir,\n",
    "                                                                    'potential_suites_matches_activeis_all.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and light clean data extracted from pdfs\n",
    " - replace missing values (or values that are not string) with either -1 (version number) or \"empty\"\n",
    " - mark those rows as empty\n",
    " - make lowercase\n",
    " - extract years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Load and concatenate all NOS\n",
    "\n",
    "data_dir = '../../results/NOS/extracted'\n",
    "figure_dir = '../../results/NOS/nlp_analysis'\n",
    "which_files = ['extracted_standards_New NOS 1.pickle', 'extracted_standards_New NOS 2.pickle']\n",
    "for ii in range(1,23):\n",
    "    which_files.append('extracted_standards_Old NOS {}.pickle'.format(ii))\n",
    "#which_file = 'extracted_standards_Old NOS 22.pickle'\n",
    "\n",
    "repeated_refs = []\n",
    "tot_count = 0\n",
    "# load all the extracted NOS and concatenate them\n",
    "for ii,which_file in enumerate(which_files):\n",
    "    version = 'new_' #'v2_'\n",
    "    print(version + which_file)\n",
    "    with open(os.path.join(data_dir, version + which_file),'rb') as f:\n",
    "        standard_info_partial, standard_ref_partial, failed, _ = pickle.load(f)\n",
    "    tot_count += len(standard_ref_partial)\n",
    "    # quick check that the list of keys in the standard dictionary is the\n",
    "    # same as the URNs in the standard_ref list\n",
    "    tmp = [k.replace('_v2','').replace('_v3','').replace('_v4','') for k in standard_info_partial.keys()]\n",
    "    if not unique(standard_ref_partial)[0] == unique(tmp)[0]:\n",
    "        print('Dictionary keys and standard refs do not correspond. Something is wrong.')\n",
    "    if ii == 0:\n",
    "        standard_info= standard_info_partial\n",
    "        standard_ref = standard_ref_partial\n",
    "    else:\n",
    "        # check if there is any ref that is already in the full list\n",
    "        ref_intersection= list(set(standard_ref_partial).intersection(set(standard_ref)))\n",
    "        if len(ref_intersection):\n",
    "            #keep track of the repeated ones and add _v followed by the next \n",
    "            # available number to their dict keys\n",
    "            repeated_refs = repeated_refs + ref_intersection\n",
    "            for ref in ref_intersection:\n",
    "                new_version = 2\n",
    "                while ref + '_v{}'.format(new_version) in list(standard_info_partial.keys()):\n",
    "                    new_version+=1\n",
    "                    #print(new_version)\n",
    "                standard_info_partial[ref + '_v{}'.format(new_version)\n",
    "                    ] = standard_info_partial.pop(ref)\n",
    "        standard_info.update(standard_info_partial)\n",
    "        standard_ref = standard_ref + standard_ref_partial\n",
    "\n",
    "\n",
    "standard_info_partial = None\n",
    "standard_ref_partial = None\n",
    "print(len(standard_info),tot_count)\n",
    "\n",
    "# NOTE: The total number of NOS is 22757\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it's fine to turn some of the fields into string\n",
    "# Specifically, these fields should be strings: URN, Title, Developed_by, Date_approved\n",
    "# Indicative review date, Validity, Status, Originating organisation, Suite\n",
    "# If it's not possible or the field is empty, then mark it as empty\n",
    "# OBS:I would keep the Original URN as a list, since it can contain multiple one\n",
    "# Also, version number should be an integer\n",
    "for idct, key_dct in enumerate(standard_info):\n",
    "    dct = standard_info[key_dct]\n",
    "    needed_sections = []\n",
    "    for key in ['Developed_by','Date_approved', 'Indicative_review_date', \n",
    "                'Originating_organisation', 'Status','Suite', 'Validity','Version_number']:\n",
    "        if key in dct.keys():\n",
    "            if len(dct[key]):\n",
    "                tmp = dct[key][0]\n",
    "                for ii in range(1,len(dct[key])):\n",
    "                    tmp += dct[key][ii]\n",
    "                dct[key] = tmp\n",
    "                if key=='Version_number':\n",
    "                    # sometimes, the Version number contains spurious characters, not only digit\n",
    "                    # so I need to dig out whether there is a digit in it and take it out\n",
    "                    # yes: sometimes this can return the wrong value, because it will stop at the\n",
    "                    # first digit it encounters\n",
    "                    s = dct['Version_number']\n",
    "                    re_result = re.search('\\d{1,2}',s) #('\\d+',s)\n",
    "                    if re_result:\n",
    "                        dct['Version_number'] = int(s[re_result.span()[0]:re_result.span()[1]])\n",
    "                    else:\n",
    "                        dct['Version_number'] = -1\n",
    "            else:\n",
    "                # in this case the field was an empty list, meaning the section was empty.\n",
    "                if key=='Version_number':\n",
    "                    dct['Version_number'] = -1\n",
    "                else:\n",
    "                    dct[key] = 'empty'\n",
    "    \n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Turn the nested dictionary into a dataframe\n",
    "nos_data = pd.DataFrame.from_dict(standard_info, orient = 'index')\n",
    "print(nos_data.columns)\n",
    "# mark all the rows with at least one column == 'empty' or == -1 as empty (see above as to why)\n",
    "nos_data['empty'] = nos_data.apply(lambda x: (x=='empty') | (x==-1)).any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make everything lowercase\n",
    "nos_data = nos_data.applymap(make_lowercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the month for the indicative review date, turn into a datetime object, extract the year\n",
    "nos_data['Indicative_review_year'] = pd.to_datetime(\n",
    "    nos_data['Indicative_review_date'].map(extract_monthyear),errors= 'coerce',infer_datetime_format=True).map(\n",
    "    lambda x: x.year)\n",
    "\n",
    "# do the same for the date approved field\n",
    "nos_data['Date_approved_year'] = pd.to_datetime(\n",
    "    nos_data['Date_approved'].map(extract_monthyear),errors= 'coerce',infer_datetime_format=True).map(\n",
    "    lambda x: x.year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy clean of the pdf extracted data\n",
    "- clean the suites names (see function above)\n",
    "- clean Status, Validity and Developed_by (see functions above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cleans the pdf extracted suites\n",
    "nos_data_clean = nos_data\n",
    "nos_data_clean['Clean_suite'] = nos_data_clean['Suite'].map(clean_suites)\n",
    "print('Done')\n",
    "nos_data_clean[['Clean_suite','Suite']].sample(n=10)\n",
    "print('Number of unique suites before and after cleaning: {} and {}'.format(\n",
    "        len(nos_data_clean['Suite'].value_counts()),\n",
    "        len(nos_data_clean['Clean_suite'].value_counts())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pdf extracted suites, it's probably better to show potential matches after the data cleaning, \n",
    "# since the data cleaning I already do is fairly obvious (mostly it's white space that's missing)\n",
    "# So first, get unique suites\n",
    "all_pdf_suites0 = list(set(list(nos_data_clean['Clean_suite'])))\n",
    "# some of them have multiple suites separated by ;, split them\n",
    "all_pdf_suites = []\n",
    "for row in all_pdf_suites0:\n",
    "    if isinstance(row,str):\n",
    "        all_pdf_suites += row.split(\";\")\n",
    "#all_pdf_suites = [t for t in all_pdf_suites if isinstance(t,str)]\n",
    "all_pdf_suites = [t.strip() for t in all_pdf_suites if len(t)]\n",
    "all_pdf_suites = list(set(all_pdf_suites))\n",
    "all_pdf_suites = all_pdf_suites[1:]\n",
    "pdf_suites_checked = []\n",
    "# for each of them print those with fuzzy matching more than tot\n",
    "# do not do them twice, it just needs to be roughly consistent\n",
    "PDFSUITEMATCH = False\n",
    "if PDFSUITEMATCH:\n",
    "    pdf_suites_matched = {}\n",
    "    for ix in range(len(all_pdf_suites)):\n",
    "        if all_pdf_suites[ix] not in pdf_suites_checked:\n",
    "            out = process.extract(all_pdf_suites[ix], all_pdf_suites, limit=4)\n",
    "            scores = [t[1] for t in out]\n",
    "            try:\n",
    "                max_ix = np.where(np.array(scores)>88)[0][-1]\n",
    "            except:\n",
    "                print(all_pdf_suites[ix])\n",
    "                stop\n",
    "            pdf_suites_matched[all_pdf_suites[ix]] = {}\n",
    "            for ix2 in range(1,max_ix+1):\n",
    "                pdf_suites_matched[all_pdf_suites[ix]]['{}'.format(ix2)] = out[ix2][0]\n",
    "                pdf_suites_checked.append(out[ix2][0])\n",
    "    pd.DataFrame.from_dict(pdf_suites_matched, orient= 'index').to_csv(os.path.join(output_dir,\n",
    "                                                                        'potential_suites_matches_pdfs_all.csv'))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean other columns\n",
    "nos_data_clean['Status'] = nos_data_clean['Status'].map(lower_or_empty_status)\n",
    "nos_data_clean['Validity'] = nos_data_clean['Validity'].map(lower_or_empty_validity)\n",
    "nos_data_clean['Developed_by'] = nos_data_clean['Developed_by'].map(lower_or_empty)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# just change the incredibly long one in the validity field\n",
    "tmp = [nos_data_clean['Validity'].value_counts().index][0]\n",
    "tmp = [t for t in tmp if len(t)> 100]\n",
    "if len(tmp):\n",
    "    tmp = tmp[0]\n",
    "    for row in nos_data_clean.index:\n",
    "        if nos_data_clean['Validity'].loc[row] == tmp:\n",
    "            nos_data_clean['Validity'].loc[row] = 'current'\n",
    "            print(row)\n",
    "            print(nos_data_clean['file_name'].loc[row])\n",
    "            print(nos_data_clean['Title'].loc[row])\n",
    "'''\n",
    "            \n",
    "SAVEFIG = False\n",
    "for col in ['Status', 'Validity']:\n",
    "    plt.figure(figsize = (6,4))\n",
    "    tmp =nos_data_clean[col].value_counts()[::-1].plot('barh', color = nesta_colours[3])\n",
    "    plt.ylabel(col, fontsize = 18)\n",
    "    plt.xlabel('Counts',fontsize = 18)\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_{}.png'.format(col,qualifier)))\n",
    "    \n",
    "for col in ['Developed_by']:\n",
    "    plt.figure(figsize = (5,15))\n",
    "    tmp = nos_data_clean[col].value_counts()[::-1].plot('barh', color = nesta_colours[3])\n",
    "    plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    plt.xlabel('Counts',fontsize = 18)\n",
    "    if SAVEFIG:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_{}.png'.format(col,qualifier)))\n",
    "    #print(nos_data_clean[col].value_counts())\n",
    "# TODO: get empty rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data_clean.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy clean of the Active IS data\n",
    "- clean suites names\n",
    "- delete variable with only \"old NOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, split, clean and rejoin the suites\n",
    "def clean_meta_suites(x):\n",
    "    if isinstance(x,str):\n",
    "        x = \";\".join([meta_suites_corrections(t) for t in x.split(';')])\n",
    "        if x[-1]==';':\n",
    "            x = x[:-1]\n",
    "    return x\n",
    "\n",
    "A = metadata['Suite']\n",
    "metadata['Suite'] = metadata['Suite'].map(clean_meta_suites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many unique suites after cleaning\n",
    "new_ground_suites = []\n",
    "for row in metadata.index:\n",
    "    if isinstance(metadata['Suite'].loc[row],str):\n",
    "        new_ground_suites += metadata['Suite'].loc[row].split(\";\")\n",
    "t0 = time.time()\n",
    "\n",
    "print('Number of unique suites after cleaning: {}'.format(len(list(set(new_ground_suites)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_old = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} rows in the metadata with null SOC code'.format(metadata['SOC Code'].isnull().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join pdf-extracted data and the metadata from ActiveIS\n",
    "\n",
    "Note that:\n",
    "- both datasets should have already been cleaned. However, some leading and trailing spaces might still appear when we split the suites by \";\"\n",
    "- some datasets have different file names but the same URN: I still join them by URN (252 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of NOS extracted from pdfs is {}'.format(len(nos_data_clean)))\n",
    "print('Number of NOS for whom we have metadata is {}'.format(len(metadata)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some filenames appear different but they actually do have a match: find that match by using the URN\n",
    "t0 = time.time()\n",
    "missings = list(set(metadata['Name'])-set(nos_data_clean['file_name']))\n",
    "# + len(set(nos_data_clean['file_name'])-set(metadata['Name']))\n",
    "counter = 0\n",
    "for ix in missings:\n",
    "    row = metadata[metadata['Name']==ix]\n",
    "    urn = row['URN'].values\n",
    "    if len(urn)==1:\n",
    "        row2 = nos_data_clean[nos_data_clean['URN']==urn[0]]\n",
    "        if len(row2):\n",
    "            counter += 1\n",
    "            ixx = row2.index[0]\n",
    "            nos_data_clean['file_name'].loc[ixx] = ix\n",
    "print(counter,len(missings),'{:.4f}'.format(time.time()-t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered = counter\n",
    "metadata_missing = len(list(set(metadata['Name'])-set(nos_data_clean['file_name'])))\n",
    "intersected = set(metadata['Name']).intersection(set(nos_data_clean['file_name']))\n",
    "print('Number of NOS matched from metadata to pdf-extracted is {}'.format(len(list(intersected))))\n",
    "print('Number of metadata NOS not matched is {}'.format(metadata_missing))\n",
    "print('Number of pdf-extracted NOS not matched is {}'.format(\n",
    "    len(list(set(nos_data_clean['file_name']) - set(metadata['Name'])))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nos_data = nos_data_clean.set_index('file_name').join(metadata.set_index('Name'), lsuffix = '_2') \n",
    "tmp = nos_data.isnull().sum()\n",
    "print('Missing values from the joined dataframe: ')\n",
    "for col in nos_data.columns.sort_values():\n",
    "    print(col,'-',tmp[col])\n",
    "#print(len(nos_data)-len(metadata)+523-252)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse mismatch between Active IS and pdf extracted data\n",
    "- take the intersection\n",
    "- compare columns (sometimes I remove spaces to avoid extra spaces influencing the comparison too much)\n",
    "- save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two dataframes as intersection\n",
    "nos_data_int = nos_data_clean.set_index('file_name').join(metadata.set_index('Name'), lsuffix = '_2',how = 'inner') \n",
    "#nos_data_int.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year from metadata\n",
    "nos_data_int['Indicative Review Year'] = pd.DatetimeIndex(nos_data_int['Indicative Review Date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = [['Developed By', 'Developed_by'], \n",
    "         ['Indicative Review Year', 'Indicative_review_year'], ['Keywords', 'Keywords_2'], \n",
    "         ['NOS Document Status', 'Status'], ['Validity', 'Validity_2'],\n",
    "        ['NOS Title','Title'], ['Suite','Clean_suite'], ['URN', 'URN_2']]\n",
    "A = []\n",
    "t0 = time.time()\n",
    "for p in range(len(pairs)):\n",
    "    print(pairs[p])\n",
    "    if 'Year' in pairs[p][0]:\n",
    "        condition = nos_data_int[pairs[p][0]] != nos_data_int[pairs[p][1]]\n",
    "    else:\n",
    "        if 'Validity' in pairs[p][0]:\n",
    "            activeis_str = lambda row: '{}'.format(nos_data_int[pairs[p][0]].loc[row]).replace(' ','')\n",
    "        else:\n",
    "            activeis_str = lambda row: '{}'.format(nos_data_int[pairs[p][0]].loc[row]).replace(' ',\n",
    "                                                                                        '').replace('legacy','')\n",
    "        pdf_str = lambda row: '{}'.format(nos_data_int[pairs[p][1]].loc[row]).replace(' ','')\n",
    "        # find those that have overlap lower than a certain threshold\n",
    "        # also, check the length - all these fields should not be particularly long. \n",
    "        # If they are too long, the pdf extraction likely went wrong\n",
    "        condition = [(process.extract(activeis_str(row), [pdf_str(row)])[0][1]<80) \n",
    "                         & (len(pdf_str(row))<3*len(activeis_str(row)))\n",
    "                     for row in nos_data_int.index]\n",
    "    A.append(nos_data_int[condition])\n",
    "print('Time: {:.4f}'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "PRINTMISMATCHES = False\n",
    "for p in range(len(A)):\n",
    "    print(len(A[p]))\n",
    "    print(A[p][pairs[p]].sample(n=10))\n",
    "    if PRINTMISMATCHES:\n",
    "        A[p][pairs[p]].to_csv(os.path.join(output_dir,'mismatches_pdf_activeis_{}_new.csv'.format(pairs[p][1])))\n",
    "    print('-'*116)\n",
    "print('Time: {:.4f}'.format(time.time()-t0))\n",
    "'''\n",
    "# one case: COSVR27 Produce complex shaped product details (in old nos 5)\n",
    "The active IS spreadsheet writes it down as Wood Occupations (construction)\n",
    "The pdf instead has heritage skills (construction).\n",
    "It says it applies to carpenter and joiners but I don't see much about wood\n",
    "ActiveIS and the pdf agrees on the keywords and relevant occupations though\n",
    "'''\n",
    "'''\n",
    "Note: the keyword comparison is fairly difficult to do, because it could be a \n",
    "different order. I would ignore it.\n",
    "Note: the indicative review date field might be particulary interesting because\n",
    "lots of the rows in the ActiveIS meta-data are empty/wrong\n",
    "Note: I think skillset has been reusing old URNs, but changing the content a fair\n",
    "bit. For example, SKSJ17 has formerly been used for \"Write for online \n",
    "distribution\" and now for \"Acquire content material for editorial use\"\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing of joined dataset\n",
    "- when the metadata doesn't exist, replace it with the pdf-extracted data\n",
    "- print repeated file names, URNs, and NOS titles\n",
    "- drop rows with same file name as another row (repeated indices)\n",
    "- extract year from metadata\n",
    "- replace missing or wrong year (i.e. = 1905) with year extracted from pdf. Now 2030 represents a missing year\n",
    "- for multi-suites NOS, get full list of suites and just the first one (with some light-touch cleaning of known bad cases)\n",
    "- save list of unique suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[['Clean_suite','Suite']].sample(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the existing metadata when it's null\n",
    "cols2copy = ['Developed By', 'NOS Title', 'Occupations', \n",
    "       'Validity', 'NOS Document Status', 'URN',\n",
    "       'Keywords', 'Original URN', 'Suite']\n",
    "backup_cols = ['Developed_by', 'Title', 'Relevant_occupations',\n",
    "       'Validity_2', 'Status', 'URN_2',\n",
    "        'Keywords_2', 'Original_URN', 'Clean_suite']\n",
    "for ix,col in enumerate(cols2copy):\n",
    "    nullrows = nos_data[col].isnull()\n",
    "    nos_data[col].loc[nullrows] = nos_data[backup_cols[ix]].loc[nullrows]\n",
    "nos_data['Suite'][nos_data['Suite'].isnull()] = 'others'\n",
    "# The year needs to be dealt with separately\n",
    "# Indicative Review Date - 1029\n",
    "# Indicative_review_date - 260\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (~nos_data.isnull()).sum()\n",
    "print('Non missing values from the joined dataframe (with {} NOS): '.format(len(nos_data)))\n",
    "for col in nos_data.columns.sort_values():\n",
    "    print(col,'-',tmp[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find repeated file names, URNs and NOS titles\n",
    "# store repeated indices (that is, file names) - I already removed duplicate file names from the metadata, \n",
    "# so I inherited all possible duplicate file names from nos_data\n",
    "ix = np.where(nos_data.index.value_counts().values==1)[0][0]\n",
    "repeated_names = list(nos_data.index.value_counts()[0:ix].index.values)\n",
    "repeated_names_counts = list(nos_data.index.value_counts()[0:ix].values)\n",
    "repeated_names += list(repeated_meta_names)\n",
    "\n",
    "# now print the repeated URNs\n",
    "col = 'URN'\n",
    "ix = np.where(nos_data[col].value_counts().values==1)[0][0]\n",
    "repeated_urns = nos_data[col].value_counts()[0:ix].index.values\n",
    "repeated_urns_counts = nos_data[col].value_counts()[0:ix].values\n",
    "\n",
    "col = 'NOS Title'\n",
    "ix = np.where(nos_data[col].value_counts().values==1)[0][0]\n",
    "repeated_titles = nos_data[col].value_counts()[0:ix].index.values\n",
    "repeated_titles_counts = nos_data[col].value_counts()[0:ix].values\n",
    "\n",
    "# now, get rows for whom both URN and Title are exactly the same\n",
    "B = nos_data[nos_data.duplicated(subset= ['NOS Title','URN'])]\n",
    "\n",
    "# save them for another time\n",
    "with open(os.path.join(figure_dir,'repeated_rows.pickle'),'wb') as f:\n",
    "    pickle.dump((repeated_names,repeated_names_counts,repeated_urns,repeated_urns_counts,repeated_titles,\n",
    "               repeated_titles_counts,B,['repeated_names','repeated_names_counts','repeated_urns',\n",
    "                                         'repeated_urns_counts','repeated_titles','repeated_titles_counts',\n",
    "                                         'Titles+URNs duplicates']), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, drop the duplicate indices\n",
    "nos_data = nos_data.groupby(nos_data.index).first()\n",
    "## now, drop rows that have both URN and Title exactly the same\n",
    "#B = A.drop_duplicates(subset = ['NOS Title','URN'])\n",
    "#len(B),len(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the year from the metadata information. As expected, lots of them have the year 1905\n",
    "nos_data['Indicative Review Year'] = pd.to_datetime(nos_data['Indicative Review Date'],\n",
    "                                                         errors = 'coerce').map(lambda x: x.year)\n",
    "\n",
    "# for the rows with year = 1905.0 or that are null it's better to use the extracted ones\n",
    "nos_data['Clean Ind Review Year'] = nos_data['Indicative Review Year']\n",
    "nos_data['Clean Ind Review Year'].loc[nos_data['Indicative Review Year']==1905.0] = nos_data[\n",
    "    'Indicative_review_year'].loc[nos_data['Indicative Review Year']==1905.0]\n",
    "\n",
    "# also replace the rows with a null value with the extracted year\n",
    "nullrows = nos_data['Clean Ind Review Year'].isnull()\n",
    "nos_data['Clean Ind Review Year'].loc[nullrows] = nos_data['Indicative_review_year'].loc[nullrows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Analyse the suite fields to check for possible outliers. Specifically, I want to find a length threshold to \n",
    "# discard spurious elements (some might be mistakes, some might be multiple suites combined into one)\n",
    "\n",
    "# One suite fields, can contain multiple suites split by ';'. Perform the split and\n",
    "# get the lenght of each sub-element\n",
    "\n",
    "suite_len = []\n",
    "all_suites = []\n",
    "not_string_suites_rows = []\n",
    "not_string_suites_titles = []\n",
    "not_string_suites_names = []\n",
    "col2use = 'Suite'\n",
    "for row in nos_data.index:\n",
    "    if isinstance(nos_data[col2use].loc[row],str):\n",
    "        # split by ;\n",
    "        y = nos_data[col2use].loc[row].split(';')\n",
    "        # remove extra white spaces\n",
    "        y = [t.strip() for t in y]\n",
    "        # collect suite lengths and suites\n",
    "        suite_len += [len(t) for t in y]\n",
    "        all_suites += y\n",
    "    else:\n",
    "        not_string_suites_rows_names.append(row)\n",
    "        not_string_suites.append(nos_data['URN'].loc[row])\n",
    "        not_string_suites_titles.append(nos_data['Title'].loc[row])\n",
    "        \n",
    "\n",
    "# get the unique list of suites\n",
    "unique_suites = list(set(all_suites))\n",
    "unique_len = [len(t) for t in unique_suites]\n",
    "sort_ind = np.argsort(unique_len)\n",
    "# order them by length\n",
    "unique_len = itemgetter(*sort_ind)(unique_len)\n",
    "unique_suites = itemgetter(*sort_ind)(unique_suites)\n",
    "\n",
    "# now print all the suites that contain more than N characters: how many can we safely disregard? Is there a\n",
    "# threshold that separates good from bad ones:\n",
    "N = 90\n",
    "# collect the indices of unique suites that are longer than N characters\n",
    "tmp = [ t for t,tval in enumerate(unique_len) if tval>N]\n",
    "for t in tmp:\n",
    "    print(unique_len[t],unique_suites[t])\n",
    "    print('\\n')\n",
    "    \n",
    "# Observation: if I select threshold = 90, I shouldn't be keeping any \"bad\" suites, but I'll be discarding some \n",
    "# good ones. There are, I think, 4 good and 3 bad ones that are between 90 and 122 characters. \n",
    "# DECISION: Let's set the threshold to 125\n",
    "print(not_string_suites_rows)\n",
    "'''\n",
    "print('Not needed anymore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle again through the dataframe to keep the whole suites list, have one extra column with only the first \n",
    "# suite in each list (not ideal, I know, but then I can categorise all the rows), and eliminate the \n",
    "# sub-elements that are longer than Nth characters.\n",
    "# Note that for rows with only one suite to begin with, nothing should change\n",
    "\n",
    "def get_all_and_single_suites(Nth,nos_data_clean,col):\n",
    "    # this function splits the suites by \";\", joins them as a list (after removing ones that are too long), \n",
    "    # returns the full list and the first element\n",
    "    not_string_suites_rows = []\n",
    "    not_string_suites_titles = []\n",
    "    not_string_suites_names = []\n",
    "    all_suites = []\n",
    "    one_suite = []\n",
    "    for row in nos_data_clean.index:\n",
    "        if isinstance(nos_data_clean[col].loc[row],str):\n",
    "            # split by \";\"\n",
    "            y = nos_data_clean[col].loc[row].split(';')\n",
    "            # remove the ones longer than a threshold\n",
    "            y = [t for t in y if len(t)<Nth]\n",
    "            if len(y):\n",
    "                # remove extra spaces and strings of length 0\n",
    "                y = [t.strip() for t in y]\n",
    "                y = [t for t in y if len(t)]\n",
    "                # append full list\n",
    "                all_suites.append(y)\n",
    "                # append first suite\n",
    "                one_suite.append(y[0])\n",
    "            else:\n",
    "                # this is for when all suites are too long - they're specific cases extracted from pdf\n",
    "                # get all individual words and keep the unique ones\n",
    "                tmp= ''\n",
    "                y = nos_data_clean[col].loc[row].split(' ')\n",
    "                for t in y:\n",
    "                    if t not in tmp:\n",
    "                        tmp += ' ' + t\n",
    "                    else:\n",
    "                        break\n",
    "                # now split by comma\n",
    "                tmp = tmp.split(',')[0].strip()\n",
    "                print(tmp)\n",
    "                all_suites.append([tmp])\n",
    "                one_suite.append(tmp)\n",
    "        else:\n",
    "            # this is actually useless at the moment (it's legacy code)\n",
    "            not_string_suites_names.append(row)\n",
    "            not_string_suites_rows.append(nos_data_clean['URN'].loc[row])\n",
    "            not_string_suites_titles.append(nos_data_clean['Title'].loc[row])\n",
    "            all_suites.append(['empty'])\n",
    "            one_suite.append('empty')\n",
    "    return all_suites, one_suite, not_string_suites_rows, not_string_suites_titles, not_string_suites_names\n",
    "\n",
    "# apply the above function once for the metadata suites:\n",
    "Nth = 200\n",
    "# 'Suite' comes from the meta-data, augmented with the clean suites from the pdf if missing\n",
    "col = 'Suite'\n",
    "all_suites, one_suite, _, _, _ = get_all_and_single_suites(Nth,nos_data,col)\n",
    "nos_data['All_suites'] = all_suites\n",
    "nos_data['One_suite'] = one_suite\n",
    "\n",
    "'''\n",
    "# now for the pdf extracted suites, for an eventual comparison\n",
    "Nth = 125\n",
    "col = 'Clean_suite'\n",
    "all_suites, one_suite, _, _, _ = get_all_and_single_suites(Nth,nos_data,col)\n",
    "nos_data['All_suites'] = all_suites\n",
    "nos_data['One_suite'] = one_suite\n",
    "'''\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nos_data[[col,'All_suites','One_suite']].sample(n=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now flatten the lists of lists with the suites and count how many unique suites there are:\n",
    "all_suites_flatten = flatten_lol(all_suites)\n",
    "# compare with the numbers from the ActiveIS metadata\n",
    "print('Unique new suites: ',len(list(set(all_suites_flatten))),'Unique ActiveIS suites: ', len(all_ground_suites))\n",
    "print('\\n')\n",
    "'''\n",
    "tmp = set(all_suites_flatten) - set(all_ground_suites)\n",
    "print('Suites ''obtained'' after joining all the data ({}):'.format(len(tmp)))\n",
    "print(tmp)\n",
    "print('\\n')\n",
    "tmp = set(all_ground_suites) - set(all_suites_flatten)\n",
    "print('Suites ''lost'' after joining all the data ({})'.format(len(tmp)))\n",
    "print(tmp)\n",
    "'''\n",
    "tmp = set(all_suites_flatten) - set(list(nos_data['One_suite']))\n",
    "print(len(tmp))\n",
    "print(tmp)\n",
    "pd.DataFrame(list(tmp)).rename(columns = {0: 'Omitted suite names'}).to_csv(\n",
    "    os.path.join(output_dir + '/List_of_omitted_suite_names.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list of unique suites\n",
    "SAVESUITES = False\n",
    "if SAVESUITES:\n",
    "    all_suites_sorted= list(set(all_suites_flatten))\n",
    "    all_suites_sorted.sort()\n",
    "    pd.DataFrame(all_suites_sorted).to_csv(os.path.join(figure_dir, 'All_suites_{}.csv'.format(qualifier)))\n",
    "#for t in list(set(all_suites_flatten)):\n",
    "#    print(t)\n",
    "#    #print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot summary statistics\n",
    "- To what extent did rate of NOS development remain consistent over time?\n",
    "- Which originating organisations contribute the most to NOS development?\n",
    "- Which suite areas are best covered (distribution by suite names)?\n",
    "- Are most of NOS core/ job specific /generic?\n",
    "- Which proportion of NOS is matched to occupations and whether certain occupations are over/underrepresented?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIG2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To what extent did rate of NOS development remain consistent over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approval year\n",
    "years = nos_data['Date_approved_year'].value_counts().index\n",
    "counts = nos_data['Date_approved_year'].value_counts().values\n",
    "# sort the years in ascending order\n",
    "IX = np.argsort(years)\n",
    "years = years[IX][:-4]\n",
    "counts = counts[IX]\n",
    "print(counts[-4:].sum()/counts.sum()*100)\n",
    "counts = counts[:-4]\n",
    "f = plt.figure(figsize= (10,5))\n",
    "plt.plot(years,counts,'-o', color = nesta_colours[3])\n",
    "#tmp = plt.xticks(np.arange(len(years)), [int(t) for t in years[:-1]] + ['missing'], rotation = 45)\n",
    "# suggested review year\n",
    "years = nos_data['Clean Ind Review Year'].value_counts().index\n",
    "counts = nos_data['Clean Ind Review Year'].value_counts().values\n",
    "# sort the years in ascending order\n",
    "IX = np.argsort(years)\n",
    "years = years[IX][:-2]\n",
    "counts = counts[IX]\n",
    "print(counts[-2:].sum()/counts.sum()*100)\n",
    "counts = counts[:-2]\n",
    "#f = plt.figure(figsize= (10,5))\n",
    "plt.plot(years,counts,'-o', color = nesta_colours[2])\n",
    "#tmp = plt.xticks(np.arange(len(years)), [int(t) for t in years[:-1]] + ['missing'], rotation = 45)\n",
    "plt.legend(['Year of approval','Suggested review year'], fontsize = 18)\n",
    "plt.xlabel('Year',fontsize = 18)\n",
    "plt.ylabel('Counts', fontsize = 18)\n",
    "plt.tight_layout()\n",
    "if SAVEFIG2:\n",
    "    plt.savefig(os.path.join(figure_dir, 'NOS_counts_per_approval_and_review_year_{}.png'.format(qualifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# suggested review year\n",
    "years = nos_data['Clean Ind Review Year'].value_counts().index\n",
    "counts = nos_data['Clean Ind Review Year'].value_counts().values\n",
    "# sort the years in ascending order\n",
    "IX = np.argsort(years)\n",
    "years = years[IX]\n",
    "counts = counts[IX]\n",
    "f = plt.figure(figsize= (10,5))\n",
    "plt.plot(counts,'-o')\n",
    "tmp = plt.xticks(np.arange(len(years)), [int(t) for t in years[:-1]] + ['missing'], rotation = 45)\n",
    "plt.xlabel('Suggested year of review',fontsize = 18)\n",
    "plt.ylabel('Counts', fontsize = 18)\n",
    "plt.tight_layout()\n",
    "if SAVEFIG2:\n",
    "    plt.savefig(os.path.join(figure_dir, 'NOS_counts_per_review_year_{}.png'.format(qualifier)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter with both approval and review year - size (or colour) of markers dependent on number of counts\n",
    "with sns.axes_style(\"white\"):\n",
    "    g = (sns.jointplot(x = 'Date_approved_year', y = 'Clean Ind Review Year', data = nos_data, kind = 'hex', \n",
    "             xlim = (1993,2030), ylim = (2000,2030),height=8, ratio=3)).set_axis_labels('Year of approval',\n",
    "    'Suggested year of review', fontsize = 18)\n",
    "#tmp = plt.xticks(np.arange(len(years)), [int(t) for t in years[:-1]] + ['missing'], rotation = 45)\n",
    "#plt.xlim(1998,2030)\n",
    "#plt.ylabel('Suggested year of review',fontsize = 18)\n",
    "#plt.xlabel('Year of approval', fontsize = 18)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "if SAVEFIG2:\n",
    "    plt.savefig(os.path.join(figure_dir, 'NOS_counts_per_approval_and_review_year_{}.png'.format(qualifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_in_2010 = (nos_data['Date_approved_year']==2010).sum() \n",
    "nos_by_2012 = (nos_data['Clean Ind Review Year']==2012).sum()\n",
    "nos_in_2010_by_2012 = (\n",
    "(nos_data['Date_approved_year']==2010) & (nos_data['Clean Ind Review Year']==2012)).sum()\n",
    "nos_in_2012 = (nos_data['Date_approved_year']==2012).sum()\n",
    "nos_in_2010_original = ((nos_data['Date_approved_year']==2010) & (nos_data['Status'] == 'original')).sum()\n",
    "nos_in_2012_original = ((nos_data['Date_approved_year']==2012) & (nos_data['Status'] == 'original')).sum()\n",
    "all_nos_in_2010 = nos_data[nos_data['Date_approved_year']==2010]\n",
    "all_nos_in_2012 = nos_data[nos_data['Date_approved_year']==2012]\n",
    "all_nos_after_2010 = nos_data[nos_data['Date_approved_year']>2010]\n",
    "all_nos_before_2010 = nos_data[nos_data['Date_approved_year']<=2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nos_data.columns)\n",
    "nos_in_2010, nos_by_2012, nos_in_2010_by_2012, nos_in_2012, nos_in_2010_original, nos_in_2012_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('2010')\n",
    "#print(all_nos_in_2010['Version_number'].value_counts())\n",
    "#print(all_nos_in_2010['Status'].value_counts())\n",
    "print('Before 2010 included')\n",
    "#print(all_nos_before_2012['Status'].value_counts())\n",
    "print(all_nos_before_2010['Version_number'].value_counts())\n",
    "print(all_nos_before_2010['Version_number'].value_counts().sum())\n",
    "#print('2012')\n",
    "#print(all_nos_in_2012['Status'].value_counts())\n",
    "#print(all_nos_in_2012['Version_number'].value_counts())\n",
    "print('After 2010 excluded')\n",
    "#print(all_nos_after_2012['Status'].value_counts())\n",
    "print(all_nos_after_2010['Version_number'].value_counts())\n",
    "print(all_nos_after_2010['Version_number'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nos_data[(nos_data['Date_approved_year']>2019) & (nos_data['Date_approved_year']<2100)]['Date_approved_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which originating organisations contribute the most to NOS development?\n",
    "#### Which suite areas are best covered (distribution by suite names)?\n",
    "#### Are most of NOS core/ job specific /generic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot some summary statistics of fields content.\n",
    "import matplotlib.ticker as ticker\n",
    "cols2plot = ['Developed By',\n",
    "        'Validity',  'Status',\n",
    "        'NOSCategory', 'One_suite']\n",
    "# 'Originating_organisation', 'Date_approved_year', 'Clean Ind Review Year', 'Version_number'\n",
    "#        'Keywords', 'Occupations', 'Skills']\n",
    "ws = [10,5,5,5,12]\n",
    "hs = [15,5,5,5,18]\n",
    "xlabels = ['Developing organisation','Validity','Status','Category','Suite']\n",
    "with sns.plotting_context('talk'):\n",
    "    for ix,col in enumerate(cols2plot):\n",
    "        fig= plt.figure(figsize = (ws[ix],hs[ix]))\n",
    "        tmp = nos_data[col].value_counts()\n",
    "        N = min(70,len(tmp))\n",
    "        tmp.iloc[:N][::-1].plot('barh', color = nesta_colours[3])\n",
    "        plt.ylabel(xlabels[ix],fontsize = 18)\n",
    "        plt.xlabel('Counts',fontsize = 18)\n",
    "        ax = plt.gca()\n",
    "        fig.canvas.draw()\n",
    "        labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "        ax.set_yticklabels(labels)\n",
    "        plt.tight_layout()\n",
    "        if SAVEFIG2:\n",
    "            plt.savefig(os.path.join(figure_dir, 'Most_common_{}_postjoining_{}.png'.format(col,qualifier)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which proportion of NOS is matched to occupations and whether certain occupations are over/underrepresented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socnames_file = '/Users/stefgarasto/Google Drive/Documents/data/ONS/soc2010indexversion705june2018.xls'\n",
    "socnames = pd.read_excel(socnames_file, sheet_name = 'SOC2010 Structure')\n",
    "socnames.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually clean the SOC codes\n",
    "nos_data['Clean SOC Code'] = nos_data['SOC Code'].map(lambda x: re.findall(r\"[\\d']+\", str(x)), na_action = 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[['Clean SOC Code','SOC Code']].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the SOC codes separately.\n",
    "# NOTES: some are a list, some are wrong, some are less than 4 digits (only few exceptions: twentytwo 1s and one 9)\n",
    "col = 'SOC Code'\n",
    "soccolnames = {1: 'Major Group', 2:'Sub-Major Group', 3: 'Minor Group', 4: 'Unit Group'}\n",
    "with sns.plotting_context('talk'):\n",
    "    plt.figure(figsize = (5,12))\n",
    "    # separate groups of digits and eliminate everything that is not a digit\n",
    "    flattened_socs= flatten_lol(nos_data[col].map(lambda x: re.findall(r\"[\\d']+\", str(x))).values)\n",
    "    tmp = pd.DataFrame(flattened_socs)[0].value_counts()\n",
    "    tmp.iloc[0:50][::-1].plot('barh', color = nesta_colours[3])\n",
    "    plt.ylabel('Occupations (4-digits SOC codes)',fontsize = 18)\n",
    "    plt.xlabel('Counts',fontsize = 18)\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG2:\n",
    "        plt.savefig(os.path.join(figure_dir, 'Most_common_{}_4digit_postjoining_{}.png'.format(col,qualifier)))\n",
    "    \n",
    "    # now plot for different number of digits\n",
    "    hs = [7,10,13]\n",
    "    for ix in [1,3,2]:\n",
    "        f = plt.figure(figsize = (10,hs[ix-1]))\n",
    "        tmp = pd.DataFrame(flattened_socs)[0].map(lambda x: x[0:ix]).value_counts()\n",
    "        if ix<3:\n",
    "            if ix == 2:\n",
    "                tmp = tmp[:-1]\n",
    "            g = tmp[::-1].plot('barh', color = nesta_colours[3])\n",
    "            g.set_xticklabels(g.get_xticklabels(), rotation = 0)\n",
    "        else:\n",
    "            g = tmp.iloc[0:50][::-1].plot('barh', color = nesta_colours[3])\n",
    "        # substitute labels\n",
    "        TX = plt.xticks()\n",
    "        T = plt.yticks()\n",
    "        for t in T[1]:\n",
    "            k = socnames[soccolnames[len(t.get_text())]]==float(t.get_text())\n",
    "            try:\n",
    "                t.set_text(socnames['Group Title'][k].values[0].lower().capitalize())\n",
    "            except:\n",
    "                print(t)\n",
    "        plt.yticks(T[0],T[1])\n",
    "        plt.gca().set_xticks([0,500,1000,1500,2000])\n",
    "        plt.gca().set_xticklabels([0,500,1000,1500,2000])\n",
    "        plt.ylabel('Occupations ({}-digits SOC codes)'.format(ix), fontsize = 18)\n",
    "        plt.xlabel('Counts', fontsize = 18)\n",
    "        plt.tight_layout()\n",
    "        if SAVEFIG2:\n",
    "            plt.savefig(os.path.join(figure_dir, \n",
    "                                     'Most_common_{}_{}digit_postjoining_{}.png'.format(col,ix,qualifier)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Suite'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# heathmap 2-digits SOC codes vs suites\n",
    "from collections import OrderedDict\n",
    "def extract2digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:2])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def extract3digits(x):\n",
    "    if isinstance(x,str):\n",
    "        try:\n",
    "            return float(x[:3])\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "col = 'SOC Code'\n",
    "flattened_socs= flatten_lol(nos_data[col].map(lambda x: re.findall(r\"[\\d']+\", str(x))).values)\n",
    "small_df = nos_data[['One_suite','SOC Code','Developed By']]\n",
    "small_df['SOC Code'] = small_df['SOC Code'].map(extract2digits)\n",
    "all_orgs= list(small_df['Developed By'].value_counts().index)\n",
    "all_main_suites = list(small_df['One_suite'].value_counts().index[:70])\n",
    "all_soc = list(small_df['SOC Code'].value_counts().index)\n",
    "soc_indices = {}\n",
    "for ixs,s in enumerate(all_soc):\n",
    "    soc_indices[s]= ixs\n",
    "heatmap_dict = np.zeros((len(all_orgs),len(all_soc))) #OrderedDict()\n",
    "heatmap_suite = np.zeros((len(all_main_suites),len(all_soc))) #OrderedDict()\n",
    "label_suites = []\n",
    "label_soc = copy.deepcopy(all_soc)\n",
    "label_orgs = []\n",
    "for it,t in enumerate(all_orgs):\n",
    "    T = t.capitalize()\n",
    "    label_orgs.append(T)\n",
    "    soc_by_org = small_df[small_df['Developed By']==t]['SOC Code'].value_counts()\n",
    "    #heatmap_dict[T] = {}\n",
    "    for ixs, s in enumerate(all_soc):\n",
    "        k = socnames['Sub-Major Group']==float(s)\n",
    "        try:\n",
    "            sname = socnames['Group Title'][k].values[0].lower().capitalize()\n",
    "            if s in soc_by_org:\n",
    "                label_soc[soc_indices[s]] =sname\n",
    "                #heatmap_dict[T][sname] = soc_by_org.loc[s]\n",
    "                heatmap_dict[it,soc_indices[s]] = soc_by_org.loc[s]\n",
    "            #else:\n",
    "                #heatmap_dict[T][sname] = 0\n",
    "        except:\n",
    "            continue\n",
    "# now for the main suites\n",
    "for it,t in enumerate(all_main_suites):\n",
    "    T = t.capitalize()\n",
    "    label_suites.append(T)\n",
    "    soc_by_org = small_df[small_df['One_suite']==t]['SOC Code'].value_counts()\n",
    "    #heatmap_suite[T] = {}\n",
    "    for ixs,s in enumerate(all_soc):\n",
    "        k = socnames['Sub-Major Group']==float(s)\n",
    "        try:\n",
    "            sname = socnames['Group Title'][k].values[0].lower().capitalize()\n",
    "            if s in soc_by_org:\n",
    "                #heatmap_suite[T][sname] = soc_by_org.loc[s]\n",
    "                heatmap_suite[it,soc_indices[s]] = soc_by_org.loc[s]\n",
    "            #else:\n",
    "            #    heatmap_suite[T][sname] = 0\n",
    "        except:\n",
    "            continue\n",
    "label_soc[24] = 'MANAGERS, DIRECTORS AND SENIOR OFFICIALS'.lower().capitalize()\n",
    "label_soc[-1] = 'ELEMENTARY OCCUPATIONS'.lower().capitalize()\n",
    "label_soc = [t for it,t in enumerate(label_soc) if it!=26]\n",
    "tmp = np.ones((28))\n",
    "tmp[26] = 0\n",
    "heatmap_dict = heatmap_dict[:, tmp==1]\n",
    "heatmap_suite = heatmap_suite[:, tmp==1]\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['seaborn-white','seaborn-poster'])\n",
    "plt.figure(figsize = (15,25))\n",
    "sns.heatmap(np.log(heatmap_dict+1), square = False, xticklabels = label_soc, yticklabels = label_orgs,\n",
    "           mask = heatmap_dict==0, cmap = sns.cm.rocket_r)\n",
    "plt.xticks(np.arange(28)+.9)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=70, horizontalalignment='right')\n",
    "plt.tight_layout()\n",
    "if SAVEFIG2:\n",
    "        plt.savefig(os.path.join(output_dir,'Heatmap_Developed By_by_SOC.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,30))\n",
    "#sns.heatmap(pd.DataFrame.from_dict(heatmap_suite, orient = 'index').applymap(lambda x: np.log(x+1)))\n",
    "sns.heatmap(np.log(heatmap_suite+1), square = False, xticklabels = label_soc, yticklabels = label_suites,\n",
    "           mask = heatmap_suite==0, cmap = sns.cm.rocket_r)\n",
    "plt.xticks(np.arange(28)+.9)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=70, horizontalalignment='right')\n",
    "plt.tight_layout()\n",
    "if SAVEFIG2:\n",
    "        plt.savefig(os.path.join(output_dir,'Heatmap_One_suite_by_SOC.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['seaborn-darkgrid','seaborn-poster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many NOS are in the most common and the least common suites\n",
    "tmp = nos_data['One_suite'].value_counts()\n",
    "print('There are {} suites when only taking the first one.'.format(len(tmp)))\n",
    "print('The least common suites are (all appear only once):')\n",
    "for t in tmp.index[-40:]:\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now get the distribution of suites and originating organisations for NOS that have SOC codes\n",
    "# this needs to be moved to collate_and_analyse_nos\n",
    "for col in ['One_suite', 'Developed By']:\n",
    "    duplicated_suites = nos_data[nos_data['SOC Code'].notnull()][col].value_counts()\n",
    "    all_suites = nos_data[col].value_counts()\n",
    "    if col == 'One_suite':\n",
    "        N = 70\n",
    "    else:\n",
    "        N = 32\n",
    "        \n",
    "    # divide by the sum (so we get fractions)\n",
    "    #duplicated_suites = duplicated_suites.map(lambda x: x/duplicated_suites.sum())\n",
    "    #all_suites = all_suites.map(lambda x: x/all_suites.sum())\n",
    "    \n",
    "    # get the ratio of proportions with respect to the full distribution\n",
    "    suites_ratio = {}\n",
    "    for row in all_suites.index[:N]:\n",
    "        try:\n",
    "            suites_ratio[row] = duplicated_suites.loc[row]/all_suites.loc[row]\n",
    "        except:\n",
    "            # if there's an error, it means this suite is never associated with a SOC code\n",
    "            suites_ratio[row] = 0\n",
    "    suites_ratio = pd.DataFrame.from_dict(suites_ratio, orient = 'index', columns = ['ratio'])\n",
    "    ## order by decreasing ratios \n",
    "    #suites_ratio= suites_ratio.sort_values(by='ratio', ascending = False)\n",
    "    # plot the ratio\n",
    "    if col == 'One_suite':\n",
    "        fig = plt.figure(figsize = (12,16))\n",
    "        ix = list(np.arange(0,N))\n",
    "        plt.ylabel('Suite', fontsize = 18)\n",
    "    else:\n",
    "        fig = plt.figure(figsize = (8,10))\n",
    "        ix = list(np.arange(0,N))\n",
    "        plt.ylabel('Developing organisation', fontsize = 18)\n",
    "    with sns.plotting_context('talk'):\n",
    "        suites_ratio['ratio'].iloc[ix][::-1].plot('barh', color = nesta_colours[3])\n",
    "    plt.xlabel('Proportion of NOS', fontsize = 18)\n",
    "    ax = plt.gca()\n",
    "    fig.canvas.draw()\n",
    "    labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG2:\n",
    "        plt.savefig(os.path.join(output_dir,'{}_ratios_for_soc_having_nos.png'.format(col)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are core NOS assigned to more than one suite more often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data['Multi_suite'] = nos_data['All_suites'].map(lambda x: len(x))\n",
    "B = nos_data[nos_data['Multi_suite']>1]['NOSCategory'].value_counts()\n",
    "print(B)\n",
    "B = B/B.sum()\n",
    "fig = plt.figure(figsize = (6,5))\n",
    "C = nos_data['NOSCategory'].value_counts()\n",
    "print(C)\n",
    "C = C/C.sum()\n",
    "print(B/C)\n",
    "(B/C).plot(kind = 'barh', color = nesta_colours[3])\n",
    "ax = plt.gca()\n",
    "fig.canvas.draw()\n",
    "labels = [item.get_text().capitalize() for item in ax.get_yticklabels()]\n",
    "ax.set_yticklabels(labels)\n",
    "plt.tight_layout()\n",
    "if SAVEFIG2:\n",
    "    plt.savefig(os.path.join(output_dir,'Category_ratios_for_multi_suite_nos.png'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in ['ASTD7', 'TDASTL67', 'LSIILARH10', 'ASTRBM14', 'SKSPI42', 'FSPPSA41']:\n",
    "    print(nos_data['NOSCategory'].loc[ix.lower() + '.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[nos_data['One_suite'] == 'food business strategic leadership']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[nos_data['One_suite'] == 'food business operational management']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the full text field, and save a subset of the data (only the relevant columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add one columns that collate all the data from multiple sections: title, overview, keywords, \n",
    "# relevant occupations (?), knowledge, performance, scope, glossary, behaviours, skills, values.\n",
    "\n",
    "def get_full_text(data_rows):\n",
    "    t0 = time.time()\n",
    "    full_texts = []\n",
    "    full_texts_keywords = []\n",
    "    for ii,row in enumerate(data_rows.index):\n",
    "        data_row = data_rows.loc[row]\n",
    "        for col in ['NOS Title', 'Knowledge_and_understanding', \n",
    "                    'Performance_criteria', 'Overview','Scope_range', 'Glossary', \n",
    "                    'Behaviours', 'Skills','Values','Occupations']: #'Relevant_occupations', #'extra_meta_info'?\n",
    "            if col == 'NOS Title':\n",
    "                full_texts.append(data_row[col] + ' ')\n",
    "                full_texts_keywords.append(data_row[col] + ' ')\n",
    "            else:\n",
    "                if isinstance(data_row[col],list): #not np.isnan(data_row[col]):\n",
    "                    L = len(data_row[col])\n",
    "                    for l in range(L):\n",
    "                        full_texts[-1] += data_row[col][l] + ' '\n",
    "                        full_texts_keywords[-1] += data_row[col][l] + ' '\n",
    "        # add the keywords columns only to the full_text_keywords\n",
    "        if isinstance(data_row['Keywords'], list):\n",
    "            L = len(data_row['Keywords'])\n",
    "            for l in range(L):\n",
    "                full_texts_keywords[-1] += data_row['Keywords'][l] + ' '\n",
    "        # keep track of where you are\n",
    "        if ii%1000 == 999:\n",
    "            print('Got to row {}. Time elapsed so far is {}'.format(ii,time.time()-t0))\n",
    "    return full_texts, full_texts_keywords\n",
    "\n",
    "out, _ = get_full_text(nos_data) #.apply(get_full_text, axis = 0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example of full text\n",
    "out[0][0:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the full_text column to the dataset: TODO: check\n",
    "nos_data_full = nos_data.join(pd.DataFrame(out, columns = ['full_text'], index = nos_data.index))\n",
    "#nos_data_full = nos_data_full.join(pd.DataFrame(out_kw, columns = ['full_text_keywords'], \n",
    "#                                                   index = nos_data_cleaner.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the relevant columns\n",
    "relevant_cols = ['Title', 'NOS Title', 'URN', 'Original URN', \n",
    "                 'Overview', 'Knowledge_and_understanding', 'Performance_criteria',\n",
    "                 'Scope_range', 'Glossary', 'Behaviours', 'Skills', 'Values', 'full_text',\n",
    "                 'Originating_organisation', 'Date_approved', 'Date_approved_year', \n",
    "                 'Indicative Review Date', 'Clean Ind Review Year', 'Version_number',\n",
    "                 'Links_to_other_NOS', 'External_Links', 'Developed By', 'Validity', \n",
    "                 'Keywords', 'Clean SOC Code', 'NOS Document Status', 'NOSCategory',\n",
    "                 'Suite', 'SuiteMetadata', 'Occupations', 'OccupationsMetadata', 'One_suite', 'All_suites',\n",
    "                 'notes', 'empty', 'extra_meta_info', 'Created', 'Modified', 'Item Type', 'Path']\n",
    "#final_relevant_cols = relevant_cols + ['full_text']\n",
    "nos_data_full.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset with only a subset of the columns (title, suite and full_text)\n",
    "#nos_data_full.to_csv(\n",
    "#    data_dir + '/all_nos_input_for_nlp.csv', columns = ['Title','full_text','Suite','All_suites','One_suite'], \n",
    "#    index = True, header = True)\n",
    "#[['Title','full_text','Suite','All_suites','One_suite']]\n",
    "SAVETEXT = False\n",
    "if SAVETEXT:\n",
    "    nos_data_full[relevant_cols].to_pickle(\n",
    "        data_dir + '/all_nos_input_for_nlp_postjoining_{}.zip'.format(qualifier))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive stats for the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_suites_files=  '/Users/stefgarasto/Google Drive/Documents/data/NOS_meta_data/NOS_Suite_Priority.xlsx'\n",
    "super_suites_names = ['Engineering','Management','FinancialServices','Construction']\n",
    "all_super_suites = {}\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_super_suites[which_super_suite] = pd.read_excel(super_suites_files, sheet_name = which_super_suite)\n",
    "    all_super_suites[which_super_suite]['NOS Suite name'] = all_super_suites[which_super_suite]['NOS Suite name'].map(\n",
    "        lambda x: x.replace('(','').replace('(','').replace('&','and').strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_labels = list(nos_data.groupby('One_suite').groups.keys())\n",
    "all_matches = {}\n",
    "all_match_names = {}\n",
    "#match_name = []\n",
    "for which_super_suite in super_suites_names:\n",
    "    all_matches[which_super_suite] = []\n",
    "    for suite in all_super_suites[which_super_suite]['NOS Suite name'].values:\n",
    "        # do manually some selected suites\n",
    "        if 'insurance claims' in suite:\n",
    "            tmp = standard_labels.index('general insurance')\n",
    "            all_matches[which_super_suite].append(tmp)\n",
    "            continue\n",
    "        # for the \"management and leadership marketing 2013\" both marketing and marketing 2013 would fit,\n",
    "        # but I'm only taking the latter\n",
    "        # find a fuzzy match between \n",
    "        out = process.extract(suite, standard_labels, limit=3)\n",
    "        if len(out) and out[0][1]>89:\n",
    "            # note: most of them are above 96% similarity (only one is 90%)\n",
    "            tmp = standard_labels.index(out[0][0])\n",
    "            #print(suite, out[0])\n",
    "            if tmp not in all_matches[which_super_suite]:\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "            else:\n",
    "                if suite == 'installing domestic fascia, soffit, and bargeboards':\n",
    "                    # this suite is kind of a duplicate - I aggregated it in my suites list\n",
    "                    continue\n",
    "                tmp = standard_labels.index(out[2][0])\n",
    "                all_matches[which_super_suite].append(tmp)\n",
    "                print(out[0][0],',',out[1][0],',',out[2][0],',',suite)\n",
    "        else:\n",
    "            print(suite, ' not found')\n",
    "            print(out)\n",
    "            print('\\n')\n",
    "    print(len(all_matches[which_super_suite]),len(all_super_suites[which_super_suite]))\n",
    "    all_match_names[which_super_suite] = [standard_labels[t] for t in all_matches[which_super_suite]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_supersuite(x):\n",
    "    for supersuite in all_match_names.keys():\n",
    "        if x in all_match_names[supersuite]:\n",
    "            return supersuite.lower()\n",
    "    # if no match has been found\n",
    "    return 'other'\n",
    "def adjustsoccode(x):\n",
    "    y = re.findall(r\"[\\d']+\", str(x))\n",
    "    if len(y):\n",
    "        return y[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "nos_data['supersuite'] = nos_data['One_suite'].apply(assign_supersuite)\n",
    "# extract 2 digit soc\n",
    "nos_data['SOC4'] = nos_data['SOC Code'].map(adjustsoccode)\n",
    "nos_data['SOC2'] = nos_data['SOC4'].map(extract2digits)\n",
    "nos_data['SOC3'] = nos_data['SOC4'].map(extract3digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data['supersuite'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing SOC codes for each supersuite? How many different SOC codes? Which ones are they? \n",
    "# Do they overlap across supersuites?\n",
    "soc4dist = {}\n",
    "soc3dist= {}\n",
    "filename = '/Users/stefgarasto/Google Drive/Documents/results/BG/info_from_NOS_supersuites.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    print('In total there are {} different 4-digit SOC codes and {} different 3-digit SOC codes'.format(\n",
    "        len(nos_data['SOC4'].value_counts()),len(nos_data['SOC3'].value_counts())), file = f)\n",
    "    supergroups = nos_data.groupby('supersuite')\n",
    "    for supersuite,g in supergroups:\n",
    "        missing_soc = g['SOC4'].isnull().sum()\n",
    "        print('Number of missing SOC codes for super suite {} is {} over {} NOS.'.format(supersuite.capitalize(),\n",
    "                                                                                    missing_soc,len(g)), file = f)\n",
    "        soc4dist[supersuite]= g['SOC4'].value_counts().index\n",
    "        soc3dist[supersuite]= g['SOC3'].value_counts().index\n",
    "        print('There are {} different 4-digit SOC codes and {} different 3-digit SOC codes'.format(\n",
    "        len(soc4dist[supersuite]),len(soc3dist[supersuite])), file = f)\n",
    "        print('\\n', file = f)\n",
    "\n",
    "    total_socs4 = []\n",
    "    total_socs3 = []\n",
    "    for ix,supersuite1 in enumerate(all_matches.keys()):\n",
    "        for supersuite2 in list(all_matches.keys())[ix+1:]:\n",
    "            if supersuite1 == supersuite2:\n",
    "                continue\n",
    "            print('Intersection between {} and {}'.format(supersuite1, supersuite2),file=f)\n",
    "            print('SOC 4 digits:',file=f)\n",
    "            print(set(soc4dist[supersuite1.lower()]).intersection(set(soc4dist[supersuite2.lower()])),file=f)\n",
    "            print('SOC 3 digits:',file=f)\n",
    "            print(set(soc3dist[supersuite1.lower()]).intersection(set(soc3dist[supersuite2.lower()])),file=f)\n",
    "            print('\\n',file=f)\n",
    "        total_socs4 += list(soc4dist[supersuite1.lower()])\n",
    "        total_socs3 += list(soc3dist[supersuite1.lower()])\n",
    "\n",
    "total_socs4 = list(set(total_socs4))\n",
    "total_socs3 = list(set(total_socs3))\n",
    "\n",
    "print('Lenght of stored SOC codes (for all super-suites)',len(total_socs4),len(total_socs3))\n",
    "SAVEDATA = False\n",
    "if SAVEDATA:\n",
    "    with open('/Users/stefgarasto/Google Drive/Documents/results/NOS/notes/soc_codes_list_for_bg.pickle','wb') as f:\n",
    "        pickle.dump((total_socs4,total_socs3,soc4dist,soc3dist),f)\n",
    "\n",
    "#super_nos_data = nos_data[nos_data['supersuite']!='other']\n",
    "#print(len(super_nos_data['SOC Code'].value_counts()))\n",
    "#print(len(super_nos_data['SOC3'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc4dist['management']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[nos_data['SOC4']=='1170']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_data[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
